"""
check_outlier é¡¹ç›®é€‚é…å™¨
å°è£…æ¨ç†æ£€æµ‹åŠŸèƒ½
"""
import os
import sys
import json
import uuid
import subprocess
from pathlib import Path
from typing import List, Dict, Optional, Any

from configs.settings import settings


class CheckOutlierAdapter:
    """check_outlier é¡¹ç›®é€‚é…å™¨"""
    
    def __init__(self):
        self.project_path = Path(settings.CHECK_OUTLIER_PATH)
        self.project_path = Path(settings.CHECK_OUTLIER_PATH)
        self.run_script = self.project_path / "run.py"
        self.active_processes: Dict[str, subprocess.Popen] = {}
        self.cancelled_tasks: set = set()

    def stop_inference_task(self, task_id: str):
        """åœæ­¢æŒ‡å®šçš„æ¨ç†ä»»åŠ¡"""
        self.cancelled_tasks.add(task_id)
        if task_id in self.active_processes:
            try:
                self.active_processes[task_id].terminate()
                self.active_processes[task_id].kill()  # Force kill to be safe
                del self.active_processes[task_id]
                return True
            except Exception as e:
                print(f"Error stopping task {task_id}: {e}")
                return False
        # If not in active_processes, we still marked it as cancelled, so return True
        return True

    def convert_to_annotation_format(self, inference_result: Any) -> str:
        """
        å°†æ¨ç†ç»“æœè½¬æ¢ä¸ºæ ‡æ³¨å·¥å…·å¯å¯¼å…¥çš„ JSON æ–‡ä»¶ã€‚

        æ”¯æŒè¾“å…¥:
        - dict: {"results": [...]} æˆ–å·²æ˜¯ {"filename","annotations"} ç»“æ„
        - list: å¤šæ¡ç»“æœ
        - str: JSON å­—ç¬¦ä¸²æˆ– JSON æ–‡ä»¶è·¯å¾„
        """
        payload = inference_result
        if isinstance(inference_result, str):
            candidate_path = Path(inference_result.strip())
            if candidate_path.exists():
                with open(candidate_path, "r", encoding="utf-8") as f:
                    payload = json.load(f)
            else:
                payload = json.loads(inference_result)

        rows = self._extract_annotation_rows(payload)

        output_dir = Path("/tmp/ts_iteration_loop")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_path = output_dir / f"inference_annotations_{uuid.uuid4().hex}.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(rows, f, ensure_ascii=False, indent=2)
        return str(output_path)

    def _extract_annotation_rows(self, payload: Any) -> List[Dict]:
        if payload is None:
            return []

        if isinstance(payload, list):
            rows: List[Dict] = []
            for item in payload:
                rows.extend(self._normalize_result_item(item))
            return rows

        if isinstance(payload, dict):
            if isinstance(payload.get("results"), list):
                rows: List[Dict] = []
                for item in payload["results"]:
                    rows.extend(self._normalize_result_item(item))
                return rows
            return self._normalize_result_item(payload)

        return []

    def _normalize_result_item(self, item: Any) -> List[Dict]:
        if not isinstance(item, dict):
            return []

        # Already in annotator import format.
        filename = item.get("filename") or item.get("file")
        annotations = item.get("annotations")
        if filename and isinstance(annotations, list):
            return [{
                "filename": filename,
                "annotations": annotations,
                "source": item.get("source", "inference")
            }]

        # Wrapped result from adapter output: {"file": "...", "result": ...}
        wrapped_result = item.get("result")
        if wrapped_result is None:
            return []

        return self._parse_wrapped_result(wrapped_result, fallback_filename=item.get("file"))

    def _parse_wrapped_result(self, wrapped_result: Any, fallback_filename: Optional[str]) -> List[Dict]:
        if isinstance(wrapped_result, str):
            try:
                wrapped_result = json.loads(wrapped_result)
            except Exception:
                return []

        if isinstance(wrapped_result, list):
            rows: List[Dict] = []
            for entry in wrapped_result:
                if not isinstance(entry, dict):
                    continue
                if entry.get("filename") and isinstance(entry.get("annotations"), list):
                    rows.append({
                        "filename": entry["filename"],
                        "annotations": entry["annotations"],
                        "source": entry.get("source", "inference")
                    })
            return rows

        if isinstance(wrapped_result, dict):
            # Single record already in target format.
            if wrapped_result.get("filename") and isinstance(wrapped_result.get("annotations"), list):
                return [{
                    "filename": wrapped_result["filename"],
                    "annotations": wrapped_result["annotations"],
                    "source": wrapped_result.get("source", "inference")
                }]

            # Legacy shape from mocked tests.
            anomalies = wrapped_result.get("detected_anomalies") or wrapped_result.get("anomalies") or []
            if not fallback_filename or not isinstance(anomalies, list):
                return []

            converted_annotations = []
            for idx, anomaly in enumerate(anomalies):
                if not isinstance(anomaly, dict):
                    continue
                segment = self._extract_segment(anomaly)
                if segment is None:
                    continue
                converted_annotations.append({
                    "label": str(anomaly.get("type") or anomaly.get("label") or f"inference_{idx+1}"),
                    "color": "#d946ef",
                    "segments": [segment],
                    "analysis": anomaly.get("reason") or anomaly.get("analysis") or ""
                })

            if converted_annotations:
                return [{
                    "filename": fallback_filename,
                    "annotations": converted_annotations,
                    "source": "inference"
                }]

        return []

    def _extract_segment(self, anomaly: Dict) -> Optional[Dict]:
        interval = anomaly.get("interval") or anomaly.get("segment")
        if isinstance(interval, (list, tuple)) and len(interval) >= 2:
            try:
                start = int(interval[0])
                end = int(interval[1])
                if end < start:
                    start, end = end, start
                return {"start": start, "end": end}
            except Exception:
                return None

        if "start" in anomaly and "end" in anomaly:
            try:
                start = int(anomaly["start"])
                end = int(anomaly["end"])
                if end < start:
                    start, end = end, start
                return {"start": start, "end": end}
            except Exception:
                return None

        return None
    
    def run_batch_inference(
        self,
        task_id: str,
        model: str,
        algorithm: str,
        input_files: List[str],
        **kwargs
    ) -> Dict:
        """
        æ‰§è¡Œæ‰¹é‡æ¨ç†ä»»åŠ¡
        è°ƒç”¨ run.py è„šæœ¬
        """
        if not self.run_script.exists():
            return {"success": False, "error": f"è„šæœ¬ä¸å­˜åœ¨: {self.run_script}"}
        
        # æ ¹æ®ç®—æ³•é€‰æ‹©åŸºç¡€å‚æ•°
        algorithm_args = self._build_algorithm_args(algorithm, model)
        
        # åˆå¹¶é¢å¤–çš„ UI å‚æ•°
        algorithm_args.update(kwargs)
        
        # æ„å»ºé…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°
        results = []
        errors = []
        
        for input_file in input_files:
            try:
                result = self._run_single_inference(
                    input_file, 
                    algorithm, 
                    algorithm_args,
                    task_id=task_id
                )
                results.append(result)
            except Exception as e:
                errors.append({"file": input_file, "error": str(e)})
        
        return {
            "success": len(errors) == 0,
            "results": results,
            "errors": errors,
            "total": len(input_files),
            "successful": len(results)
        }
    
    def _build_algorithm_args(self, algorithm: str, model: str) -> Dict:
        """æ„å»ºç®—æ³•å‚æ•°"""
        if algorithm == "chatts":
            return {
                "method": "chatts",
                "chatts_model_path": model, # æ³¨æ„ï¼šè¿™æ˜¯ä¼ ç»™ run.py çš„ --chatts_model_path
                "chatts_enabled": True
            }
        elif algorithm == "qwen":
            return {
                "method": "qwen",
                "chatts_model_path": model, # Pass model path to reused argument in run.py
                "chatts_enabled": False # Not using legacy ChatTS logic but new Qwen block
            }
        elif algorithm == "adtk_hbos":
            return {
                "method": "adtk_hbos",
                "chatts_enabled": False
            }
        else:
            return {"method": algorithm}
    
    def _run_single_inference(
        self, 
        input_file: str, 
        algorithm: str,
        args: Dict,
        task_id: Optional[str] = None
    ) -> Dict:
        """æ‰§è¡Œå•ä¸ªæ–‡ä»¶æ¨ç†"""
        # ä½¿ç”¨ Python è§£é‡Šå™¨ï¼ˆç»Ÿä¸€æ¨¡å¼ä½¿ç”¨ PYTHON_UNIFIEDï¼‰
        python_exe = settings.PYTHON_UNIFIED if settings.USE_LOCAL_MODULES else settings.PYTHON_ILABEL
        cmd = [
            python_exe, str(self.run_script),
            "--input", input_file,
            "--method", algorithm,
            "--task_name", ""  # Suppress default 'global' subfolder
        ]
        if task_id:
            cmd.extend(["--task-id", str(task_id)])
        if task_id:
            cmd.extend(["--task-id", str(task_id)])
        
        # å¤„ç†å‚æ•°æ˜ å°„
        # 1. å¿…éœ€å‚æ•°
        # é»˜è®¤é™é‡‡æ ·ç‚¹æ•°ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥ n_downsampleï¼Œåˆ™ä½¿ç”¨ settings çš„é»˜è®¤å€¼
        if "n_downsample" not in args:
             cmd.extend(["--n_downsample", str(settings.DEFAULT_DOWNSAMPLE_POINTS)])
        
        # 2. éå† args æ·»åŠ å‚æ•°
        for k, v in args.items():
            if v is None or v == "":
                continue
            
            # ç‰¹æ®Šå¤„ç†å†…éƒ¨æ ‡è®°
            if k == "chatts_enabled":
                cmd.append("--use-chatts")
                continue
            if k == "model_path": # å…¼å®¹æ—§ä»£ç ï¼Œè™½ç„¶ä¸Šé¢æ”¹æˆäº† chatts_model_path
                 cmd.extend(["--model", str(v)])
                 continue

            # å¤„ç†å¸ƒå°”å€¼å‚æ•° (ä¾‹å¦‚ --chatts_load_in_4bit)
            # æ³¨æ„ï¼šrun.py ä¸­æŸäº›å¸ƒå°”å‚æ•°å¯èƒ½æ˜¯æ¥æ”¶å­—ç¬¦ä¸² "true"/"false" æˆ– action="store_true"
            # æ ¹æ® default_params.json åˆ†æï¼Œå¤§éƒ¨åˆ†æ˜¯å­—ç¬¦ä¸²ç±»å‹çš„ true/false æˆ– auto
            
            # å°†ä¸‹åˆ’çº¿è½¬æ¢ä¸ºè¿å­—ç¬¦ï¼Œä¾‹å¦‚ chatts_load_in_4bit -> --chatts-load-in-4bit
            arg_name = f"--{k.replace('_', '-')}"
            
            # é˜²æ­¢é‡å¤æ·»åŠ  method
            if arg_name == "--method":
                continue
            
            cmd.extend([arg_name, str(v)])
        
        # å¼ºåˆ¶æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œç¡®ä¿ä¸ç³»ç»Ÿç»Ÿä¸€é…ç½®ä¸€è‡´
        if "--data_path" not in cmd:
            cmd.extend(["--data_path", settings.DATA_INFERENCE_DIR])
        
        try:
            result = subprocess.run(
                cmd,
                cwd=str(self.project_path),
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            # å°è¯•è§£æè¾“å‡ºä¸º JSON
            output = result.stdout.strip()
            parsed = None
            
            # å°è¯•æå–æ ‡è®°ä¹‹é—´çš„ JSON
            import re
            match = re.search(r"__JSON_START__\s*(.*?)\s*__JSON_END__", output, re.DOTALL)
            if match:
                try:
                    json_str = match.group(1)
                    parsed = json.loads(json_str)
                    # parsed åº”è¯¥æ˜¯ä¸€ä¸ª list, æˆ‘ä»¬å–ç¬¬ä¸€ä¸ªæˆ–è€…åˆå¹¶
                    # run.py è¾“å‡ºçš„æ˜¯ [{"filename":..., "annotations":...}, ...]
                    # è¿™é‡Œçš„ adapter æ˜¯ _run_single_inference, æ‰€ä»¥é€šå¸¸åªæœ‰ä¸€ä¸ªç»“æœï¼Œä½†è¿™å–å†³äº run.py æ˜¯æ€ä¹ˆè¢«è°ƒç”¨çš„
                    # å¦‚æœ input_file åªæœ‰ä¸€ä¸ªï¼Œrun.py loop åªè·‘ä¸€æ¬¡ï¼Œlist len=1
                    # æˆ‘ä»¬è¿”å›æ•´ä¸ª list æˆ–è€… result
                except:
                    pass
            
            if parsed is None:
                # Fallback: try parsing whole output (legacy behavior)
                try:
                    parsed = json.loads(output)
                except json.JSONDecodeError:
                    parsed = {"raw_output": output}
            
            return {
                "file": input_file,
                "success": result.returncode == 0,
                "result": parsed
            }
        except subprocess.TimeoutExpired:
            return {"file": input_file, "success": False, "error": "è¶…æ—¶"}
        except Exception as e:
            return {"file": input_file, "success": False, "error": str(e)}
    
    def run_batch_inference_streaming(
        self,
        task_id: str,
        model: str,
        algorithm: str,
        input_files: List[str],
        **kwargs
    ):
        """
        æ‰§è¡Œæ‰¹é‡æ¨ç†ä»»åŠ¡ï¼ˆæµå¼è¾“å‡ºç‰ˆï¼‰
        ä½¿ç”¨ç”Ÿæˆå™¨ yield å®æ—¶è¾“å‡ºæ—¥å¿—
        """
        if not self.run_script.exists():
            yield f"âŒ è„šæœ¬ä¸å­˜åœ¨: {self.run_script}"
            return
        
        # æ ¹æ®ç®—æ³•é€‰æ‹©åŸºç¡€å‚æ•°
        algorithm_args = self._build_algorithm_args(algorithm, model)
        algorithm_args.update(kwargs)
        
        total_files = len(input_files)
        yield f"ğŸš€ Starting batch inference for {total_files} files...\n"
        
        success_count = 0
        failed_count = 0
        errors = []
        
        for idx, input_file in enumerate(input_files, 1):
            if task_id in self.cancelled_tasks:
                yield "ğŸ›‘ Task cancelled by user.\n"
                break
                
            yield f"\nğŸ“„ **Processing file ({idx}/{total_files}):** `{Path(input_file).name}`\n"
            
            try:
                # æ‰§è¡Œå•ä¸ªæ–‡ä»¶æ¨ç†ï¼ˆæµå¼ï¼‰
                file_success = False
                for log in self._run_single_inference_streaming(
                    task_id,
                    input_file, 
                    algorithm, 
                    algorithm_args
                ):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæˆæ ‡è®°ï¼ˆè‡ªå®šä¹‰åè®®ï¼Œæˆ–è€…ä»…ä½œä¸ºæ—¥å¿—è¾“å‡ºï¼‰
                    if isinstance(log, dict) and "success" in log:
                         file_success = log["success"]
                         if not file_success:
                             errors.append({"file": input_file, "error": log.get("error", "Unknown error")})
                    else:
                        yield log
                
                if file_success:
                    success_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                failed_count += 1
                errors.append({"file": input_file, "error": str(e)})
                yield f"âŒ Error processing {input_file}: {str(e)}\n"
        
        # æ±‡æ€»ç»“æœ
        yield f"\n\n---\nâœ… **Batch Inference Completed**\n"
        yield f"- Success: {success_count}\n"
        yield f"- Failed: {failed_count}\n"
        
        if errors:
            yield "\n**Errors:**\n"
            for e in errors:
                yield f"- {Path(e['file']).name}: {e['error']}\n"

    def _run_single_inference_streaming(
        self, 
        task_id: str,
        input_file: str, 
        algorithm: str,
        args: Dict
    ):
        """æ‰§è¡Œå•ä¸ªæ–‡ä»¶æ¨ç†ï¼ˆæµå¼ï¼‰"""
        # æ„å»ºå‘½ä»¤ï¼ˆä¸ _run_single_inference ä¿æŒä¸€è‡´ï¼‰
        python_exe = settings.PYTHON_UNIFIED if settings.USE_LOCAL_MODULES else settings.PYTHON_ILABEL
        cmd = [
            python_exe, str(self.run_script),
            "--input", input_file,
            "--method", algorithm,
            "--task_name", ""  # Suppress default 'global' subfolder
        ]
        
        # å¤„ç†å‚æ•°æ˜ å°„
        if "n_downsample" not in args:
             cmd.extend(["--n_downsample", str(settings.DEFAULT_DOWNSAMPLE_POINTS)])
        
        # ç‰¹æ®Šå¤„ç† ChatTS æ¨¡å‹å‚æ•°
        if algorithm == "chatts":
            # LoRA Adapter
            if args.get("lora_adapter_path"):
                 cmd.extend(["--chatts_lora_adapter_path", str(args["lora_adapter_path"])])
            # Base Model (å¦‚æœæœªæŒ‡å®šï¼Œrun.py ä¼šä½¿ç”¨é»˜è®¤å€¼ï¼Œè¿™é‡Œæ˜¾å¼ä¼ é€’æ›´å®‰å…¨)
            if args.get("base_model_path"):
                 cmd.extend(["--chatts_model_path", str(args["base_model_path"])])
        
        # æ–°å¢ Qwen å‚æ•°å¤„ç†
        if algorithm == "qwen":
            if args.get("base_model_path"):
                 cmd.extend(["--qwen_model_path", str(args["base_model_path"])])
            
        for k, v in args.items():
            if v is None or v == "": continue
            
            # è·³è¿‡å·²å¤„ç†çš„å‚æ•°
            if k in ["model", "base_model_path", "lora_adapter_path"]: continue
            if k == "chatts_enabled": continue 
            
            # ç‰¹æ®Šå¤„ç†å¸ƒå°”å€¼
            if k == "timer_streaming":
                if str(v).lower() == "true":
                    cmd.extend(["--timer_streaming", "True"])
                continue
            
            # å¸¸è§„å‚æ•°å¤„ç†
            arg_name = f"--{k}"
            cmd.extend([arg_name, str(v)])

        # å¼ºåˆ¶æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œç¡®ä¿ä¸ç³»ç»Ÿç»Ÿä¸€é…ç½®ä¸€è‡´
        if "--data_path" not in cmd:
            cmd.extend(["--data_path", settings.DATA_INFERENCE_DIR])
            
        yield f"Running command: `{' '.join(cmd[:10])}...`\n"

        process = None
        try:
            # ä½¿ç”¨ Popen è·å–å®æ—¶è¾“å‡º
            process = subprocess.Popen(
                cmd,
                cwd=str(self.project_path),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )
            
            # æ³¨å†Œè¿›ç¨‹
            if task_id:
                self.active_processes[task_id] = process
            
            output_lines = []
            
            # Local variable to track result directory for this specific run
            current_result_dir = None

            # å®æ—¶è¯»å–æ—¥å¿—
            for line in iter(process.stdout.readline, ''):
                if line:
                    stripped_line = line.rstrip()
                    output_lines.append(stripped_line)
                    
                    # æ•è·ç»“æœæ–‡ä»¶ç›®å½•
                    # Log format: "Saving results to: /path/to/dir"
                    if "Saving results to:" in stripped_line:
                        parts = stripped_line.split("Saving results to:")
                        if len(parts) >= 2:
                            current_result_dir = parts[-1].strip()

                    # æ•è·å®é™…ä¿å­˜çš„æ–‡ä»¶å
                    # Log format: "ä¿å­˜ç»“æœ: filename.csv"
                    if "ä¿å­˜ç»“æœ" in stripped_line and ".csv" in stripped_line:
                        parts = stripped_line.split(":")
                        if len(parts) >= 2:
                            filename = parts[-1].strip()
                            # æ„å»ºå®Œæ•´è·¯å¾„
                            if current_result_dir:
                                full_path = f"{current_result_dir}/{filename}"
                            else:
                                # Fallback: use algorithm specific directory
                                default_dir = os.path.join(settings.DATA_INFERENCE_DIR, algorithm)
                                full_path = f"{default_dir}/{filename}"
                            yield {"file_path": full_path, "file_name": filename}

                    # è¿‡æ»¤å¹¶æ ¼å¼åŒ–æœ‰ç”¨ä¿¡æ¯
                    # 1. è¿›åº¦æ¡ (tqdm)
                    if "%" in stripped_line or "it/s" in stripped_line:
                        # å¯¹äºè¿›åº¦æ¡ï¼Œä½¿ç”¨è¡Œå†…ä»£ç å—æˆ–ç‰¹å®šæ ¼å¼ï¼Œé¿å…åˆ·å±
                         if "ChatTS å¤„ç†è¿›åº¦" in stripped_line or "Loading" in stripped_line:
                             yield f"> {stripped_line}\n"
                         continue

                    # 2. å…³é”®çŠ¶æ€ä¿¡æ¯
                    if stripped_line.startswith("[ChatTS]") or "Data shape" in stripped_line or "Saving results" in stripped_line:
                         yield f"- {stripped_line}\n"
                    elif "Error" in stripped_line or "Exception" in stripped_line:
                         yield f"âŒ **{stripped_line}**\n"
                    # 3. å…è®¸æ›´å¤šè°ƒè¯•ä¿¡æ¯é€šè¿‡
                    elif any(k in stripped_line for k in ["Traceback", "File \"", "line ", "KeyError", "ValueError", "DEBUG", "INFO"]):
                        yield f"```\n{stripped_line}\n```\n"
                    # 4. é»˜è®¤æ˜¾ç¤ºå…¶ä»–éç©ºè¡Œ (ä½œä¸ºå¼•è¨€ï¼Œé¿å…å¤ªä¹±)
                    else:
                        yield f"> {stripped_line}\n"
            
            process.wait()
            
            # è¿›ç¨‹ç»“æŸåç§»é™¤
            if task_id and task_id in self.active_processes:
                del self.active_processes[task_id]

            # è§£ææœ€åçš„ç»“æœï¼ˆä»…ä¸ºäº†åˆ¤æ–­æˆåŠŸä¸å¦ï¼Œä¸ç”¨äºè¿”å›å¤§é‡æ•°æ®ï¼‰
            # æ³¨æ„ï¼šæµå¼æ¨¡å¼ä¸‹æˆ‘ä»¬æ— æ³•åƒ run_batch_inference é‚£æ ·æ–¹ä¾¿åœ°è¿”å›ç»“æ„åŒ–ç»“æœ
            # è¿™é‡Œæˆ‘ä»¬åªè¿”å›ä¸€ä¸ªçŠ¶æ€æ ‡è®°
            if process.returncode == 0:
                yield {"success": True}
                yield "âœ… Finished processing file.\n"
            else:
                yield {"success": False, "error": f"Process exited with code {process.returncode}"}
                yield f"âŒ Process failed with code {process.returncode}\n"
                # è¾“å‡ºæœ€åå‡ è¡Œæ—¥å¿—ä½œä¸ºé”™è¯¯ä¸Šä¸‹æ–‡
                yield "```\n" + "\n".join(output_lines[-10:]) + "\n```\n"
                
        except Exception as e:
            yield {"success": False, "error": str(e)}
            yield f"âŒ Execution error: {str(e)}\n"
