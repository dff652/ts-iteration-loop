"""
check_outlier é¡¹ç›®é€‚é…å™¨
å°è£…æ¨ç†æ£€æµ‹åŠŸèƒ½
"""
import os
import sys
import json
import subprocess
from pathlib import Path
from typing import List, Dict, Optional

from configs.settings import settings


class CheckOutlierAdapter:
    """check_outlier é¡¹ç›®é€‚é…å™¨"""
    
    def __init__(self):
        self.project_path = Path(settings.CHECK_OUTLIER_PATH)
        self.project_path = Path(settings.CHECK_OUTLIER_PATH)
        self.run_script = self.project_path / "run.py"
        self.active_processes: Dict[str, subprocess.Popen] = {}
        self.cancelled_tasks: set = set()

    def stop_inference_task(self, task_id: str):
        """åœæ­¢æŒ‡å®šçš„æ¨ç†ä»»åŠ¡"""
        self.cancelled_tasks.add(task_id)
        if task_id in self.active_processes:
            try:
                self.active_processes[task_id].terminate()
                self.active_processes[task_id].kill()  # Force kill to be safe
                del self.active_processes[task_id]
                return True
            except Exception as e:
                print(f"Error stopping task {task_id}: {e}")
                return False
        return False
    
    def run_batch_inference(
        self,
        task_id: str,
        model: str,
        algorithm: str,
        input_files: List[str],
        **kwargs
    ) -> Dict:
        """
        æ‰§è¡Œæ‰¹é‡æ¨ç†ä»»åŠ¡
        è°ƒç”¨ run.py è„šæœ¬
        """
        if not self.run_script.exists():
            return {"success": False, "error": f"è„šæœ¬ä¸å­˜åœ¨: {self.run_script}"}
        
        # æ ¹æ®ç®—æ³•é€‰æ‹©åŸºç¡€å‚æ•°
        algorithm_args = self._build_algorithm_args(algorithm, model)
        
        # åˆå¹¶é¢å¤–çš„ UI å‚æ•°
        algorithm_args.update(kwargs)
        
        # æ„å»ºé…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°
        results = []
        errors = []
        
        for input_file in input_files:
            try:
                result = self._run_single_inference(
                    input_file, 
                    algorithm, 
                    algorithm_args
                )
                results.append(result)
            except Exception as e:
                errors.append({"file": input_file, "error": str(e)})
        
        return {
            "success": len(errors) == 0,
            "results": results,
            "errors": errors,
            "total": len(input_files),
            "successful": len(results)
        }
    
    def _build_algorithm_args(self, algorithm: str, model: str) -> Dict:
        """æ„å»ºç®—æ³•å‚æ•°"""
        if algorithm == "chatts":
            return {
                "method": "chatts",
                "chatts_model_path": model, # æ³¨æ„ï¼šè¿™æ˜¯ä¼ ç»™ run.py çš„ --chatts_model_path
                "chatts_enabled": True
            }
        elif algorithm == "adtk_hbos":
            return {
                "method": "adtk_hbos",
                "chatts_enabled": False
            }
        else:
            return {"method": algorithm}
    
    def _run_single_inference(
        self, 
        input_file: str, 
        algorithm: str,
        args: Dict
    ) -> Dict:
        """æ‰§è¡Œå•ä¸ªæ–‡ä»¶æ¨ç†"""
        # ä½¿ç”¨ Python è§£é‡Šå™¨ï¼ˆç»Ÿä¸€æ¨¡å¼ä½¿ç”¨ PYTHON_UNIFIEDï¼‰
        python_exe = settings.PYTHON_UNIFIED if settings.USE_LOCAL_MODULES else settings.PYTHON_ILABEL
        cmd = [
            python_exe, str(self.run_script),
            "--input", input_file,
            "--method", algorithm
        ]
        
        # å¤„ç†å‚æ•°æ˜ å°„
        # 1. å¿…éœ€å‚æ•°
        # é»˜è®¤é™é‡‡æ ·ç‚¹æ•°ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥ n_downsampleï¼Œåˆ™ä½¿ç”¨ settings çš„é»˜è®¤å€¼
        if "n_downsample" not in args:
             cmd.extend(["--n_downsample", str(settings.DEFAULT_DOWNSAMPLE_POINTS)])
        
        # 2. éå† args æ·»åŠ å‚æ•°
        for k, v in args.items():
            if v is None or v == "":
                continue
            
            # ç‰¹æ®Šå¤„ç†å†…éƒ¨æ ‡è®°
            if k == "chatts_enabled":
                cmd.append("--use-chatts")
                continue
            if k == "model_path": # å…¼å®¹æ—§ä»£ç ï¼Œè™½ç„¶ä¸Šé¢æ”¹æˆäº† chatts_model_path
                 cmd.extend(["--model", str(v)])
                 continue

            # å¤„ç†å¸ƒå°”å€¼å‚æ•° (ä¾‹å¦‚ --chatts_load_in_4bit)
            # æ³¨æ„ï¼šrun.py ä¸­æŸäº›å¸ƒå°”å‚æ•°å¯èƒ½æ˜¯æ¥æ”¶å­—ç¬¦ä¸² "true"/"false" æˆ– action="store_true"
            # æ ¹æ® default_params.json åˆ†æï¼Œå¤§éƒ¨åˆ†æ˜¯å­—ç¬¦ä¸²ç±»å‹çš„ true/false æˆ– auto
            
            # å°†ä¸‹åˆ’çº¿è½¬æ¢ä¸ºè¿å­—ç¬¦ï¼Œä¾‹å¦‚ chatts_load_in_4bit -> --chatts-load-in-4bit
            arg_name = f"--{k.replace('_', '-')}"
            
            # é˜²æ­¢é‡å¤æ·»åŠ  method
            if arg_name == "--method":
                continue
            
            cmd.extend([arg_name, str(v)])
        
        try:
            result = subprocess.run(
                cmd,
                cwd=str(self.project_path),
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            # å°è¯•è§£æè¾“å‡ºä¸º JSON
            output = result.stdout.strip()
            try:
                parsed = json.loads(output)
            except json.JSONDecodeError:
                parsed = {"raw_output": output}
            
            return {
                "file": input_file,
                "success": result.returncode == 0,
                "result": parsed
            }
        except subprocess.TimeoutExpired:
            return {"file": input_file, "success": False, "error": "è¶…æ—¶"}
        except Exception as e:
            return {"file": input_file, "success": False, "error": str(e)}
    
    def run_batch_inference_streaming(
        self,
        task_id: str,
        model: str,
        algorithm: str,
        input_files: List[str],
        **kwargs
    ):
        """
        æ‰§è¡Œæ‰¹é‡æ¨ç†ä»»åŠ¡ï¼ˆæµå¼è¾“å‡ºç‰ˆï¼‰
        ä½¿ç”¨ç”Ÿæˆå™¨ yield å®æ—¶è¾“å‡ºæ—¥å¿—
        """
        if not self.run_script.exists():
            yield f"âŒ è„šæœ¬ä¸å­˜åœ¨: {self.run_script}"
            return
        
        # æ ¹æ®ç®—æ³•é€‰æ‹©åŸºç¡€å‚æ•°
        algorithm_args = self._build_algorithm_args(algorithm, model)
        algorithm_args.update(kwargs)
        
        total_files = len(input_files)
        yield f"ğŸš€ Starting batch inference for {total_files} files...\n"
        
        success_count = 0
        failed_count = 0
        errors = []
        
        for idx, input_file in enumerate(input_files, 1):
            if task_id in self.cancelled_tasks:
                yield "ğŸ›‘ Task cancelled by user.\n"
                break
                
            yield f"\nğŸ“„ **Processing file ({idx}/{total_files}):** `{Path(input_file).name}`\n"
            
            try:
                # æ‰§è¡Œå•ä¸ªæ–‡ä»¶æ¨ç†ï¼ˆæµå¼ï¼‰
                file_success = False
                for log in self._run_single_inference_streaming(
                    task_id,
                    input_file, 
                    algorithm, 
                    algorithm_args
                ):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæˆæ ‡è®°ï¼ˆè‡ªå®šä¹‰åè®®ï¼Œæˆ–è€…ä»…ä½œä¸ºæ—¥å¿—è¾“å‡ºï¼‰
                    if isinstance(log, dict) and "success" in log:
                         file_success = log["success"]
                         if not file_success:
                             errors.append({"file": input_file, "error": log.get("error", "Unknown error")})
                    else:
                        yield log
                
                if file_success:
                    success_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                failed_count += 1
                errors.append({"file": input_file, "error": str(e)})
                yield f"âŒ Error processing {input_file}: {str(e)}\n"
        
        # æ±‡æ€»ç»“æœ
        yield f"\n\n---\nâœ… **Batch Inference Completed**\n"
        yield f"- Success: {success_count}\n"
        yield f"- Failed: {failed_count}\n"
        
        if errors:
            yield "\n**Errors:**\n"
            for e in errors:
                yield f"- {Path(e['file']).name}: {e['error']}\n"

    def _run_single_inference_streaming(
        self, 
        task_id: str,
        input_file: str, 
        algorithm: str,
        args: Dict
    ):
        """æ‰§è¡Œå•ä¸ªæ–‡ä»¶æ¨ç†ï¼ˆæµå¼ï¼‰"""
        # æ„å»ºå‘½ä»¤ï¼ˆä¸ _run_single_inference ä¿æŒä¸€è‡´ï¼‰
        python_exe = settings.PYTHON_UNIFIED if settings.USE_LOCAL_MODULES else settings.PYTHON_ILABEL
        cmd = [
            python_exe, str(self.run_script),
            "--input", input_file,
            "--method", algorithm
        ]
        
        # å¤„ç†å‚æ•°æ˜ å°„
        if "n_downsample" not in args:
             cmd.extend(["--n_downsample", str(settings.DEFAULT_DOWNSAMPLE_POINTS)])
        
        # ç‰¹æ®Šå¤„ç† ChatTS æ¨¡å‹å‚æ•°
        if algorithm == "chatts":
            # LoRA Adapter
            if args.get("lora_adapter_path"):
                 cmd.extend(["--chatts_lora_adapter_path", str(args["lora_adapter_path"])])
            # Base Model (å¦‚æœæœªæŒ‡å®šï¼Œrun.py ä¼šä½¿ç”¨é»˜è®¤å€¼ï¼Œè¿™é‡Œæ˜¾å¼ä¼ é€’æ›´å®‰å…¨)
            if args.get("base_model_path"):
                 cmd.extend(["--chatts_model_path", str(args["base_model_path"])])
            
        for k, v in args.items():
            if v is None or v == "": continue
            
            # è·³è¿‡å·²å¤„ç†çš„å‚æ•°
            if k in ["model", "base_model_path", "lora_adapter_path"]: continue
            if k == "chatts_enabled": continue 
            
            # ç‰¹æ®Šå¤„ç†å¸ƒå°”å€¼
            if k == "timer_streaming":
                if str(v).lower() == "true":
                    cmd.extend(["--timer_streaming", "True"])
                continue
            
            # å¸¸è§„å‚æ•°å¤„ç†
            arg_name = f"--{k}"
            cmd.extend([arg_name, str(v)])
            
        yield f"Running command: `{' '.join(cmd[:10])}...`\n"

        process = None
        try:
            # ä½¿ç”¨ Popen è·å–å®æ—¶è¾“å‡º
            process = subprocess.Popen(
                cmd,
                cwd=str(self.project_path),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )
            
            # æ³¨å†Œè¿›ç¨‹
            if task_id:
                self.active_processes[task_id] = process
            
            output_lines = []
            
            # å®æ—¶è¯»å–æ—¥å¿—
            for line in iter(process.stdout.readline, ''):
                if line:
                    stripped_line = line.rstrip()
                    output_lines.append(stripped_line)
                    
                    # æ•è·ç»“æœæ–‡ä»¶ç›®å½•
                    # Log format: "Saving results to: /path/to/dir"
                    if "Saving results to:" in stripped_line:
                        parts = stripped_line.split("Saving results to:")
                        if len(parts) >= 2:
                            result_dir = parts[-1].strip()
                            # å­˜å‚¨ç›®å½•ä¾›åç»­ä½¿ç”¨
                            if not hasattr(self, '_current_result_dir'):
                                self._current_result_dir = result_dir

                    # æ•è·å®é™…ä¿å­˜çš„æ–‡ä»¶å
                    # Log format: "ä¿å­˜ç»“æœ: filename.csv"
                    if "ä¿å­˜ç»“æœ" in stripped_line and ".csv" in stripped_line:
                        parts = stripped_line.split(":")
                        if len(parts) >= 2:
                            filename = parts[-1].strip()
                            # æ„å»ºå®Œæ•´è·¯å¾„
                            if hasattr(self, '_current_result_dir'):
                                full_path = f"{self._current_result_dir}/{filename}"
                            else:
                                # é»˜è®¤ç›®å½•
                                full_path = f"/home/share/results/data/global/chatts/{filename}"
                            yield {"file_path": full_path, "file_name": filename}

                    # è¿‡æ»¤å¹¶æ ¼å¼åŒ–æœ‰ç”¨ä¿¡æ¯
                    # 1. è¿›åº¦æ¡ (tqdm)
                    if "%" in stripped_line or "it/s" in stripped_line:
                        # å¯¹äºè¿›åº¦æ¡ï¼Œä½¿ç”¨è¡Œå†…ä»£ç å—æˆ–ç‰¹å®šæ ¼å¼ï¼Œé¿å…åˆ·å±
                         if "ChatTS å¤„ç†è¿›åº¦" in stripped_line or "Loading" in stripped_line:
                             yield f"> {stripped_line}\n"
                         continue

                    # 2. å…³é”®çŠ¶æ€ä¿¡æ¯
                    if stripped_line.startswith("[ChatTS]") or "Data shape" in stripped_line or "Saving results" in stripped_line:
                         yield f"- {stripped_line}\n"
                    elif "Error" in stripped_line or "Exception" in stripped_line:
                         yield f"âŒ **{stripped_line}**\n"
            
            process.wait()
            
            # è¿›ç¨‹ç»“æŸåç§»é™¤
            if task_id and task_id in self.active_processes:
                del self.active_processes[task_id]

            # è§£ææœ€åçš„ç»“æœï¼ˆä»…ä¸ºäº†åˆ¤æ–­æˆåŠŸä¸å¦ï¼Œä¸ç”¨äºè¿”å›å¤§é‡æ•°æ®ï¼‰
            # æ³¨æ„ï¼šæµå¼æ¨¡å¼ä¸‹æˆ‘ä»¬æ— æ³•åƒ run_batch_inference é‚£æ ·æ–¹ä¾¿åœ°è¿”å›ç»“æ„åŒ–ç»“æœ
            # è¿™é‡Œæˆ‘ä»¬åªè¿”å›ä¸€ä¸ªçŠ¶æ€æ ‡è®°
            if process.returncode == 0:
                yield {"success": True}
                yield "âœ… Finished processing file.\n"
            else:
                yield {"success": False, "error": f"Process exited with code {process.returncode}"}
                yield f"âŒ Process failed with code {process.returncode}\n"
                # è¾“å‡ºæœ€åå‡ è¡Œæ—¥å¿—ä½œä¸ºé”™è¯¯ä¸Šä¸‹æ–‡
                yield "```\n" + "\n".join(output_lines[-10:]) + "\n```\n"
                
        except Exception as e:
            yield {"success": False, "error": str(e)}
            yield f"âŒ Execution error: {str(e)}\n"

