"""
Gradio ç»Ÿä¸€ç®¡ç†ç•Œé¢
åŒ…å«ï¼šæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒã€æ¨¡å‹å¯¹æ¯”
"""
import gradio as gr
from pathlib import Path
from typing import List, Dict, Optional
import json
import time
import pandas as pd
import tempfile
import os

from configs.settings import settings
from src.adapters.chatts_training import ChatTSTrainingAdapter
from src.adapters.data_processing import DataProcessingAdapter
from src.adapters.check_outlier import CheckOutlierAdapter
from src.utils.iotdb_config import load_iotdb_config


# åˆå§‹åŒ–é€‚é…å™¨
training_adapter = ChatTSTrainingAdapter()
data_adapter = DataProcessingAdapter()
inference_adapter = CheckOutlierAdapter()

# ä¸ºäº†å…¼å®¹æ€§ä¿ç•™æ—§å˜é‡å
adapter = training_adapter

# ç»“æœæ–‡ä»¶ç›®å½•ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ï¼‰
RESULTS_BASE_PATH = Path(settings.DATA_INFERENCE_DIR)

# ç»Ÿä¸€æ•°æ®æºï¼šä½¿ç”¨ data_adapter çš„å®é™…è·¯å¾„
# æ³¨æ„ï¼šè¿™é‡Œä¸å†ç¡¬ç¼–ç è·¯å¾„ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸æ•°æ®è·å–é¡µé¢ç›¸åŒçš„è·¯å¾„

# æ–‡ä»¶ååˆ°å®Œæ•´è·¯å¾„çš„æ˜ å°„ (ç”¨äºåœ¨UIæ˜¾ç¤ºæ–‡ä»¶åï¼Œå†…éƒ¨ä½¿ç”¨å®Œæ•´è·¯å¾„)
_unified_file_mapping: Dict[str, str] = {}


def get_unified_file_list() -> List[str]:
    """
    è·å–ç»Ÿä¸€çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸æ•°æ®è·å–é¡µé¢ç›¸åŒæ•°æ®æºï¼‰
    è¿”å›å®Œæ•´è·¯å¾„åˆ—è¡¨
    """
    global _unified_file_mapping
    all_files = []
    _unified_file_mapping.clear()
    
    # ä½¿ç”¨ä¸æ•°æ®è·å–é¡µé¢ç›¸åŒçš„æ•°æ®è·¯å¾„
    data_path = data_adapter.data_path
    if data_path.exists():
        for f in data_path.glob("*.csv"):
            if f.exists():
                # è¿‡æ»¤æ‰æ¨ç†ç»“æœæ–‡ä»¶ (global_chatts_ å¼€å¤´, chatts_ å¼€å¤´, timer_ å¼€å¤´, ä»¥åŠåŒ…å« _trend_resid çš„æ–‡ä»¶)
                if f.name.startswith(("global_chatts_", "chatts_", "_chatts_", "timer_")):
                    continue
                if "_trend_resid" in f.name:
                    continue
                full_path = str(f)
                all_files.append(full_path)
                _unified_file_mapping[f.name] = full_path
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    def safe_mtime(p):
        try:
            return Path(p).stat().st_mtime
        except OSError:
            return 0
    
    all_files.sort(key=safe_mtime, reverse=True)
    return all_files[:50]  # æœ€å¤šè¿”å› 50 ä¸ª


def get_unified_file_names() -> List[str]:
    """è·å–ç»Ÿä¸€æ–‡ä»¶åˆ—è¡¨çš„æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰"""
    # ç¡®ä¿æ˜ å°„å·²æ›´æ–°
    get_unified_file_list()
    return list(_unified_file_mapping.keys())


def resolve_filenames_to_paths(filenames: List[str]) -> List[str]:
    """å°†æ–‡ä»¶ååˆ—è¡¨è½¬æ¢ä¸ºå®Œæ•´è·¯å¾„åˆ—è¡¨"""
    global _unified_file_mapping
    if not _unified_file_mapping:
        get_unified_file_list()
    
    paths = []
    for name in filenames:
        if name in _unified_file_mapping:
            paths.append(_unified_file_mapping[name])
        elif Path(name).exists():
            # å¦‚æœå·²ç»æ˜¯å®Œæ•´è·¯å¾„
            paths.append(name)
    return paths



def get_existing_results(method: str = "chatts") -> List[str]:
    """è·å–å·²æœ‰çš„ç»“æœæ–‡ä»¶åˆ—è¡¨"""
    results_dir = RESULTS_BASE_PATH / method
    if not results_dir.exists():
        return []
    
    # è·å–æ‰€æœ‰ CSV æ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    csv_files = []
    for f in results_dir.glob("*.csv"):
        if f.exists(): # åªåŒ…å«å­˜åœ¨çš„æ–‡ä»¶ï¼ˆè¿‡æ»¤æ‰æ–­è£‚çš„ç¬¦å·é“¾æ¥ï¼‰
            csv_files.append(f)
            
    # å®‰å…¨æ’åºï¼šå¦‚æœ stat å¤±è´¥ï¼ˆä¾‹å¦‚ç«æ€æ¡ä»¶ï¼‰ï¼Œä½¿ç”¨ 0 ä½œä¸ºæ—¶é—´æˆ³
    def safe_get_mtime(p):
        try:
            return p.stat().st_mtime
        except OSError:
            return 0
            
    csv_files.sort(key=safe_get_mtime, reverse=True)
    return [str(f) for f in csv_files[:20]]  # æœ€å¤šè¿”å› 20 ä¸ª


def delete_selected_files(method: str, filenames: List[str]) -> tuple:
    """æ‰¹é‡åˆ é™¤é€‰ä¸­çš„ç»“æœæ–‡ä»¶"""
    if not filenames:
        return (
            gr.CheckboxGroup(choices=get_result_filenames(method)), 
            gr.File(value=None), 
            "âš ï¸ è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶"
        )
    
    results_dir = RESULTS_BASE_PATH / method
    deleted_count = 0
    errors = []
    
    for fname in filenames:
        file_path = results_dir / fname.strip()  # Strip whitespace just in case
        print(f"DEBUG: Attempting to delete {file_path}")
        
        # å¤„ç†ç¬¦å·é“¾æ¥å’Œæ™®é€šæ–‡ä»¶
        # is_file() å¯¹ç¬¦å·é“¾æ¥å¦‚æœæŒ‡å‘å­˜åœ¨æ–‡ä»¶åˆ™ä¸ºçœŸ
        # is_symlink() åˆ¤æ–­æ˜¯å¦ä¸ºç¬¦å·é“¾æ¥
        # exists() å¦‚æœæ˜¯æ–­è£‚ç¬¦å·é“¾æ¥åˆ™ä¸ºå‡
        
        try:
            # å°è¯•åˆ é™¤ï¼ˆå¦‚æœæ˜¯ç¬¦å·é“¾æ¥åˆ™åˆ é™¤é“¾æ¥ï¼Œå¦‚æœæ˜¯æ–‡ä»¶åˆ™åˆ é™¤æ–‡ä»¶ï¼‰
            if file_path.is_symlink() or file_path.exists():
                file_path.unlink()
                deleted_count += 1
                print(f"DEBUG: Deleted {file_path}")
                
                # åŒæ­¥åˆ é™¤å…³è”çš„ç¬¦å·é“¾æ¥ (åœ¨ downsampled å’Œç”¨æˆ·ç›®å½•ä¸­)
                try:
                    # 1. åˆ é™¤ downsampled ç›®å½•ä¸‹çš„åŒåé“¾æ¥
                    symlink_path = Path(settings.DATA_DOWNSAMPLED_DIR) / fname.strip()
                    if symlink_path.is_symlink():
                        symlink_path.unlink()
                        print(f"DEBUG: Deleted symlink {symlink_path}")
                        
                    # 2. åˆ é™¤ Annotator ç”¨æˆ·ç›®å½•ä¸‹çš„åŒåé“¾æ¥
                    annotator_users_file = Path(settings.DATA_PROCESSING_PATH).parent / "annotator" / "backend" / "users.json"
                    if annotator_users_file.exists():
                        import json
                        with open(annotator_users_file, 'r') as f:
                            users = json.load(f)
                        for u_info in users.values():
                            if 'data_path' in u_info:
                                u_link = Path(u_info['data_path']) / fname.strip()
                                if u_link.is_symlink():
                                    u_link.unlink()
                                    print(f"DEBUG: Deleted user symlink {u_link}")
                except Exception as e_link:
                    print(f"DEBUG: Error cleaning up symlinks: {e_link}")
            else:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦æ˜¯â€œæ–­è£‚çš„ç¬¦å·é“¾æ¥â€ï¼ˆexists()è¿”å›Falseä½†é“¾æ¥æœ¬èº«å­˜åœ¨ï¼‰
                # Path.is_symlink() å³ä½¿ç›®æ ‡ä¸å­˜åœ¨ä¹Ÿè¿”å› True
                if file_path.is_symlink():
                     file_path.unlink()
                     deleted_count += 1
                     print(f"DEBUG: Deleted broken symlink {file_path}")
                else:
                     print(f"DEBUG: File not found {file_path}")
                     # æ­¤æ—¶å¯èƒ½ç”¨æˆ·é€‰äº†ä¸€ä¸ªå·²ç»ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼ˆç¼“å­˜é—®é¢˜ï¼‰ï¼Œä¸æŠ¥é”™ï¼Œåªè®°å½•
        except Exception as e:
            errors.append(f"{fname}: {str(e)}")
            print(f"DEBUG: Error deleting {file_path}: {e}")
    
    # åˆ·æ–°åˆ—è¡¨
    time.sleep(0.5)  # ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
    new_choices = get_result_filenames(method)
    
    status_msg = f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶"
    if errors:
        status_msg += f"\nâŒ é”™è¯¯: {'; '.join(errors)}"
        
    return (
        gr.update(choices=new_choices, value=[]), 
        gr.update(value=None, label="ğŸ“¥ ä¸‹è½½åŒºåŸŸ (è¯·å…ˆé€‰æ‹©æ–‡ä»¶)"),
        status_msg
    )


def prepare_download_files(method: str, filenames: List[str]) -> tuple:
    """å‡†å¤‡ä¸‹è½½é€‰ä¸­çš„æ–‡ä»¶"""
    if not filenames:
        return None, "âš ï¸ è¯·å…ˆé€‰æ‹©è¦ä¸‹è½½çš„æ–‡ä»¶"
    
    results_dir = RESULTS_BASE_PATH / method
    paths = []
    for fname in filenames:
        p = results_dir / fname.strip()
        if p.exists():
            paths.append(str(p))
            
    if not paths:
        return gr.update(value=None), "âŒ æœªæ‰¾åˆ°é€‰ä¸­çš„æ–‡ä»¶"
        
    return (
        gr.update(value=paths, label="ğŸ“¥ ç‚¹å‡»æ­¤å¤„ä¸‹è½½ / Click to Download", visible=True),
        f"âœ… å·²å‡†å¤‡å¥½ {len(paths)} ä¸ªæ–‡ä»¶ï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹ä¸‹è½½åŒºåŸŸè¿›è¡Œä¸‹è½½"
    )


def get_result_filenames(method: str = "chatts") -> List[str]:
    """è·å–ç»“æœæ–‡ä»¶ååˆ—è¡¨ï¼ˆç”¨äºä¸‹æ‹‰æ¡†ï¼‰"""
    results_dir = RESULTS_BASE_PATH / method  # æ–°ç›®å½•ç»“æ„ï¼š/home/share/data/inference/{method}
    if not results_dir.exists():
        return []
    
    csv_files = []
    for f in results_dir.glob("*.csv"):
        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ˜¯æ–­è£‚çš„ç¬¦å·é“¾æ¥ï¼Œf.exists() ä¼šè¿”å› False
        if f.exists() or f.is_symlink():
            csv_files.append(f)
            
    def safe_get_mtime(p):
        try:
            return p.stat().st_mtime
        except OSError:
            return 0

    csv_files.sort(key=safe_get_mtime, reverse=True)
    return [f.name for f in csv_files[:20]]


def delete_result_file(method: str, filename: str) -> tuple:
    # å·²å¼ƒç”¨ï¼Œä½¿ç”¨ delete_selected_files
    pass


def get_training_configs() -> List[str]:
    """è·å–è®­ç»ƒé…ç½®åˆ—è¡¨"""
    configs = adapter.list_configs()
    return [c["name"] for c in configs]


def get_trained_models() -> List[str]:
    """è·å–å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨"""
    models = adapter.list_models()
    return [m["name"] for m in models]


def get_model_info(model_name: str) -> str:
    """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
    if not model_name:
        return "è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹"
    
    models = adapter.list_models()
    model = next((m for m in models if m["name"] == model_name), None)
    
    if not model:
        return "æ¨¡å‹ä¸å­˜åœ¨"
    
    info_lines = [
        f"**æ¨¡å‹åç§°**: {model['name']}",
        f"**ç±»å‹**: {model.get('type', 'unknown')}",
        f"**æ£€æŸ¥ç‚¹**: {', '.join(model.get('checkpoints', []))}",
        f"**è®­ç»ƒæ­¥æ•°**: {model.get('global_step', 'N/A')}",
    ]
    
    # è®­ç»ƒç»“æœ
    train_results = model.get("train_results", {})
    if train_results:
        info_lines.append(f"**è®­ç»ƒ Loss**: {train_results.get('train_loss', 'N/A'):.4f}")
        info_lines.append(f"**è®­ç»ƒæ—¶é•¿**: {train_results.get('train_runtime', 'N/A'):.1f}s")
    
    return "\n\n".join(info_lines)


def get_loss_plot(model_name: str):
    """è·å– Loss æ›²çº¿å›¾"""
    if not model_name:
        return None
    
    models = adapter.list_models()
    model = next((m for m in models if m["name"] == model_name), None)
    
    if not model or not model.get("loss_image"):
        return None
    
    loss_image = model.get("loss_image")
    if Path(loss_image).exists():
        return loss_image
    return None


def get_comparison_plot(model_names: List[str]):
    """è·å–å¤šä¸ªæ¨¡å‹çš„ Loss å¯¹æ¯”å›¾ (ä½¿ç”¨ Matplotlib åŠ¨æ€ç”Ÿæˆ)"""
    if not model_names or len(model_names) == 0:
        return None
    
    import matplotlib.pyplot as plt
    import pandas as pd
    
    plt.figure(figsize=(10, 6))
    
    for name in model_names:
        models = adapter.list_models()
        model = next((m for m in models if m["name"] == name), None)
        if not model:
            continue
            
        logs = adapter.get_training_log(model["path"])
        if not logs:
            continue
            
        df = pd.DataFrame([{"step": l.get("current_steps", 0), "loss": l.get("loss")} for l in logs if "loss" in l])
        if not df.empty:
            plt.plot(df["step"], df["loss"], label=name)
            
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.title("Model Comparison: Training Loss")
    plt.legend()
    plt.grid(True)
    
    # ä¿å­˜åˆ°é¡¹ç›®æœ¬åœ°ä¸´æ—¶ç›®å½•ï¼Œé¿å…ç³»ç»Ÿ /tmp æƒé™é—®é¢˜
    import uuid
    
    temp_dir = Path("temp_images")
    temp_dir.mkdir(exist_ok=True)
    
    output_path = temp_dir / f"compare_{uuid.uuid4().hex[:8]}.png"
    plt.savefig(str(output_path))
    plt.close()
    
    return str(output_path)


# ==================== æ•°æ®è·å–è¾…åŠ©å‡½æ•° ====================

def get_datasets_table() -> pd.DataFrame:
    """è·å–æ•°æ®é›†åˆ—è¡¨å¹¶è¿”å› DataFrame"""
    datasets = data_adapter.list_datasets()
    if not datasets:
        return pd.DataFrame(columns=["æ–‡ä»¶å", "å¤§å° (KB)", "ä¿®æ”¹æ—¶é—´"])
    
    from datetime import datetime
    rows = []
    for d in datasets:
        rows.append({
            "æ–‡ä»¶å": d["filename"],
            "å¤§å° (KB)": round(d["size_bytes"] / 1024, 2),
            "ä¿®æ”¹æ—¶é—´": datetime.fromtimestamp(d["modified_time"]).strftime("%Y-%m-%d %H:%M")
        })
    return pd.DataFrame(rows)


def get_dataset_names() -> List[str]:
    """è·å–æ•°æ®é›†æ–‡ä»¶ååˆ—è¡¨"""
    datasets = data_adapter.list_datasets()
    return [d["filename"] for d in datasets]


def delete_selected_dataset(filename: str):
    """åˆ é™¤é€‰ä¸­çš„æ•°æ®é›†"""
    print(f"[DEBUG] delete_selected_dataset called with: '{filename}'")
    if not filename:
        return get_datasets_table(), gr.Dropdown(choices=get_dataset_names(), value=None), "âŒ No dataset selected"
    
    result = data_adapter.delete_dataset(filename)
    if result.get("success"):
        # åˆ·æ–°åˆ—è¡¨
        new_table = get_datasets_table()
        new_choices = get_dataset_names()
        return new_table, gr.Dropdown(choices=new_choices, value=None), f"âœ… Deleted: {filename}"
    else:
        return get_datasets_table(), gr.Dropdown(choices=get_dataset_names()), f"âŒ {result.get('error')}"


def preview_dataset(filename: str) -> tuple:
    """é¢„è§ˆæ•°æ®é›†ï¼Œè¿”å› (è¡¨æ ¼æ•°æ®, åˆ—é€‰æ‹©å™¨æ›´æ–°, æ›²çº¿å›¾)"""
    print(f"[DEBUG] preview_dataset called with filename: '{filename}'")
    
    if isinstance(filename, list):
        filename = filename[0] if filename else None
    
    if not filename:
        print("[DEBUG] Empty filename, returning empty")
        return [], gr.CheckboxGroup(choices=[], value=[]), None
    
    try:
        # è·å–é¢„è§ˆæ•°æ®
        print(f"[DEBUG] Calling preview_csv for: {filename}")
        data = data_adapter.preview_csv(filename, limit=5000)
        print(f"[DEBUG] preview_csv returned {len(data)} records")
        
        df = pd.DataFrame(data)
        print(f"[DEBUG] DataFrame created: shape={df.shape}, columns={df.columns.tolist()}")
        
        # è¿‡æ»¤æ‰ Unnamed å’Œ category åˆ—
        df = df.loc[:, ~df.columns.str.contains('^Unnamed|^category', case=False)]
        print(f"[DEBUG] After filtering: shape={df.shape}, columns={df.columns.tolist()}")
        
        # è·å–æ•°å€¼åˆ—ä½œä¸ºå¯é€‰é¡¹
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        print(f"[DEBUG] Numeric columns: {numeric_cols}")
        
        # é»˜è®¤é€‰ä¸­ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—
        default_selected = numeric_cols[:1] if numeric_cols else []
        print(f"[DEBUG] Default selected: {default_selected}")
        
        # ç”Ÿæˆé»˜è®¤æ›²çº¿å›¾
        plot_path = generate_plot(df, filename, default_selected)
        print(f"[DEBUG] Plot generated: {plot_path}")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œç¡®ä¿ Gradio 6.x å…¼å®¹
        # ä½¿ç”¨ values åˆ—è¡¨ + headers çš„æ–¹å¼
        table_data = df.values.tolist()
        headers = df.columns.tolist()
        print(f"[DEBUG] Table data rows: {len(table_data)}, headers: {headers}")
        
        return gr.Dataframe(value=table_data, headers=headers), gr.CheckboxGroup(choices=numeric_cols, value=default_selected), plot_path
    except Exception as e:
        import traceback
        print(f"[DEBUG ERROR] Exception: {e}")
        traceback.print_exc()
        return [], gr.CheckboxGroup(choices=[], value=[]), None


def generate_plot(df: pd.DataFrame, filename: str, selected_cols: list):
    """æ ¹æ®é€‰æ‹©çš„åˆ—ç”Ÿæˆæ›²çº¿å›¾"""
    if df.empty or not selected_cols:
        return None
    
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 4))
    
    for col in selected_cols:
        if col in df.columns:
            plt.plot(df.index, df[col], label=col, alpha=0.8)
    
    plt.xlabel("Index")
    plt.ylabel("Value")
    plt.title(f"Data Preview: {filename}")
    if selected_cols:
        plt.legend()
    plt.grid(True, alpha=0.3)
    
    temp_dir = Path("temp_images")
    temp_dir.mkdir(exist_ok=True)
    import uuid
    plot_path = temp_dir / f"preview_{uuid.uuid4().hex[:8]}.png"
    plt.savefig(str(plot_path), dpi=100, bbox_inches='tight')
    plt.close()
    
    return str(plot_path)


def update_plot_from_selection(filename: str, selected_cols: list):
    """æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„åˆ—æ›´æ–°æ›²çº¿å›¾"""
    if not filename or not selected_cols:
        return None
    
    try:
        data = data_adapter.preview_csv(filename, limit=5000)
        df = pd.DataFrame(data)
        df = df.loc[:, ~df.columns.str.contains('^Unnamed|^category', case=False)]
        return generate_plot(df, filename, selected_cols)
    except:
        return None


def start_acquire_task(
    source: str, 
    host: str,
    port: str,
    user: str,
    password: str,
    point_name: str,
    start_time: str,
    end_time: str,
    target_points: int
):
    """å¯åŠ¨æ•°æ®é‡‡é›†ä»»åŠ¡ï¼ˆæµå¼è¾“å‡ºæ—¥å¿—ï¼‰"""
    if not source:
        yield "âŒ Please enter IoTDB source path"
        return
    
    # ä½¿ç”¨æµå¼è¾“å‡ºç‰ˆæœ¬
    for log in data_adapter.run_acquire_task_streaming(
        task_id="manual",
        source=source,
        host=host,
        port=port,
        user=user,
        password=password,
        point_name=point_name,
        target_points=int(target_points),
        start_time=start_time,
        end_time=end_time
    ):
        yield log


# ==================== æ¨ç†ç›‘æ§è¾…åŠ©å‡½æ•° ====================

def get_algorithms() -> List[str]:
    """è·å–å¯ç”¨ç®—æ³•åˆ—è¡¨"""
    return ["chatts", "adtk_hbos", "ensemble", "timer"]


def get_inference_models() -> List[str]:
    """è·å–å¯ç”¨äºæ¨ç†çš„æ¨¡å‹åˆ—è¡¨"""
    # è¿‡æ»¤ lora æ¨¡å‹ï¼ˆå‡è®¾ ChatTS æ¨ç†ä¸»è¦ç”¨ LoRAï¼‰
    models = training_adapter.list_models()
    return [m["path"] for m in models] # ç›´æ¥è¿”å›è·¯å¾„ï¼Œæ–¹ä¾¿ adapter å¤„ç†

def toggle_algo_params(algorithm: str):
    """æ ¹æ®é€‰æ‹©çš„ç®—æ³•åˆ‡æ¢å‚æ•°ç»„å¯è§æ€§"""
    show_chatts = (algorithm == "chatts")
    show_timer = (algorithm == "timer")
    show_adtk = (algorithm == "adtk_hbos")
    return (
        gr.update(visible=show_chatts), 
        gr.update(visible=show_timer), 
        gr.update(visible=show_adtk)
    )

def start_inference_task(
    algorithm: str, 
    base_model_path: str,
    lora_adapter_path: str,
    files: List[str],
    n_downsample: int,
    threshold: float,
    # ChatTS args
    load_in_4bit: str,
    prompt_template: str,
    max_new_tokens: int,
    chatts_device: str,
    chatts_use_cache: str,
    # Timer args
    timer_device: str,
    timer_lookback: int,
    timer_threshold_k: float,
    timer_method: str,
    timer_streaming: bool,
    # ADTK args
    adtk_bin_nums: int,
    adtk_hbos_ratio: float
):
    """å¯åŠ¨æ¨ç†ä»»åŠ¡"""
    if not algorithm:
        yield "âŒ è¯·é€‰æ‹©ç®—æ³•", "âŒ è¯·é€‰æ‹©ç®—æ³•"
        return
    if not files:
        yield "âŒ è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶", "âŒ è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶"
        return
    
    # å°†é€‰ä¸­çš„æ–‡ä»¶åè½¬æ¢ä¸ºå®Œæ•´è·¯å¾„ï¼ˆä½¿ç”¨ç»Ÿä¸€æ•°æ®æºæ˜ å°„ï¼‰
    file_paths = resolve_filenames_to_paths(files)
    
    if not file_paths:
        yield "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶", "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶"
        return
    
    import uuid
    task_id = str(uuid.uuid4())
    
    # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
    from datetime import datetime
    from src.db.database import SessionLocal, Task
    db = SessionLocal()
    try:
        task = Task(
            id=task_id,
            type="inference",
            status="running",
            config=json.dumps({
                "algorithm": algorithm,
                "files": files,
                "base_model_path": base_model_path,
                "lora_adapter_path": lora_adapter_path
            }),
            created_at=datetime.utcnow(),
            started_at=datetime.utcnow()
        )
        db.add(task)
        db.commit()
    except Exception as e:
        print(f"[DB Error] Failed to save task: {e}")
    finally:
        db.close()
    
    yield (
        f"ğŸš€ ä»»åŠ¡å·²å¯åŠ¨ (ID: {task_id[:8]})\\næ­£åœ¨å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...", 
        f"ğŸš€ ä»»åŠ¡å·²å¯åŠ¨ (ID: {task_id[:8]})",
        gr.update(visible=True), # Show stop button
        gr.update(visible=False), # Hide submit button
        task_id, # Return task_id to state
        None # download_files
    )
    
    try:
        # å‡†å¤‡é«˜çº§å‚æ•°
        advanced_args = {
            "n_downsample": n_downsample,
            "threshold": threshold,
            "base_model_path": base_model_path,
            "lora_adapter_path": lora_adapter_path, 
            # ChatTS
            "chatts_load_in_4bit": load_in_4bit,
            "chatts_prompt_template": prompt_template,
            "chatts_max_new_tokens": max_new_tokens,
            "chatts_device": chatts_device,
            "chatts_use_cache": chatts_use_cache,
            # Timer
            "timer_device": timer_device,
            "timer_lookback_length": timer_lookback,
            "timer_threshold_k": timer_threshold_k,
            "timer_method": timer_method,
            "timer_streaming": timer_streaming,
            # ADTK
            "bin_nums": adtk_bin_nums,
            "hbos_ratio": adtk_hbos_ratio
        }
        
        generated_files = []
        
        # æ‰§è¡Œæ¨ç†ï¼ˆæµå¼ï¼‰
        accumulated_log = ""
        for log_chunk in inference_adapter.run_batch_inference_streaming(
            task_id=task_id,
            model=lora_adapter_path, # å…¼å®¹æ—§æ¥å£å‘½åï¼Œå®é™…é€»è¾‘åœ¨ adapter ä¸­å·²å¤„ç†
            algorithm=algorithm,
            input_files=file_paths,
            **advanced_args
        ):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„è¿”å›
            if isinstance(log_chunk, dict) and "file_path" in log_chunk:
                # adapter è¿”å›äº†å®Œæ•´è·¯å¾„
                generated_files.append(log_chunk["file_path"])
            elif isinstance(log_chunk, dict) and "file_name" in log_chunk:
                # å…¼å®¹æ—§æ ¼å¼ï¼Œä»…æœ‰æ–‡ä»¶å
                generated_files.append(log_chunk["file_name"])
            elif isinstance(log_chunk, dict):
                 pass # å…¶ä»–ç»“æ„åŒ–æ¶ˆæ¯
            else:
                accumulated_log += log_chunk
                yield (
                    accumulated_log, 
                    "ğŸ”„ æ­£åœ¨æ‰§è¡Œ...",
                    gr.update(visible=True),
                    gr.update(visible=False),
                    task_id,
                    None
                )
        
        # ä»»åŠ¡ç»“æŸï¼Œå°è¯•æŸ¥æ‰¾ç”Ÿæˆçš„ç»“æœæ–‡ä»¶
        # å‡è®¾ä¿å­˜åœ¨ /home/share/results/data/<method> ä¸‹ï¼ŒæŒ‰æ—¶é—´æœ€æ–°æŸ¥æ‰¾ï¼Ÿ
        # è¿™æ¯”è¾ƒ hackyã€‚æ›´å¥½çš„æ–¹æ³•æ˜¯ adapter è¿”å›ã€‚
        # æˆ‘ä»¬åœ¨ adapter ä¸­å¢åŠ äº† yield {"file_name": ...} é€»è¾‘
        # è¿™é‡Œéœ€è¦å¤„ç†å®ƒã€‚
        
        # æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        db = SessionLocal()
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "completed"
                task.completed_at = datetime.utcnow()
                db.commit()
        except Exception as e:
            print(f"[DB Error] Failed to update task: {e}")
        finally:
            db.close()
        
        # è‡ªåŠ¨å°†ç»“æœæ–‡ä»¶é“¾æ¥åˆ°ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œä»¥ä¾¿æ ‡æ³¨å·¥å…·é»˜è®¤å¯è§
        try:
            # ç›®æ ‡ç›®å½•åˆ—è¡¨ï¼šdata_adapter ç›®å½• + Annotator ç”¨æˆ·ç›®å½•
            target_dirs = [data_adapter.data_path]
            
            # å°è¯•è¯»å– Annotator ç”¨æˆ·é…ç½®
            try:
                annotator_users_file = Path(settings.DATA_PROCESSING_PATH).parent / "annotator" / "backend" / "users.json"
                if annotator_users_file.exists():
                    import json
                    with open(annotator_users_file, 'r') as f:
                        users = json.load(f)
                    # éå†æ‰€æœ‰ç”¨æˆ·ï¼Œå°†ç»“æœé“¾æ¥åˆ°æ¯ä¸ªç”¨æˆ·çš„ data_path
                    for username, user_info in users.items():
                        if 'data_path' in user_info:
                            user_dir = Path(user_info['data_path'])
                            if user_dir.exists() and user_dir not in target_dirs:
                                target_dirs.append(user_dir)
            except Exception as e:
                print(f"[Auto-Link] Warning: Could not read annotator users.json: {e}")
            
            for res_file in generated_files:
                res_path = Path(res_file)
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„æˆ–æ–‡ä»¶åï¼Œå°è¯•åœ¨ç»“æœç›®å½•æŸ¥æ‰¾
                if not res_path.exists():
                    res_path = RESULTS_BASE_PATH / algorithm / res_file
                
                if res_path.exists():
                    for target_dir in target_dirs:
                        # é¿å…è‡ªå¼•ç”¨é“¾æ¥ (å½“ç›®æ ‡ç›®å½•å°±æ˜¯ç»“æœæ–‡ä»¶æ‰€åœ¨ç›®å½•æ—¶)
                        try:
                            if target_dir.resolve() == res_path.parent.resolve():
                                continue
                        except Exception:
                            pass

                        target_link = target_dir / res_path.name
                        try:
                            # å¦‚æœé“¾æ¥ä¸å­˜åœ¨æˆ–å·²æ–­è£‚ï¼Œé‡æ–°åˆ›å»º
                            if target_link.is_symlink() or target_link.exists():
                                target_link.unlink()
                            target_link.symlink_to(res_path)
                            print(f"[Auto-Link] Created symlink for {res_path.name} in {target_dir}")
                        except Exception as link_err:
                            print(f"[Auto-Link] Failed to link to {target_dir}: {link_err}")
        except Exception as e:
            print(f"[Auto-Link Error] Failed to link results: {e}")
        
        yield (
            accumulated_log + "\nâœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ", 
            "âœ… ä»»åŠ¡å®Œæˆ",
             gr.update(visible=False),
             gr.update(visible=True),
             task_id,
             generated_files # TODO: å¡«å…… output files if capture logic works perfectly
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        
        # æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        db = SessionLocal()
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "failed"
                task.error = str(e)
                task.completed_at = datetime.utcnow()
                db.commit()
        except Exception as db_e:
            print(f"[DB Error] Failed to update task: {db_e}")
        finally:
            db.close()
        
        yield (
            f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", 
            f"âŒ é”™è¯¯: {str(e)}",
            gr.update(visible=False),
            gr.update(visible=True),
            None,
            None
        )

def stop_task_action(task_id_state):
    """å®é™…æ‰§è¡Œåœæ­¢åŠ¨ä½œ"""
    print(f"DEBUG: Stop requested for task ID: {task_id_state}")
    if task_id_state:
        if inference_adapter.stop_inference_task(task_id_state):
            print(f"DEBUG: Stop successful for {task_id_state}")
            return "ğŸ›‘ ä»»åŠ¡å·²è¯·æ±‚åœæ­¢", gr.update(visible=False), gr.update(visible=True), None, None
        else:
            print(f"DEBUG: Stop failed for {task_id_state} (not found or error)")
            return f"âŒ åœæ­¢å¤±è´¥: ä»»åŠ¡ {task_id_state} ä¸å­˜åœ¨æˆ–å·²ç»“æŸ", gr.update(visible=True), gr.update(visible=False), task_id_state, None
    print("DEBUG: No active task ID found")
    return "âš ï¸ æ— æ´»åŠ¨ä»»åŠ¡", gr.update(visible=False), gr.update(visible=True), None, None


def get_task_status_table() -> pd.DataFrame:
    """è·å–ä»»åŠ¡çŠ¶æ€åˆ—è¡¨ (ä»æ•°æ®åº“è¯»å–)"""
    try:
        from src.db.database import SessionLocal, Task
        db = SessionLocal()
        tasks = db.query(Task).order_by(Task.created_at.desc()).limit(20).all()
        db.close()
        
        if not tasks:
            return pd.DataFrame(columns=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"])
        
        rows = []
        for t in tasks:
            rows.append({
                "ID": t.id[:8] + "...",
                "ç±»å‹": t.type,
                "çŠ¶æ€": t.status,
                "åˆ›å»ºæ—¶é—´": t.created_at.strftime("%H:%M:%S") if t.created_at else "N/A"
            })
        return pd.DataFrame(rows)
    except Exception as e:
        return pd.DataFrame({"é”™è¯¯": [str(e)]})


def clear_task_history() -> tuple:
    """æ¸…ç©ºä»»åŠ¡å†å²è®°å½•"""
    try:
        from src.db.database import SessionLocal, Task
        db = SessionLocal()
        deleted = db.query(Task).delete()
        db.commit()
        db.close()
        return pd.DataFrame(columns=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"]), f"âœ… å·²æ¸…ç©º {deleted} æ¡å†å²è®°å½•"
    except Exception as e:
        return get_task_status_table(), f"âŒ æ¸…ç©ºå¤±è´¥: {str(e)}"


def start_training(
    config_name: str,
    learning_rate: str,
    num_epochs: float,
    batch_size: int,
    lora_rank: int,
    lora_alpha: int,
    output_name: str,
    model_path: Optional[str] = None,
    dataset_name: Optional[str] = None
) -> str:
    """å¯åŠ¨è®­ç»ƒ (Quick Start)"""
    if not config_name:
        return "âŒ è¯·é€‰æ‹©è®­ç»ƒé…ç½® (æ¨¡æ¿)"
    
    if not output_name:
        return "âŒ è¯·è¾“å…¥è¾“å‡ºç›®å½•åç§°"
    
    # ç”Ÿæˆä»»åŠ¡ID
    import uuid
    task_id = f"job-{str(uuid.uuid4())[:8]}"
    
    try:
        # è°ƒç”¨åç«¯é€‚é…å™¨å¯åŠ¨è®­ç»ƒ
        result = training_adapter.run_training(
            task_id=task_id,
            config_name=config_name,
            version_tag=output_name,
            # Quick Start Overrides
            override_model_path=model_path,
            override_dataset=dataset_name,
            override_learning_rate=learning_rate,
            override_epochs=num_epochs,
            override_batch_size=batch_size,
            override_lora_rank=lora_rank,
            override_lora_alpha=lora_alpha
        )
        
        if result.get("success"):
            return f"""âœ… è®­ç»ƒä»»åŠ¡å·²æˆåŠŸå¯åŠ¨!
            
**ä»»åŠ¡ ID**: {task_id}
**è¾“å‡ºç›®å½•**: `{result.get('output_dir')}`
**åŸºç¡€æ¨¡å‹**: `{model_path or 'Default (from script)'}`
**æ•°æ®é›†**: `{dataset_name or 'Default (from script)'}`

æ­£åœ¨åå°è¿è¡Œä¸­... è¯·ç•™æ„æ—¥å¿—è¾“å‡ºæˆ–ç¨ååˆ·æ–°æ¨¡å‹åˆ—è¡¨ã€‚
"""
        else:
            return f"""âŒ å¯åŠ¨å¤±è´¥
            
**é”™è¯¯ä¿¡æ¯**: {result.get('error')}
"""
            
    except Exception as e:
        return f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"


def create_training_ui() -> gr.Blocks:
    """åˆ›å»ºç»Ÿä¸€ç®¡ç†ç•Œé¢ï¼ˆæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒï¼‰"""
    
    with gr.Blocks(title="TS-Iteration-Loop", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ”„ TS-Iteration-Loop æ—¶åºè¿­ä»£å¹³å°")
        gr.Markdown("æ•´åˆæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒçš„ç»Ÿä¸€ç®¡ç†ç•Œé¢")
        
        # ==================== æ•°æ®è·å– Tab ====================
        with gr.Tab("ğŸ“ æ•°æ®è·å–"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ•°æ®é›†åˆ—è¡¨")
                    datasets_table = gr.Dataframe(
                        value=get_datasets_table(),
                        label="å·²æœ‰æ•°æ®é›†",
                        interactive=False
                    )
                    refresh_datasets_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                    
                    gr.Markdown("### é¢„è§ˆæ•°æ®")
                    preview_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æ•°æ®é›†",
                        choices=get_dataset_names(),
                        interactive=True
                    )
                    with gr.Row():
                        delete_dataset_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop", size="sm")
                        delete_status = gr.Textbox(label="", visible=False)
                    
                    column_selector = gr.CheckboxGroup(
                        label="Select columns to plot",
                        choices=[],
                        interactive=True
                    )
                
                with gr.Column(scale=2):
                    gr.Markdown("### æ•°æ®é‡‡é›†é…ç½®")
                    
                    with gr.Accordion("IoTDB è¿æ¥é…ç½®", open=False):
                        # ä»å…±äº«é…ç½®åŠ è½½é»˜è®¤å€¼
                        _iotdb_cfg = load_iotdb_config()
                        with gr.Row():
                            host_input = gr.Textbox(label="Host", value=_iotdb_cfg.get("host", "192.168.199.185"))
                            port_input = gr.Textbox(label="Port", value=_iotdb_cfg.get("port", "6667"))
                        with gr.Row():
                            user_input = gr.Textbox(label="User", value=_iotdb_cfg.get("user", "root"))
                            pwd_input = gr.Textbox(label="Password", value=_iotdb_cfg.get("password", "root"), type="password")

                    gr.Markdown("### æŸ¥è¯¢å‚æ•°")
                    source_input = gr.Textbox(
                        label="IoTDB æºè·¯å¾„ (Path)",
                        placeholder="root.zhlh_202307_202412.ZHLH_4C_1216",
                        value="root.zhlh_202307_202412.ZHLH_4C_1216",
                        scale=2
                    )
                    
                    with gr.Row():
                         point_input = gr.Textbox(
                            label="ç‚¹ä½åç§° (Point Name)",
                            placeholder="FI_10401C.PV (ç•™ç©ºæŸ¥è¯¢æ‰€æœ‰*)",
                            value="FI_10401C.PV"
                        )
                    
                    with gr.Row():
                        start_time_input = gr.Textbox(label="å¼€å§‹æ—¶é—´", value="2023-07-18 12:00:00")
                        end_time_input = gr.Textbox(label="ç»“æŸæ—¶é—´", value="2024-11-05 23:59:59")

                    target_points = gr.Slider(
                        label="ç›®æ ‡ç‚¹æ•°",
                        minimum=1000,
                        maximum=10000,
                        value=5000,
                        step=500,
                        scale=1
                    )
                    
                    acquire_btn = gr.Button("ğŸ“¥ å¼€å§‹é‡‡é›†", variant="primary")
                    acquire_output = gr.Markdown(value="ç­‰å¾…é‡‡é›†...")
            
            # æ•°æ®é¢„è§ˆåŒºåŸŸ - å›¾è¡¨ä¼˜å…ˆï¼Œè¡¨æ ¼å¯æŠ˜å 
            with gr.Row():
                preview_plot = gr.Image(label="Curve Preview", height=350)
            
            with gr.Accordion("ğŸ“‹ Data Table (first 5000 rows)", open=False):
                preview_table = gr.Dataframe(
                    label="",
                    interactive=False
                )
            
            # äº‹ä»¶ç»‘å®š - æ•°æ®è·å–
            refresh_datasets_btn.click(
                fn=get_datasets_table,
                outputs=datasets_table
            )
            refresh_datasets_btn.click(
                fn=lambda: gr.Dropdown(choices=get_dataset_names()),
                outputs=preview_dropdown
            )
            delete_dataset_btn.click(
                fn=delete_selected_dataset,
                inputs=preview_dropdown,
                outputs=[datasets_table, preview_dropdown, delete_status]
            )
            preview_dropdown.change(
                fn=preview_dataset,
                inputs=preview_dropdown,
                outputs=[preview_table, column_selector, preview_plot]
            )
            column_selector.change(
                fn=update_plot_from_selection,
                inputs=[preview_dropdown, column_selector],
                outputs=preview_plot
            )
            acquire_btn.click(
                fn=start_acquire_task,
                inputs=[
                    source_input, host_input, port_input, user_input, pwd_input,
                    point_input, start_time_input, end_time_input, target_points
                ],
                outputs=acquire_output
            )
        
        # ==================== æ¨ç†ç›‘æ§ Tab ====================
        with gr.Tab("ğŸ” æ¨ç†ç›‘æ§") as inference_tab:
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ–°å»ºæ¨ç†ä»»åŠ¡")
                    algo_dropdown = gr.Dropdown(
                        label="é€‰æ‹©ç®—æ³•",
                        choices=get_algorithms(),
                        value="chatts",
                        interactive=True
                    )
                    
                    # æ¨¡å‹é…ç½®ç»„
                    with gr.Group():
                        base_model_input = gr.Textbox(
                            label="Base Model Path (Base Model)", 
                            value="/home/share/llm_models/bytedance-research/ChatTS-8B",
                            info="åŸºç¡€æ¨¡å‹è·¯å¾„"
                        )
                        lora_adapter_select = gr.Dropdown(
                            label="LoRA Adapter Path (å¯é€‰)",
                            choices=get_inference_models(), # è¿”å›çš„æ˜¯ LoRA è·¯å¾„åˆ—è¡¨
                            interactive=True,
                            info="å¾®è°ƒåçš„ LoRA é€‚é…å™¨è·¯å¾„"
                        )
                        
                    files_select = gr.CheckboxGroup(
                        label="é€‰æ‹©è¾“å…¥æ–‡ä»¶",
                        choices=get_unified_file_names()
                    )
                    
                    with gr.Accordion("âš™ï¸ é«˜çº§é…ç½® (å¯é€‰)", open=False):
                        with gr.Row():
                            n_downsample_input = gr.Slider(
                                label="é™é‡‡æ ·ç‚¹æ•° (n_downsample)", 
                                minimum=100, maximum=10000, step=100, value=settings.DEFAULT_DOWNSAMPLE_POINTS
                            )
                            threshold_input = gr.Number(
                                label="å¼‚å¸¸é˜ˆå€¼ (threshold)", value=8.0
                            )
                        
                        # ChatTS ä¸“å±å‚æ•°
                        with gr.Group(visible=True) as chatts_group:
                            gr.Markdown("#### ChatTS é…ç½®")
                            with gr.Row():
                                load_in_4bit_input = gr.Dropdown(
                                    label="4-bit é‡åŒ–", choices=["auto", "true", "false"], value="auto",
                                    info="æ˜¾å­˜ä¸è¶³æ—¶å»ºè®®å¼€å¯(true)"
                                )
                                prompt_template_input = gr.Dropdown(
                                    label="Prompt æ¨¡æ¿",
                                    choices=["default", "detailed", "minimal", "industrial", "english"],
                                    value="default"
                                )
                            with gr.Row():
                                chatts_device_input = gr.Textbox(label="Device", value="cuda:1")
                                chatts_use_cache_input = gr.Dropdown(
                                    label="Use Cache (KV)", choices=["auto", "true", "false"], value="auto"
                                )
                            max_new_tokens_input = gr.Number(
                                label="æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max New Tokens)", value=4096, precision=0
                            )

                        # Timer ä¸“å±å‚æ•°
                        with gr.Group(visible=False) as timer_group:
                            gr.Markdown("#### Timer é…ç½®")
                            with gr.Row():
                                timer_device_input = gr.Textbox(label="Device", value="cuda:0")
                                timer_lookback_input = gr.Number(label="Lookback Length", value=256, precision=0)
                            with gr.Row():
                                timer_threshold_k_input = gr.Number(label="Threshold K", value=3.5)
                                timer_method_input = gr.Dropdown(label="Method", choices=["mad", "sigma"], value="mad")
                            timer_streaming_input = gr.Checkbox(label="Enable Streaming Mode", value=False)
                            
                        # ADTK ä¸“å±å‚æ•°
                        with gr.Group(visible=False) as adtk_group:
                            gr.Markdown("#### ADTK HBOS é…ç½®")
                            with gr.Row():
                                adtk_bin_nums_input = gr.Number(label="Bin Nums (åˆ†ç®±æ•°)", value=20, precision=0)
                                adtk_hbos_ratio_input = gr.Number(label="HBOS Ratio (è·³å˜é˜ˆå€¼)", value=None)

                    with gr.Row():
                        submit_inference_btn = gr.Button("ğŸš€ æäº¤ä»»åŠ¡", variant="primary")
                        stop_inference_btn = gr.Button("ğŸ›‘ åœæ­¢ä»»åŠ¡", variant="stop", visible=False)
                    
                    # éšè—çš„çŠ¶æ€ç»„ä»¶ï¼Œç”¨äºå­˜å‚¨ current task id
                    current_task_id_state = gr.State("")
                
                with gr.Column(scale=2):
                    gr.Markdown("### ä»»åŠ¡çŠ¶æ€ & æ—¥å¿—")
                    with gr.Tabs():
                        with gr.Tab("å®æ—¶æ—¥å¿—"):
                            inference_logs = gr.Textbox(
                                value="",
                                label="Execution Logs",
                                interactive=False,
                                lines=20,
                                max_lines=20,
                                autoscroll=True
                            )
                        with gr.Tab("ä»»åŠ¡ç»“æœ"):
                             # å½“å‰ä»»åŠ¡çŠ¶æ€
                             inference_result_md = gr.Markdown(value="ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                             download_files = gr.File(label="å½“å‰ä»»åŠ¡ç»“æœ", file_count="multiple", interactive=False, visible=False)
                             
                             # æ•´åˆçš„ç»“æœæ–‡ä»¶ç®¡ç†åŒº
                             gr.Markdown("### ğŸ“‚ ç»“æœæ–‡ä»¶ç®¡ç†")
                             with gr.Row():
                                 results_method_select = gr.Dropdown(
                                     label="ç­›é€‰æ–¹æ³•",
                                     choices=["chatts", "timer", "adtk_hbos"],
                                     value="chatts",
                                     scale=1
                                 )
                                 refresh_results_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm", scale=0)
                             
                             # ç»Ÿä¸€æ–‡ä»¶åˆ—è¡¨ï¼ˆå¤šé€‰ï¼‰
                             file_manager_list = gr.CheckboxGroup(
                                 label="æ–‡ä»¶åˆ—è¡¨ (æ–‡ä»¶å | è¾ƒæ–°çš„åœ¨å‰)",
                                 choices=get_result_filenames("chatts"),
                                 value=[],
                                 interactive=True
                             )
                             
                             with gr.Row():
                                 download_selected_btn = gr.Button("â¬‡ï¸ ä¸‹è½½é€‰ä¸­", size="sm")
                                 delete_selected_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop", size="sm")

                             operation_status = gr.Markdown(value="")

                             # ä¸‹è½½åŒºåŸŸ (åŠ¨æ€æ˜¾ç¤º)
                             history_download_files = gr.File(
                                 label="ğŸ“¥ ä¸‹è½½åŒºåŸŸ (è¯·å…ˆé€‰æ‹©æ–‡ä»¶å¹¶ç‚¹å‡»â€œä¸‹è½½é€‰ä¸­â€)",
                                 file_count="multiple",
                                 interactive=False,
                                 visible=True
                             )
                    
                    # ä»»åŠ¡å†å²è®°å½• - æ”¾å…¥å¯æŠ˜å åŒºåŸŸ
                    with gr.Accordion("ğŸ“‹ ä»»åŠ¡å†å²è®°å½•", open=False):
                        with gr.Row():
                            refresh_tasks_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", size="sm")
                            clear_tasks_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", size="sm", variant="stop")
                        clear_status = gr.Markdown(value="", visible=True)
                        task_table = gr.Dataframe(
                            headers=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"],
                            value=[],
                            interactive=False
                        )
            
            # äº‹ä»¶ç»‘å®š - æ¨ç†ç›‘æ§
            
            # æäº¤ä»»åŠ¡
            submit_event = submit_inference_btn.click(
                fn=start_inference_task,
                inputs=[
                    algo_dropdown, base_model_input, lora_adapter_select, files_select,
                    # é€šç”¨å‚æ•°
                    n_downsample_input, threshold_input,
                    # ChatTS å‚æ•°
                    load_in_4bit_input, prompt_template_input, max_new_tokens_input, chatts_device_input, chatts_use_cache_input,
                    # Timer å‚æ•°
                    timer_device_input, timer_lookback_input, timer_threshold_k_input, timer_method_input, timer_streaming_input,
                    # ADTK å‚æ•°
                    adtk_bin_nums_input, adtk_hbos_ratio_input
                ],
                outputs=[
                    inference_logs, 
                    inference_result_md, 
                    stop_inference_btn, 
                    submit_inference_btn, 
                    current_task_id_state, 
                    download_files
                ]
            )
            
            # åœæ­¢ä»»åŠ¡
            stop_inference_btn.click(
                fn=stop_task_action,
                inputs=[current_task_id_state],
                outputs=[
                    inference_result_md, 
                    stop_inference_btn, 
                    submit_inference_btn, 
                    current_task_id_state, 
                    download_files
                ]
            )
            
            refresh_tasks_btn.click(
                fn=get_task_status_table,
                outputs=task_table
            )
            refresh_tasks_btn.click(
                fn=lambda: gr.CheckboxGroup(choices=get_unified_file_names()),
                outputs=files_select
            )
            refresh_tasks_btn.click(
                fn=lambda: gr.Dropdown(choices=get_inference_models()),
                outputs=lora_adapter_select
            )
            
            # Tab åˆ‡æ¢æ—¶è‡ªåŠ¨åˆ·æ–°æ–‡ä»¶åˆ—è¡¨
            inference_tab.select(
                fn=lambda: gr.CheckboxGroup(choices=get_unified_file_names()),
                outputs=files_select
            )
            inference_tab.select(
                fn=lambda: gr.Dropdown(choices=get_inference_models()),
                outputs=lora_adapter_select
            )
            # æ¸…ç©ºå†å²è®°å½•
            clear_tasks_btn.click(
                fn=clear_task_history,
                outputs=[task_table, clear_status]
            )
            
            # å†å²ç»“æœæ–‡ä»¶åˆ·æ–°
            refresh_results_btn.click(
                fn=lambda m: gr.CheckboxGroup(choices=get_result_filenames(m)),
                inputs=results_method_select,
                outputs=file_manager_list
            )
            
            # åˆ‡æ¢æ–¹æ³•æ—¶åˆ·æ–°ç»“æœåˆ—è¡¨
            results_method_select.change(
                fn=lambda m: gr.CheckboxGroup(choices=get_result_filenames(m)),
                inputs=results_method_select,
                outputs=file_manager_list
            )
            
            # åˆ é™¤é€‰ä¸­æ–‡ä»¶
            delete_selected_btn.click(
                fn=delete_selected_files,
                inputs=[results_method_select, file_manager_list],
                outputs=[file_manager_list, history_download_files, operation_status]
            )
            
            # ä¸‹è½½é€‰ä¸­æ–‡ä»¶
            # ä¸‹è½½é€‰ä¸­æ–‡ä»¶
            download_selected_btn.click(
                fn=prepare_download_files,
                inputs=[results_method_select, file_manager_list],
                outputs=[history_download_files, operation_status]
            )
            
            # ç®—æ³•åˆ‡æ¢äº‹ä»¶ï¼šæ§åˆ¶å‚æ•°ç»„æ˜¾ç¤º
            algo_dropdown.change(
                fn=toggle_algo_params,
                inputs=algo_dropdown,
                outputs=[chatts_group, timer_group, adtk_group]
            )
        
        # ==================== æ ‡æ³¨å·¥å…· Tab ====================
        with gr.Tab("ğŸ·ï¸ æ ‡æ³¨å·¥å…·"):
            with gr.Row():
                with gr.Column(scale=3):
                    gr.Markdown("### ğŸ”— å¿«é€Ÿè®¿é—®")
                    # ä½¿ç”¨ HTML æŒ‰é’®æ‰“å¼€é“¾æ¥ï¼Œæ›´ç›´è§‚
                    gr.HTML("""
                    <div style="padding: 10px; background-color: #f0f9ff; border-radius: 8px; border: 1px solid #bae6fd;">
                        <p style="margin-bottom: 10px; font-weight: bold; color: #0369a1;">
                            æ ‡æ³¨å·¥å…·è¿è¡Œåœ¨ç‹¬ç«‹æœåŠ¡ç«¯å£ (5000)
                        </p>
                        <a href="http://192.168.199.126:5000" target="_blank" style="
                            display: inline-block;
                            padding: 10px 20px;
                            background-color: #0284c7;
                            color: white;
                            text-decoration: none;
                            border-radius: 6px;
                            font-weight: bold;
                        ">
                            ğŸš€ æ‰“å¼€æ ‡æ³¨å·¥å…· (Open Annotator)
                        </a>
                    </div>
                    """)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“Š çŠ¶æ€æ¦‚è§ˆ")
                    # åŠ¨æ€è·å–æ ‡æ³¨æ–‡ä»¶æ•°
                    def get_annotation_stats():
                        # ä½¿ç”¨ Settings
                        ann_dir = Path(settings.ANNOTATIONS_ROOT) / "douff"
                        if not ann_dir.exists():
                            return "æš‚æ— æ ‡æ³¨ç›®å½•"
                        count = len(list(ann_dir.glob("*.json")))
                        return f"å·²æ ‡æ³¨æ–‡ä»¶æ•°: {count}"

                    annotation_stats = gr.Textbox(
                        value=get_annotation_stats(),
                        label="å½“å‰æ ‡æ³¨è¿›åº¦",
                        interactive=False
                    )
                    refresh_ann_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", size="sm")
                    refresh_ann_btn.click(fn=get_annotation_stats, outputs=annotation_stats)

            gr.Markdown("---")
            gr.Markdown("### ğŸ”„ æ•°æ®è½¬æ¢ (Annotator -> ChatTS)")
            
            with gr.Row():
                with gr.Column(scale=1):
                    # é…ç½®åŒºåŸŸ
                    with gr.Accordion("âš™ï¸ è·¯å¾„ä¸å‚æ•°é…ç½® (Settings)", open=False):
                        conf_input_dir = gr.Textbox(
                            label="æ ‡æ³¨æ–‡ä»¶æ¥æº (Annotation Dir)", 
                            value=str(Path(settings.ANNOTATIONS_ROOT) / "douff")
                        )
                        conf_image_dir = gr.Textbox(
                            label="å›¾ç‰‡æ–‡ä»¶æ¥æº (Image Dir)", 
                            value=settings.DATA_DOWNSAMPLED_DIR
                        )
                        conf_output_path = gr.Textbox(
                            label="è½¬æ¢è¾“å‡ºè·¯å¾„ (Output Path)", 
                            value=str(Path(settings.DATA_TRAINING_DIR) / "converted_data.json")
                        )

                    # è·å–æ ‡æ³¨æ–‡ä»¶åˆ—è¡¨
                    def get_file_choices(ann_dir):
                        path_obj = Path(ann_dir)
                        if not path_obj.exists():
                            return []
                        try:
                            files = list(path_obj.glob("*.json"))
                            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                            return [f.name for f in files]
                        except Exception:
                            return []

                    # åˆå§‹åŠ è½½
                    # åˆå§‹åŠ è½½
                    default_ann_dir = str(Path(settings.ANNOTATIONS_ROOT) / "douff")
                    initial_choices = get_file_choices(default_ann_dir)
                    initial_val = initial_choices[0] if initial_choices else None

                    ann_file_dropdown = gr.Dropdown(
                        label="é€‰æ‹©è¦é¢„è§ˆ/è½¬æ¢çš„æ–‡ä»¶",
                        choices=initial_choices,
                        value=initial_val,
                        multiselect=False,
                        interactive=True,
                        allow_custom_value=False
                    )
                    
                    def refresh_files(ann_dir):
                        choices = get_file_choices(ann_dir)
                        val = choices[0] if choices else None
                        return gr.Dropdown(choices=choices, value=val)
                        
                    refresh_files_btn = gr.Button("ğŸ”„ åˆ·æ–°æ–‡ä»¶åˆ—è¡¨", size="sm")
                    
                    with gr.Row():
                        convert_curr_btn = gr.Button("ğŸš€ ä»…è½¬æ¢é€‰ä¸­", variant="primary")
                        convert_all_btn = gr.Button("ğŸ“¦ æ‰¹é‡è½¬æ¢æ‰€æœ‰", variant="secondary")
                    
                with gr.Column(scale=2):
                    convert_status = gr.Textbox(label="æ“ä½œæ—¥å¿— (Execution Log)", lines=10, interactive=False)
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### ğŸ“ è½¬æ¢å‰ (Annotator JSON)")
                    before_json = gr.JSON(label="Source Data", height=400)
                with gr.Column():
                    gr.Markdown("#### ğŸ¯ è½¬æ¢å (ChatTS Training Data)")
                    after_json = gr.JSON(label="Converted Data", height=400)
                
            def preview_source_file(selected_file, input_dir_val, image_dir_val):
                """é€‰æ‹©æ–‡ä»¶æ—¶ç«‹å³é¢„è§ˆï¼Œå¹¶æ‰§è¡ŒçœŸå®è½¬æ¢ï¼ˆä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼‰"""
                if not selected_file or not input_dir_val:
                    return None, None
                 
                # 1. è¯»å–æºæ–‡ä»¶
                src_p = Path(input_dir_val) / selected_file
                source_content = None
                
                try:
                    if src_p.exists():
                        with open(src_p, 'r', encoding='utf-8') as f:
                            source_content = json.load(f)
                except Exception as e:
                    source_content = {"error": str(e)}
                
                # 2. æ‰§è¡Œè½¬æ¢é¢„è§ˆ (çœŸå®è°ƒç”¨é€‚é…å™¨)
                converted_content = None
                try:
                    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä½œä¸ºè¾“å‡º
                    with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
                        tmp_path = tmp.name
                    
                    # é»˜è®¤ image_dir å¦‚æœä¸ºç©º
                    img_d = image_dir_val if image_dir_val else settings.DATA_DOWNSAMPLED_DIR
                    
                    res = data_adapter.convert_annotations(
                        input_dir=input_dir_val,
                        output_path=tmp_path,
                        image_dir=img_d,
                        filename=selected_file
                    )
                    
                    if res["success"]:
                        try:
                            with open(tmp_path, 'r', encoding='utf-8') as f:
                                converted_content = json.load(f)
                        except Exception as read_err:
                            converted_content = {"error": f"Read converted file failed: {read_err}"}
                    else:
                        converted_content = {
                            "error": "Conversion failed", 
                            "stderr": res.get("stderr", ""),
                            "stdout": res.get("stdout", "")
                        }
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        Path(tmp_path).unlink(missing_ok=True)
                    except:
                        pass

                except Exception as e:
                    converted_content = {"error": f"Preview failed: {str(e)}"}
                
                return source_content, converted_content

            def convert_core(selected_file, input_dir, image_dir, output_path, mode="single"):
                """æ ¸å¿ƒè½¬æ¢é€»è¾‘"""
                # åˆ›å»ºè¾“å‡ºç›®å½•
                try:
                    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    return f"âŒ åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {str(e)}", {}, {}
                
                target_filename = selected_file if mode == "single" else None
                
                # æ‰§è¡Œè½¬æ¢
                result = data_adapter.convert_annotations(
                    input_dir, 
                    output_path, 
                    image_dir=image_dir,
                    filename=target_filename
                )
                
                status_msg = ""
                source_sample = {}
                target_sample = {}
                
                if result.get("success"):
                    log_output = result.get("stdout", "")
                    prefix = "å•æ–‡ä»¶" if mode == "single" else "æ‰¹é‡"
                    status_msg = f"âœ… {prefix}è½¬æ¢æˆåŠŸ! \nè¾“å‡º: {result.get('output_path')}\n\næ‰§è¡Œæ—¥å¿—:\n{log_output}"
                    
                    # å°è¯•è¯»å–æºæ–‡ä»¶è¿›è¡Œé¢„è§ˆ
                    try:
                         # å¦‚æœæ˜¯æ‰¹é‡è½¬æ¢ä½†ä¾ç„¶é€‰äº†æ–‡ä»¶ï¼Œæˆ–è€…å•æ–‡ä»¶æ¨¡å¼
                         preview_file = selected_file
                         if not preview_file and Path(input_dir).exists():
                             # å¦‚æœéƒ½æ²¡é€‰ï¼Œæ‰¾ç¬¬ä¸€ä¸ª
                             all_jsons = list(Path(input_dir).glob("*.json"))
                             if all_jsons:
                                 preview_file = all_jsons[0].name
                         
                         if preview_file:
                             src_p = Path(input_dir) / preview_file
                             if src_p.exists():
                                 with open(src_p, 'r', encoding='utf-8') as f:
                                    source_sample = json.load(f)
                    except Exception as e:
                        source_sample = {"error": f"Read source failed: {str(e)}"}

                    # è¯»å–è½¬æ¢åçš„ç»“æœ
                    try:
                        with open(output_path, 'r', encoding='utf-8') as f:
                            converted_data = json.load(f)
                            if converted_data and isinstance(converted_data, list):
                                if preview_file:
                                    # å°è¯•åŒ¹é… image è·¯å¾„
                                    core_name = preview_file.replace("annotations_æ•°æ®é›†", "").replace(".json", "")
                                    # ç®€å•å»åç¼€
                                    core_name = core_name.replace(".csv", "")
                                    
                                    matched = False
                                    for item in converted_data:
                                        if core_name in item.get("image", ""):
                                            target_sample = item
                                            matched = True
                                            break
                                    if not matched:
                                        target_sample = converted_data[-1] if mode == "single" else converted_data[0]
                                        status_msg = f"(æ³¨æ„ï¼šé¢„è§ˆæœªç²¾ç¡®åŒ¹é…ï¼Œæ˜¾ç¤º{'æœ€åä¸€æ¡' if mode=='single' else 'ç¬¬ä¸€æ¡'})\n" + status_msg
                                else:
                                    target_sample = converted_data[0]
                    except Exception as e:
                        target_sample = {"error": f"Read output failed: {str(e)}"}
                        
                else:
                    status_msg = f"âŒ è½¬æ¢å¤±è´¥: {result.get('error')}\næ—¥å¿—:\n{result.get('stderr', '')}"
                
                return status_msg, source_sample, target_sample

            # ç»‘å®šäº‹ä»¶
            convert_curr_btn.click(
                fn=lambda f, i, m, o: convert_core(f, i, m, o, mode="single"),
                inputs=[ann_file_dropdown, conf_input_dir, conf_image_dir, conf_output_path],
                outputs=[convert_status, before_json, after_json]
            )
            
            convert_all_btn.click(
                fn=lambda f, i, m, o: convert_core(f, i, m, o, mode="batch"),
                inputs=[ann_file_dropdown, conf_input_dir, conf_image_dir, conf_output_path],
                outputs=[convert_status, before_json, after_json]
            )
            
            refresh_files_btn.click(
                fn=refresh_files,
                inputs=[conf_input_dir],
                outputs=ann_file_dropdown
            )
            
            # é€‰æ‹©æ–‡ä»¶ç«‹å³é¢„è§ˆ
            ann_file_dropdown.change(
                fn=preview_source_file,
                inputs=[ann_file_dropdown, conf_input_dir, conf_image_dir],
                outputs=[before_json, after_json]
            )
            
            # åˆå§‹åŒ–é¢„è§ˆ
            if initial_val:
                init_src, init_ex = preview_source_file(initial_val, default_ann_dir, settings.DATA_DOWNSAMPLED_DIR)
                before_json.value = init_src
                after_json.value = init_ex

        # ==================== æ•°æ®èµ„äº§ç®¡ç† Tab (New) ====================
        with gr.Tab("ğŸ“¦ æ•°æ®èµ„äº§ç®¡ç†"):
            with gr.Tabs():
                # 1. æ ‡æ³¨æ•°æ®ç®¡ç†
                with gr.Tab("æ ‡æ³¨æ•°æ® (Annotations)"):
                    gr.Markdown("### ğŸ“ æ ‡æ³¨æ–‡ä»¶ç®¡ç†")
                    with gr.Row():
                        with gr.Column(scale=1):
                            ann_mgr_dir = gr.Textbox(label="æ ‡æ³¨ç›®å½•", value=str(Path(settings.ANNOTATIONS_ROOT) / "douff"), interactive=False)
                            ann_mgr_list = gr.Dropdown(label="é€‰æ‹©æ–‡ä»¶", interactive=True)
                            refresh_ann_mgr = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                            delete_ann_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­æ–‡ä»¶", variant="stop")
                            ann_op_status = gr.Textbox(label="æ“ä½œçŠ¶æ€", interactive=False)
                        
                        with gr.Column(scale=2):
                            ann_mgr_view = gr.JSON(label="æ–‡ä»¶å†…å®¹é¢„è§ˆ", height=600)

                    # Logic
                    def list_ann_files(path_str):
                        p = Path(path_str)
                        if not p.exists(): return []
                        files = list(p.glob("*.json"))
                        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                        return [f.name for f in files]

                    def load_ann_content(path_str, filename):
                        if not filename: return None
                        try:
                            with open(Path(path_str) / filename, 'r') as f:
                                return json.load(f)
                        except Exception as e:
                            return {"error": str(e)}

                    def delete_ann_file(path_str, filename):
                        if not filename: return "æœªé€‰æ‹©æ–‡ä»¶", gr.update()
                        try:
                            p = Path(path_str) / filename
                            p.unlink()
                            # åˆ·æ–°åˆ—è¡¨
                            new_list = list_ann_files(path_str)
                            return f"å·²åˆ é™¤: {filename}", gr.update(choices=new_list, value=None)
                        except Exception as e:
                            return f"åˆ é™¤å¤±è´¥: {e}", gr.update()

                    # Bindings
                    ann_mgr_dir.change(fn=lambda p: gr.update(choices=list_ann_files(p)), inputs=ann_mgr_dir, outputs=ann_mgr_list)
                    refresh_ann_mgr.click(fn=lambda p: gr.update(choices=list_ann_files(p)), inputs=ann_mgr_dir, outputs=ann_mgr_list)
                    ann_mgr_list.change(fn=load_ann_content, inputs=[ann_mgr_dir, ann_mgr_list], outputs=ann_mgr_view)
                    delete_ann_btn.click(fn=delete_ann_file, inputs=[ann_mgr_dir, ann_mgr_list], outputs=[ann_op_status, ann_mgr_list])

                # 2. è®­ç»ƒæ•°æ®ç®¡ç†
                with gr.Tab("è®­ç»ƒæ•°æ® (Training Data)"):
                    gr.Markdown("### ğŸ¯ å¾®è°ƒæ•°æ®ç®¡ç† (Converted JSONL)")
                    with gr.Row():
                        with gr.Column(scale=1):
                            train_mgr_dir = gr.Textbox(label="æ•°æ®ç›®å½•", value=settings.DATA_TRAINING_DIR, interactive=False)
                            train_mgr_list = gr.Dropdown(label="é€‰æ‹©æ–‡ä»¶", interactive=True)
                            refresh_train_mgr = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                            delete_train_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­æ–‡ä»¶", variant="stop")
                            train_op_status = gr.Textbox(label="æ“ä½œçŠ¶æ€", interactive=False)
                        
                        with gr.Column(scale=2):
                            train_mgr_view = gr.JSON(label="æ–‡ä»¶å†…å®¹é¢„è§ˆ (Head 50 lines / JSON)", height=600)

                    # Logic
                    def list_train_files(path_str):
                        p = Path(path_str)
                        if not p.exists(): return []
                        files = list(p.glob("*.json")) + list(p.glob("*.jsonl"))
                        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                        return [f.name for f in files]

                    def load_train_content(path_str, filename):
                        if not filename: return None
                        try:
                            p = Path(path_str) / filename
                            
                            # Strategy 1: Small file (<50MB) -> Try full JSON load
                            # This handles standard JSON lists (pretty printed or minified)
                            if p.stat().st_size < 50 * 1024 * 1024: 
                                try:
                                    with open(p, 'r') as f:
                                        data = json.load(f)
                                    if isinstance(data, list):
                                        return data[:50]  # Preview first 50 items
                                    return data
                                except:
                                    pass # Fallback to Strategy 2
                            
                            # Strategy 2: JSONL or Large File -> Line-by-line
                            records = []
                            with open(p, 'r') as f:
                                for _ in range(50):
                                    line = f.readline()
                                    if not line: break
                                    line = line.strip()
                                    if not line: continue
                                    try:
                                        records.append(json.loads(line))
                                    except:
                                        # If it looks like start/end of array, skip or show raw
                                        if line in ['[', ']', '],']: continue
                                        records.append({"raw_text": line})
                            return records
                        except Exception as e:
                            return {"error": str(e)}

                    def delete_train_file(path_str, filename):
                        if not filename: return "æœªé€‰æ‹©æ–‡ä»¶", gr.update()
                        try:
                            p = Path(path_str) / filename
                            p.unlink()
                            new_list = list_train_files(path_str)
                            return f"å·²åˆ é™¤: {filename}", gr.update(choices=new_list, value=None)
                        except Exception as e:
                            return f"åˆ é™¤å¤±è´¥: {e}", gr.update()

                    # Bindings
                    # Init load
                    train_mgr_dir.change(fn=lambda p: gr.update(choices=list_train_files(p)), inputs=train_mgr_dir, outputs=train_mgr_list)
                    refresh_train_mgr.click(fn=lambda p: gr.update(choices=list_train_files(p)), inputs=train_mgr_dir, outputs=train_mgr_list)
                    train_mgr_list.change(fn=load_train_content, inputs=[train_mgr_dir, train_mgr_list], outputs=train_mgr_view)
                    delete_train_btn.click(fn=delete_train_file, inputs=[train_mgr_dir, train_mgr_list], outputs=[train_op_status, train_mgr_list])
        
        # ==================== å¾®è°ƒè®­ç»ƒ Tab (åŸæœ‰) ====================
        with gr.Tab("ğŸ¯ å¼€å§‹è®­ç»ƒ"):
            with gr.Row():
                with gr.Column(scale=2):
                    # åŸºç¡€é…ç½®
                    gr.Markdown("### åŸºç¡€é…ç½®")
                    
                    # New Dropdowns for Quick Start
                    with gr.Row():
                        model_path_dropdown = gr.Dropdown(
                            label="åŸºç¡€æ¨¡å‹ (Base Model)",
                            choices=adapter.get_base_models(),
                            value=adapter.get_base_models()[0] if adapter.get_base_models() else None,
                            interactive=True,
                            allow_custom_value=True
                        )
                        dataset_dropdown = gr.Dropdown(
                            label="å¾®è°ƒæ•°æ®é›† (Dataset)",
                            choices=adapter.get_dataset_list(),
                            value=adapter.get_dataset_list()[0] if adapter.get_dataset_list() else None,
                            interactive=True
                        )

                    config_dropdown = gr.Dropdown(
                        label="è®­ç»ƒæ¨¡æ¿ (Template Script)",
                        choices=get_training_configs(),
                        interactive=True,
                        info="é€‰æ‹©ä¸€ä¸ªè„šæœ¬ä½œä¸ºå‚æ•°æ¨¡æ¿ (å¦‚ DeepSpeed é…ç½®)"
                    )
                    output_name = gr.Textbox(
                        label="è¾“å‡ºç›®å½•åç§°",
                        placeholder="ä¾‹å¦‚: my_model_v1"
                    )
                    
                    with gr.Row():
                        learning_rate = gr.Textbox(
                            label="å­¦ä¹ ç‡",
                            value="2e-5"
                        )
                        num_epochs = gr.Number(
                            label="è®­ç»ƒè½®æ•°",
                            value=3.0
                        )
                        batch_size = gr.Slider(
                            label="æ‰¹æ¬¡å¤§å°",
                            minimum=1,
                            maximum=32,
                            value=2,
                            step=1
                        )
                    
                    # LoRA é…ç½®ï¼ˆæŠ˜å ï¼‰
                    with gr.Accordion("LoRA é…ç½®", open=False):
                        with gr.Row():
                            lora_rank = gr.Slider(
                                label="LoRA Rank",
                                minimum=1,
                                maximum=128,
                                value=8,
                                step=1
                            )
                            lora_alpha = gr.Slider(
                                label="LoRA Alpha",
                                minimum=1,
                                maximum=256,
                                value=16,
                                step=1
                            )
                    
                    # æ§åˆ¶æŒ‰é’®
                    # --- Backend Functions ---
                    def start_training_wrap(
                        config_name, lr, epochs, batch_size, rank, alpha, output_name,
                        model_path, dataset_name
                    ):
                        if not config_name:
                            return "âŒ è¯·é€‰æ‹©è®­ç»ƒæ¨¡æ¿", ""
                            
                        task_id = f"qs_{int(time.time())}"
                        
                        # Call backend
                        overrides = {
                            "override_learning_rate": lr,
                            "override_epochs": epochs,
                            "override_batch_size": batch_size,
                            "override_lora_rank": rank,
                            "override_lora_alpha": alpha,
                            "override_model_path": model_path,
                            "override_dataset": dataset_name
                        }
                        
                        res = adapter.run_training(task_id, config_name, version_tag=output_name, **overrides)
                        
                        if res.get("success"):
                            return f"âœ… è®­ç»ƒä»»åŠ¡å·²æˆåŠŸå¯åŠ¨!\nä»»åŠ¡ID: {task_id}\nè¾“å‡ºç›®å½•: {res.get('output_dir')}\n\næ­£åœ¨åå°è¿è¡Œä¸­... è¯·ç•™æ„ä¸‹æ–¹å®æ—¶æ—¥å¿—ã€‚", task_id
                        else:
                            return f"âŒ å¯åŠ¨é”™è¯¯: {res.get('error')}", ""

                    def stream_logs(task_id, current_log, offset):
                        if not task_id:
                            return current_log, offset
                        
                        # Read increment
                        res = adapter.get_training_log(task_id, offset)
                        new_content = res.get("log", "")
                        new_offset = res.get("offset", offset)
                        
                        if new_content:
                            current_log = (current_log or "") + new_content
                            
                        return current_log, new_offset

                    def stop_training_wrap(task_id):
                        if not task_id:
                            return "æ— è¿è¡Œä¸­çš„ä»»åŠ¡"
                        res = adapter.stop_training(task_id)
                        if res.get("success"):
                            return "ğŸ›‘ ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢"
                        else:
                            return f"åœæ­¢å¤±è´¥: {res.get('error')}"

                    # --- Layout & Events ---
                    with gr.Column():
                        with gr.Row():
                            start_btn = gr.Button("ğŸš€ (Quick Start) å¼€å§‹è®­ç»ƒ", variant="primary", scale=2)
                            stop_btn = gr.Button("ğŸ›‘ åœæ­¢è®­ç»ƒ", variant="stop", scale=1)
                            refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°é…ç½®", scale=1)
                        
                        # Hidden state for Task ID and Log Offset
                        task_id_state = gr.State("")
                        log_offset_state = gr.State(0)
                        
                        output_box = gr.Textbox(label="è®­ç»ƒçŠ¶æ€", lines=4)
                        log_box = gr.Code(label="å®æ—¶æ—¥å¿— (Real-time Logs)", language="shell", lines=15)
                        
                        # Timer for polling
                        timer = gr.Timer(1) # 1s interval

                        # Events
                        start_btn.click(
                            fn=start_training_wrap,
                            inputs=[
                                config_dropdown, learning_rate, num_epochs,
                                batch_size, lora_rank, lora_alpha, output_name,
                                model_path_dropdown, dataset_dropdown
                            ],
                            outputs=[output_box, task_id_state]
                        ).then(
                            fn=lambda: 0, outputs=log_offset_state # Reset offset
                        ).then(
                            fn=lambda: "", outputs=log_box # Clear logs
                        )
                        
                        stop_btn.click(
                            fn=stop_training_wrap,
                            inputs=[task_id_state],
                            outputs=output_box
                        )
                        
                        # Timer ticks -> Update logs
                        timer.tick(
                            fn=stream_logs,
                            inputs=[task_id_state, log_box, log_offset_state],
                            outputs=[log_box, log_offset_state]
                        )
                        refresh_btn.click(
                            fn=lambda: (
                                gr.Dropdown(choices=get_training_configs()),
                                gr.Dropdown(choices=adapter.get_dataset_list())
                            ),
                            outputs=[config_dropdown, dataset_dropdown]
                        )

            # ==================== Advanced Mode (Native Integration) ====================
        with gr.Accordion("âš™ï¸ é«˜çº§æ¨¡å¼ (Native WebUI Integration)", open=False):
                gr.Markdown("""
                > **ä¸“å®¶æ¨¡å¼**: å°†å½“å‰é€‰æ‹©çš„ Shell è„šæœ¬è‡ªåŠ¨è½¬æ¢ä¸º LLaMA-Factory é…ç½®ï¼Œå¹¶å¯åŠ¨åŸç”Ÿ WebUI è¿›è¡Œå¾®è°ƒã€‚
                > é€‚åˆéœ€è¦è°ƒæ•´ DeepSpeedã€LR Scheduler ç­‰é«˜çº§å‚æ•°çš„ç”¨æˆ·ã€‚
                """)
                with gr.Row():
                    convert_btn = gr.Button("ğŸ› ï¸ 1. è½¬æ¢è„šæœ¬ä¸ºæ¨¡æ¿", variant="secondary")
                    launch_native_btn = gr.Button("ğŸš€ 2. å¯åŠ¨åŸç”Ÿ WebUI", variant="primary")
                
                native_status = gr.Markdown("ç­‰å¾…æ“ä½œ...")
                native_ui_link = gr.Markdown(visible=False)

                # Logic
                def convert_action(script_name):
                    if not script_name: return "âš ï¸ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªè„šæœ¬é…ç½®"
                    res = adapter.convert_script_to_config(script_name)
                    if res["success"]:
                        return f"âœ… {res['message']}"
                    return f"âŒ {res['error']}"

                def launch_action():
                    res = adapter.start_native_webui()
                    if res["success"]:
                        url = res['url']
                        return f"âœ… {res['message']}", gr.Markdown(f"### [ğŸ‘‰ ç‚¹å‡»è®¿é—®åŸç”Ÿ WebUI ({url})]({url})", visible=True)
                    return f"âŒ {res['error']}", gr.update(visible=False)

                convert_btn.click(convert_action, inputs=config_dropdown, outputs=native_status)
                launch_native_btn.click(launch_action, outputs=[native_status, native_ui_link])
        
        with gr.Tab("ğŸ“Š å·²è®­ç»ƒæ¨¡å‹"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ¨¡å‹åˆ—è¡¨")
                    model_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æ¨¡å‹",
                        choices=get_trained_models(),
                        interactive=True
                    )
                    refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                
                with gr.Column(scale=2):
                    gr.Markdown("### æ¨¡å‹è¯¦æƒ…")
                    model_info = gr.Markdown(value="è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
                    
                    gr.Markdown("### Loss æ›²çº¿")
                    loss_image = gr.Image(label="Training Loss", type="filepath")
            
            # äº‹ä»¶ç»‘å®š
            model_dropdown.change(
                fn=get_model_info,
                inputs=model_dropdown,
                outputs=model_info
            )
            model_dropdown.change(
                fn=get_loss_plot,
                inputs=model_dropdown,
                outputs=loss_image
            )
            refresh_models_btn.click(
                fn=lambda: gr.Dropdown(choices=get_trained_models()),
                outputs=model_dropdown
            )
            
        with gr.Tab("âš–ï¸ æ¨¡å‹å¯¹æ¯”"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### é€‰æ‹©å¯¹æ¯”æ¨¡å‹")
                    compare_models = gr.CheckboxGroup(
                        label="æ¨¡å‹åˆ—è¡¨",
                        choices=get_trained_models()
                    )
                    compare_btn = gr.Button("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾", variant="primary")
                    refresh_compare_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                
                with gr.Column(scale=3):
                    gr.Markdown("### å¯¹æ¯”ç»“æœ")
                    comparison_plot = gr.Image(label="Loss Comparison")
            
            # äº‹ä»¶ç»‘å®š
            compare_btn.click(
                fn=get_comparison_plot,
                inputs=compare_models,
                outputs=comparison_plot
            )
            refresh_compare_btn.click(
                fn=lambda: gr.CheckboxGroup(choices=get_trained_models()),
                outputs=compare_models
            )
        
        with gr.Tab("âš™ï¸ é…ç½®è¯´æ˜"):
            gr.Markdown("""
## è®­ç»ƒé…ç½®è¯´æ˜

### åŸºç¡€å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| **å­¦ä¹ ç‡** | æ¨¡å‹æ›´æ–°æ­¥é•¿ | 2e-5 ~ 5e-5 |
| **è®­ç»ƒè½®æ•°** | å®Œæ•´éå†æ•°æ®é›†æ¬¡æ•° | 3 ~ 5 |
| **æ‰¹æ¬¡å¤§å°** | æ¯æ¬¡æ›´æ–°çš„æ ·æœ¬æ•° | 2 ~ 8 (å–å†³äºæ˜¾å­˜) |

### LoRA å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| **LoRA Rank** | ä½ç§©åˆ†è§£ç»´åº¦ | 8 ~ 64 |
| **LoRA Alpha** | ç¼©æ”¾å› å­ | é€šå¸¸ä¸º Rank çš„ 2 å€ |

### æ˜¾å­˜éœ€æ±‚

| æ¨¡å‹ | æ‰¹æ¬¡å¤§å° | æ˜¾å­˜éœ€æ±‚ |
|------|---------|---------|
| ChatTS-8B + LoRA | 2 | ~16GB |
| ChatTS-8B + LoRA | 4 | ~24GB |
| ChatTS-14B + LoRA | 2 | ~24GB |

### è®­ç»ƒè„šæœ¬

å¯ç”¨çš„è®­ç»ƒé…ç½®æ¥è‡ª `/home/douff/ts/ChatTS-Training/scripts/lora/` ç›®å½•ã€‚
""")
    
        # åˆå§‹åŒ–åŠ è½½
        demo.load(
            fn=get_dataset_names,
            outputs=preview_dropdown
        ).then(
            fn=lambda x: x[0] if x else None,
            inputs=preview_dropdown,
            outputs=preview_dropdown
        ).then(
            fn=preview_dataset,
            inputs=preview_dropdown,
            outputs=[preview_table, column_selector, preview_plot]
        )
            
    return demo


# åˆ›å»ºå…¨å±€ Gradio åº”ç”¨å®ä¾‹
training_ui = create_training_ui()
