"""
Gradio ç»Ÿä¸€ç®¡ç†ç•Œé¢
åŒ…å«ï¼šæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒã€æ¨¡å‹å¯¹æ¯”
"""
import gradio as gr
from pathlib import Path
from typing import List, Dict, Optional
import json
import time
import pandas as pd

from configs.settings import settings
from src.adapters.chatts_training import ChatTSTrainingAdapter
from src.adapters.data_processing import DataProcessingAdapter
from src.adapters.check_outlier import CheckOutlierAdapter


# åˆå§‹åŒ–é€‚é…å™¨
training_adapter = ChatTSTrainingAdapter()
data_adapter = DataProcessingAdapter()
inference_adapter = CheckOutlierAdapter()

# ä¸ºäº†å…¼å®¹æ€§ä¿ç•™æ—§å˜é‡å
adapter = training_adapter

# ç»“æœæ–‡ä»¶ç›®å½•
RESULTS_BASE_PATH = Path("/home/share/results/data")


def get_existing_results(method: str = "chatts") -> List[str]:
    """è·å–å·²æœ‰çš„ç»“æœæ–‡ä»¶åˆ—è¡¨"""
    results_dir = RESULTS_BASE_PATH / "global" / method
    if not results_dir.exists():
        return []
    
    # è·å–æ‰€æœ‰ CSV æ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    csv_files = list(results_dir.glob("*.csv"))
    csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return [str(f) for f in csv_files[:20]]  # æœ€å¤šè¿”å› 20 ä¸ª


def delete_selected_files(method: str, filenames: List[str]) -> tuple:
    """æ‰¹é‡åˆ é™¤é€‰ä¸­çš„ç»“æœæ–‡ä»¶"""
    if not filenames:
        return (
            gr.CheckboxGroup(choices=get_result_filenames(method)), 
            gr.File(value=None), 
            "âš ï¸ è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶"
        )
    
    results_dir = RESULTS_BASE_PATH / "global" / method
    deleted_count = 0
    errors = []
    
    for fname in filenames:
        file_path = results_dir / fname.strip()  # Strip whitespace just in case
        print(f"DEBUG: Attempting to delete {file_path}")
        if file_path.exists():
            try:
                file_path.unlink()
                deleted_count += 1
                print(f"DEBUG: Deleted {file_path}")
            except Exception as e:
                errors.append(f"{fname}: {str(e)}")
                print(f"DEBUG: Error deleting {file_path}: {e}")
        else:
            print(f"DEBUG: File not found {file_path}")
            # Try verify if it's a encoding issue or partial path
            errors.append(f"{fname}: File not found")
    
    # åˆ·æ–°åˆ—è¡¨
    time.sleep(0.5)  # ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
    new_choices = get_result_filenames(method)
    
    status_msg = f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶"
    if errors:
        status_msg += f"\nâŒ é”™è¯¯: {'; '.join(errors)}"
        
    return (
        gr.update(choices=new_choices, value=[]), 
        gr.update(value=None, label="ğŸ“¥ ä¸‹è½½åŒºåŸŸ (è¯·å…ˆé€‰æ‹©æ–‡ä»¶)"),
        status_msg
    )


def prepare_download_files(method: str, filenames: List[str]) -> tuple:
    """å‡†å¤‡ä¸‹è½½é€‰ä¸­çš„æ–‡ä»¶"""
    if not filenames:
        return None, "âš ï¸ è¯·å…ˆé€‰æ‹©è¦ä¸‹è½½çš„æ–‡ä»¶"
    
    results_dir = RESULTS_BASE_PATH / "global" / method
    paths = []
    for fname in filenames:
        p = results_dir / fname.strip()
        if p.exists():
            paths.append(str(p))
            
    if not paths:
        return gr.update(value=None), "âŒ æœªæ‰¾åˆ°é€‰ä¸­çš„æ–‡ä»¶"
        
    return (
        gr.update(value=paths, label="ğŸ“¥ ç‚¹å‡»æ­¤å¤„ä¸‹è½½ / Click to Download", visible=True),
        f"âœ… å·²å‡†å¤‡å¥½ {len(paths)} ä¸ªæ–‡ä»¶ï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹ä¸‹è½½åŒºåŸŸè¿›è¡Œä¸‹è½½"
    )


def get_result_filenames(method: str = "chatts") -> List[str]:
    """è·å–ç»“æœæ–‡ä»¶ååˆ—è¡¨ï¼ˆç”¨äºä¸‹æ‹‰æ¡†ï¼‰"""
    results_dir = RESULTS_BASE_PATH / "global" / method
    if not results_dir.exists():
        return []
    
    csv_files = list(results_dir.glob("*.csv"))
    csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return [f.name for f in csv_files[:20]]


def delete_result_file(method: str, filename: str) -> tuple:
    # å·²å¼ƒç”¨ï¼Œä½¿ç”¨ delete_selected_files
    pass


def get_training_configs() -> List[str]:
    """è·å–è®­ç»ƒé…ç½®åˆ—è¡¨"""
    configs = adapter.list_configs()
    return [c["name"] for c in configs]


def get_trained_models() -> List[str]:
    """è·å–å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨"""
    models = adapter.list_models()
    return [m["name"] for m in models]


def get_model_info(model_name: str) -> str:
    """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
    if not model_name:
        return "è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹"
    
    models = adapter.list_models()
    model = next((m for m in models if m["name"] == model_name), None)
    
    if not model:
        return "æ¨¡å‹ä¸å­˜åœ¨"
    
    info_lines = [
        f"**æ¨¡å‹åç§°**: {model['name']}",
        f"**ç±»å‹**: {model.get('type', 'unknown')}",
        f"**æ£€æŸ¥ç‚¹**: {', '.join(model.get('checkpoints', []))}",
        f"**è®­ç»ƒæ­¥æ•°**: {model.get('global_step', 'N/A')}",
    ]
    
    # è®­ç»ƒç»“æœ
    train_results = model.get("train_results", {})
    if train_results:
        info_lines.append(f"**è®­ç»ƒ Loss**: {train_results.get('train_loss', 'N/A'):.4f}")
        info_lines.append(f"**è®­ç»ƒæ—¶é•¿**: {train_results.get('train_runtime', 'N/A'):.1f}s")
    
    return "\n\n".join(info_lines)


def get_loss_plot(model_name: str):
    """è·å– Loss æ›²çº¿å›¾"""
    if not model_name:
        return None
    
    models = adapter.list_models()
    model = next((m for m in models if m["name"] == model_name), None)
    
    if not model or not model.get("loss_image"):
        return None
    
    loss_image = model.get("loss_image")
    if Path(loss_image).exists():
        return loss_image
    return None


def get_comparison_plot(model_names: List[str]):
    """è·å–å¤šä¸ªæ¨¡å‹çš„ Loss å¯¹æ¯”å›¾ (ä½¿ç”¨ Matplotlib åŠ¨æ€ç”Ÿæˆ)"""
    if not model_names or len(model_names) == 0:
        return None
    
    import matplotlib.pyplot as plt
    import pandas as pd
    
    plt.figure(figsize=(10, 6))
    
    for name in model_names:
        models = adapter.list_models()
        model = next((m for m in models if m["name"] == name), None)
        if not model:
            continue
            
        logs = adapter.get_training_log(model["path"])
        if not logs:
            continue
            
        df = pd.DataFrame([{"step": l.get("current_steps", 0), "loss": l.get("loss")} for l in logs if "loss" in l])
        if not df.empty:
            plt.plot(df["step"], df["loss"], label=name)
            
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.title("Model Comparison: Training Loss")
    plt.legend()
    plt.grid(True)
    
    # ä¿å­˜åˆ°é¡¹ç›®æœ¬åœ°ä¸´æ—¶ç›®å½•ï¼Œé¿å…ç³»ç»Ÿ /tmp æƒé™é—®é¢˜
    import uuid
    
    temp_dir = Path("temp_images")
    temp_dir.mkdir(exist_ok=True)
    
    output_path = temp_dir / f"compare_{uuid.uuid4().hex[:8]}.png"
    plt.savefig(str(output_path))
    plt.close()
    
    return str(output_path)


# ==================== æ•°æ®è·å–è¾…åŠ©å‡½æ•° ====================

def get_datasets_table() -> pd.DataFrame:
    """è·å–æ•°æ®é›†åˆ—è¡¨å¹¶è¿”å› DataFrame"""
    datasets = data_adapter.list_datasets()
    if not datasets:
        return pd.DataFrame(columns=["æ–‡ä»¶å", "å¤§å° (KB)", "ä¿®æ”¹æ—¶é—´"])
    
    from datetime import datetime
    rows = []
    for d in datasets:
        rows.append({
            "æ–‡ä»¶å": d["filename"],
            "å¤§å° (KB)": round(d["size_bytes"] / 1024, 2),
            "ä¿®æ”¹æ—¶é—´": datetime.fromtimestamp(d["modified_time"]).strftime("%Y-%m-%d %H:%M")
        })
    return pd.DataFrame(rows)


def get_dataset_names() -> List[str]:
    """è·å–æ•°æ®é›†æ–‡ä»¶ååˆ—è¡¨"""
    datasets = data_adapter.list_datasets()
    return [d["filename"] for d in datasets]


def delete_selected_dataset(filename: str):
    """åˆ é™¤é€‰ä¸­çš„æ•°æ®é›†"""
    print(f"[DEBUG] delete_selected_dataset called with: '{filename}'")
    if not filename:
        return get_datasets_table(), gr.Dropdown(choices=get_dataset_names(), value=None), "âŒ No dataset selected"
    
    result = data_adapter.delete_dataset(filename)
    if result.get("success"):
        # åˆ·æ–°åˆ—è¡¨
        new_table = get_datasets_table()
        new_choices = get_dataset_names()
        return new_table, gr.Dropdown(choices=new_choices, value=None), f"âœ… Deleted: {filename}"
    else:
        return get_datasets_table(), gr.Dropdown(choices=get_dataset_names()), f"âŒ {result.get('error')}"


def preview_dataset(filename: str) -> tuple:
    """é¢„è§ˆæ•°æ®é›†ï¼Œè¿”å› (è¡¨æ ¼æ•°æ®, åˆ—é€‰æ‹©å™¨æ›´æ–°, æ›²çº¿å›¾)"""
    print(f"[DEBUG] preview_dataset called with filename: '{filename}'")
    
    if isinstance(filename, list):
        filename = filename[0] if filename else None
    
    if not filename:
        print("[DEBUG] Empty filename, returning empty")
        return [], gr.CheckboxGroup(choices=[], value=[]), None
    
    try:
        # è·å–é¢„è§ˆæ•°æ®
        print(f"[DEBUG] Calling preview_csv for: {filename}")
        data = data_adapter.preview_csv(filename, limit=5000)
        print(f"[DEBUG] preview_csv returned {len(data)} records")
        
        df = pd.DataFrame(data)
        print(f"[DEBUG] DataFrame created: shape={df.shape}, columns={df.columns.tolist()}")
        
        # è¿‡æ»¤æ‰ Unnamed å’Œ category åˆ—
        df = df.loc[:, ~df.columns.str.contains('^Unnamed|^category', case=False)]
        print(f"[DEBUG] After filtering: shape={df.shape}, columns={df.columns.tolist()}")
        
        # è·å–æ•°å€¼åˆ—ä½œä¸ºå¯é€‰é¡¹
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        print(f"[DEBUG] Numeric columns: {numeric_cols}")
        
        # é»˜è®¤é€‰ä¸­ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—
        default_selected = numeric_cols[:1] if numeric_cols else []
        print(f"[DEBUG] Default selected: {default_selected}")
        
        # ç”Ÿæˆé»˜è®¤æ›²çº¿å›¾
        plot_path = generate_plot(df, filename, default_selected)
        print(f"[DEBUG] Plot generated: {plot_path}")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œç¡®ä¿ Gradio 6.x å…¼å®¹
        # ä½¿ç”¨ values åˆ—è¡¨ + headers çš„æ–¹å¼
        table_data = df.values.tolist()
        headers = df.columns.tolist()
        print(f"[DEBUG] Table data rows: {len(table_data)}, headers: {headers}")
        
        return gr.Dataframe(value=table_data, headers=headers), gr.CheckboxGroup(choices=numeric_cols, value=default_selected), plot_path
    except Exception as e:
        import traceback
        print(f"[DEBUG ERROR] Exception: {e}")
        traceback.print_exc()
        return [], gr.CheckboxGroup(choices=[], value=[]), None


def generate_plot(df: pd.DataFrame, filename: str, selected_cols: list):
    """æ ¹æ®é€‰æ‹©çš„åˆ—ç”Ÿæˆæ›²çº¿å›¾"""
    if df.empty or not selected_cols:
        return None
    
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 4))
    
    for col in selected_cols:
        if col in df.columns:
            plt.plot(df.index, df[col], label=col, alpha=0.8)
    
    plt.xlabel("Index")
    plt.ylabel("Value")
    plt.title(f"Data Preview: {filename}")
    if selected_cols:
        plt.legend()
    plt.grid(True, alpha=0.3)
    
    temp_dir = Path("temp_images")
    temp_dir.mkdir(exist_ok=True)
    import uuid
    plot_path = temp_dir / f"preview_{uuid.uuid4().hex[:8]}.png"
    plt.savefig(str(plot_path), dpi=100, bbox_inches='tight')
    plt.close()
    
    return str(plot_path)


def update_plot_from_selection(filename: str, selected_cols: list):
    """æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„åˆ—æ›´æ–°æ›²çº¿å›¾"""
    if not filename or not selected_cols:
        return None
    
    try:
        data = data_adapter.preview_csv(filename, limit=5000)
        df = pd.DataFrame(data)
        df = df.loc[:, ~df.columns.str.contains('^Unnamed|^category', case=False)]
        return generate_plot(df, filename, selected_cols)
    except:
        return None


def start_acquire_task(
    source: str, 
    host: str,
    port: str,
    user: str,
    password: str,
    point_name: str,
    start_time: str,
    end_time: str,
    target_points: int
):
    """å¯åŠ¨æ•°æ®é‡‡é›†ä»»åŠ¡ï¼ˆæµå¼è¾“å‡ºæ—¥å¿—ï¼‰"""
    if not source:
        yield "âŒ Please enter IoTDB source path"
        return
    
    # ä½¿ç”¨æµå¼è¾“å‡ºç‰ˆæœ¬
    for log in data_adapter.run_acquire_task_streaming(
        task_id="manual",
        source=source,
        host=host,
        port=port,
        user=user,
        password=password,
        point_name=point_name,
        target_points=int(target_points),
        start_time=start_time,
        end_time=end_time
    ):
        yield log


# ==================== æ¨ç†ç›‘æ§è¾…åŠ©å‡½æ•° ====================

def get_algorithms() -> List[str]:
    """è·å–å¯ç”¨ç®—æ³•åˆ—è¡¨"""
    return ["chatts", "adtk_hbos", "ensemble", "timer"]


def get_inference_models() -> List[str]:
    """è·å–å¯ç”¨äºæ¨ç†çš„æ¨¡å‹åˆ—è¡¨"""
    # è¿‡æ»¤ lora æ¨¡å‹ï¼ˆå‡è®¾ ChatTS æ¨ç†ä¸»è¦ç”¨ LoRAï¼‰
    models = training_adapter.list_models()
    return [m["path"] for m in models] # ç›´æ¥è¿”å›è·¯å¾„ï¼Œæ–¹ä¾¿ adapter å¤„ç†

def toggle_algo_params(algorithm: str):
    """æ ¹æ®é€‰æ‹©çš„ç®—æ³•åˆ‡æ¢å‚æ•°ç»„å¯è§æ€§"""
    show_chatts = (algorithm == "chatts")
    show_timer = (algorithm == "timer")
    show_adtk = (algorithm == "adtk_hbos")
    return (
        gr.update(visible=show_chatts), 
        gr.update(visible=show_timer), 
        gr.update(visible=show_adtk)
    )

def start_inference_task(
    algorithm: str, 
    base_model_path: str,
    lora_adapter_path: str,
    files: List[str],
    n_downsample: int,
    threshold: float,
    # ChatTS args
    load_in_4bit: str,
    prompt_template: str,
    max_new_tokens: int,
    chatts_device: str,
    chatts_use_cache: str,
    # Timer args
    timer_device: str,
    timer_lookback: int,
    timer_threshold_k: float,
    timer_method: str,
    timer_streaming: bool,
    # ADTK args
    adtk_bin_nums: int,
    adtk_hbos_ratio: float
):
    """å¯åŠ¨æ¨ç†ä»»åŠ¡"""
    if not algorithm:
        yield "âŒ è¯·é€‰æ‹©ç®—æ³•", "âŒ è¯·é€‰æ‹©ç®—æ³•"
        return
    if not files:
        yield "âŒ è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶", "âŒ è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶"
        return
    
    # å°†é€‰ä¸­çš„æ–‡ä»¶åè½¬æ¢ä¸ºå®Œæ•´è·¯å¾„
    file_paths = []
    for f in files:
        full_path = data_adapter.data_path / f
        if full_path.exists():
            file_paths.append(str(full_path))
    
    if not file_paths:
        yield "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶", "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶"
        return
    
    import uuid
    task_id = str(uuid.uuid4())
    
    # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
    from datetime import datetime
    from src.db.database import SessionLocal, Task
    db = SessionLocal()
    try:
        task = Task(
            id=task_id,
            type="inference",
            status="running",
            config=json.dumps({
                "algorithm": algorithm,
                "files": files,
                "base_model_path": base_model_path,
                "lora_adapter_path": lora_adapter_path
            }),
            created_at=datetime.utcnow(),
            started_at=datetime.utcnow()
        )
        db.add(task)
        db.commit()
    except Exception as e:
        print(f"[DB Error] Failed to save task: {e}")
    finally:
        db.close()
    
    yield (
        f"ğŸš€ ä»»åŠ¡å·²å¯åŠ¨ (ID: {task_id[:8]})\\næ­£åœ¨å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...", 
        f"ğŸš€ ä»»åŠ¡å·²å¯åŠ¨ (ID: {task_id[:8]})",
        gr.update(visible=True), # Show stop button
        gr.update(visible=False), # Hide submit button
        task_id, # Return task_id to state
        None # download_files
    )
    
    try:
        # å‡†å¤‡é«˜çº§å‚æ•°
        advanced_args = {
            "n_downsample": n_downsample,
            "threshold": threshold,
            "base_model_path": base_model_path,
            "lora_adapter_path": lora_adapter_path, 
            # ChatTS
            "chatts_load_in_4bit": load_in_4bit,
            "chatts_prompt_template": prompt_template,
            "chatts_max_new_tokens": max_new_tokens,
            "chatts_device": chatts_device,
            "chatts_use_cache": chatts_use_cache,
            # Timer
            "timer_device": timer_device,
            "timer_lookback_length": timer_lookback,
            "timer_threshold_k": timer_threshold_k,
            "timer_method": timer_method,
            "timer_streaming": timer_streaming,
            # ADTK
            "bin_nums": adtk_bin_nums,
            "hbos_ratio": adtk_hbos_ratio
        }
        
        generated_files = []
        
        # æ‰§è¡Œæ¨ç†ï¼ˆæµå¼ï¼‰
        accumulated_log = ""
        for log_chunk in inference_adapter.run_batch_inference_streaming(
            task_id=task_id,
            model=lora_adapter_path, # å…¼å®¹æ—§æ¥å£å‘½åï¼Œå®é™…é€»è¾‘åœ¨ adapter ä¸­å·²å¤„ç†
            algorithm=algorithm,
            input_files=file_paths,
            **advanced_args
        ):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„è¿”å›
            if isinstance(log_chunk, dict) and "file_path" in log_chunk:
                # adapter è¿”å›äº†å®Œæ•´è·¯å¾„
                generated_files.append(log_chunk["file_path"])
            elif isinstance(log_chunk, dict) and "file_name" in log_chunk:
                # å…¼å®¹æ—§æ ¼å¼ï¼Œä»…æœ‰æ–‡ä»¶å
                generated_files.append(log_chunk["file_name"])
            elif isinstance(log_chunk, dict):
                 pass # å…¶ä»–ç»“æ„åŒ–æ¶ˆæ¯
            else:
                accumulated_log += log_chunk
                yield (
                    accumulated_log, 
                    "ğŸ”„ æ­£åœ¨æ‰§è¡Œ...",
                    gr.update(visible=True),
                    gr.update(visible=False),
                    task_id,
                    None
                )
        
        # ä»»åŠ¡ç»“æŸï¼Œå°è¯•æŸ¥æ‰¾ç”Ÿæˆçš„ç»“æœæ–‡ä»¶
        # å‡è®¾ä¿å­˜åœ¨ /home/share/results/data/<method> ä¸‹ï¼ŒæŒ‰æ—¶é—´æœ€æ–°æŸ¥æ‰¾ï¼Ÿ
        # è¿™æ¯”è¾ƒ hackyã€‚æ›´å¥½çš„æ–¹æ³•æ˜¯ adapter è¿”å›ã€‚
        # æˆ‘ä»¬åœ¨ adapter ä¸­å¢åŠ äº† yield {"file_name": ...} é€»è¾‘
        # è¿™é‡Œéœ€è¦å¤„ç†å®ƒã€‚
        
        # æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        db = SessionLocal()
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "completed"
                task.completed_at = datetime.utcnow()
                db.commit()
        except Exception as e:
            print(f"[DB Error] Failed to update task: {e}")
        finally:
            db.close()
        
        yield (
            accumulated_log + "\nâœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ", 
            "âœ… ä»»åŠ¡å®Œæˆ",
             gr.update(visible=False),
             gr.update(visible=True),
             task_id,
             generated_files # TODO: å¡«å…… output files if capture logic works perfectly
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        
        # æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        db = SessionLocal()
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "failed"
                task.error = str(e)
                task.completed_at = datetime.utcnow()
                db.commit()
        except Exception as db_e:
            print(f"[DB Error] Failed to update task: {db_e}")
        finally:
            db.close()
        
        yield (
            f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", 
            f"âŒ é”™è¯¯: {str(e)}",
            gr.update(visible=False),
            gr.update(visible=True),
            None,
            None
        )

def stop_task_action(task_id_state):
    """å®é™…æ‰§è¡Œåœæ­¢åŠ¨ä½œ"""
    print(f"DEBUG: Stop requested for task ID: {task_id_state}")
    if task_id_state:
        if inference_adapter.stop_inference_task(task_id_state):
            print(f"DEBUG: Stop successful for {task_id_state}")
            return "ğŸ›‘ ä»»åŠ¡å·²è¯·æ±‚åœæ­¢", gr.update(visible=False), gr.update(visible=True), None, None
        else:
            print(f"DEBUG: Stop failed for {task_id_state} (not found or error)")
            return f"âŒ åœæ­¢å¤±è´¥: ä»»åŠ¡ {task_id_state} ä¸å­˜åœ¨æˆ–å·²ç»“æŸ", gr.update(visible=True), gr.update(visible=False), task_id_state, None
    print("DEBUG: No active task ID found")
    return "âš ï¸ æ— æ´»åŠ¨ä»»åŠ¡", gr.update(visible=False), gr.update(visible=True), None, None


def get_task_status_table() -> pd.DataFrame:
    """è·å–ä»»åŠ¡çŠ¶æ€åˆ—è¡¨ (ä»æ•°æ®åº“è¯»å–)"""
    try:
        from src.db.database import SessionLocal, Task
        db = SessionLocal()
        tasks = db.query(Task).order_by(Task.created_at.desc()).limit(20).all()
        db.close()
        
        if not tasks:
            return pd.DataFrame(columns=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"])
        
        rows = []
        for t in tasks:
            rows.append({
                "ID": t.id[:8] + "...",
                "ç±»å‹": t.type,
                "çŠ¶æ€": t.status,
                "åˆ›å»ºæ—¶é—´": t.created_at.strftime("%H:%M:%S") if t.created_at else "N/A"
            })
        return pd.DataFrame(rows)
    except Exception as e:
        return pd.DataFrame({"é”™è¯¯": [str(e)]})


def clear_task_history() -> tuple:
    """æ¸…ç©ºä»»åŠ¡å†å²è®°å½•"""
    try:
        from src.db.database import SessionLocal, Task
        db = SessionLocal()
        deleted = db.query(Task).delete()
        db.commit()
        db.close()
        return pd.DataFrame(columns=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"]), f"âœ… å·²æ¸…ç©º {deleted} æ¡å†å²è®°å½•"
    except Exception as e:
        return get_task_status_table(), f"âŒ æ¸…ç©ºå¤±è´¥: {str(e)}"


def start_training(
    config_name: str,
    learning_rate: str,
    num_epochs: float,
    batch_size: int,
    lora_rank: int,
    lora_alpha: int,
    output_name: str
) -> str:
    """å¯åŠ¨è®­ç»ƒ"""
    if not config_name:
        return "âŒ è¯·é€‰æ‹©è®­ç»ƒé…ç½®"
    
    if not output_name:
        return "âŒ è¯·è¾“å…¥è¾“å‡ºç›®å½•åç§°"
    
    # è°ƒç”¨é€‚é…å™¨å¯åŠ¨è®­ç»ƒ
    import uuid
    task_id = str(uuid.uuid4())[:8]
    
    try:
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ adapter.run_trainingï¼Œä½†ç”±äºæ˜¯åå°ä»»åŠ¡ï¼Œå…ˆè¿”å›æç¤º
        return f"""âœ… è®­ç»ƒä»»åŠ¡å·²æäº¤

**ä»»åŠ¡ ID**: {task_id}
**é…ç½®**: {config_name}
**è¾“å‡ºç›®å½•**: {output_name}

**å‚æ•°**:
- å­¦ä¹ ç‡: {learning_rate}
- è®­ç»ƒè½®æ•°: {num_epochs}
- æ‰¹æ¬¡å¤§å°: {batch_size}
- LoRA Rank: {lora_rank}
- LoRA Alpha: {lora_alpha}

è¯·é€šè¿‡ API `/api/v1/training/status/{task_id}` æŸ¥è¯¢è¿›åº¦ã€‚
"""
    except Exception as e:
        return f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}"


def create_training_ui() -> gr.Blocks:
    """åˆ›å»ºç»Ÿä¸€ç®¡ç†ç•Œé¢ï¼ˆæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒï¼‰"""
    
    with gr.Blocks(title="TS-Iteration-Loop", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ”„ TS-Iteration-Loop æ—¶åºè¿­ä»£å¹³å°")
        gr.Markdown("æ•´åˆæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒçš„ç»Ÿä¸€ç®¡ç†ç•Œé¢")
        
        # ==================== æ•°æ®è·å– Tab ====================
        with gr.Tab("ğŸ“ æ•°æ®è·å–"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ•°æ®é›†åˆ—è¡¨")
                    datasets_table = gr.Dataframe(
                        value=get_datasets_table(),
                        label="å·²æœ‰æ•°æ®é›†",
                        interactive=False
                    )
                    refresh_datasets_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                    
                    gr.Markdown("### é¢„è§ˆæ•°æ®")
                    preview_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æ•°æ®é›†",
                        choices=get_dataset_names(),
                        interactive=True
                    )
                    with gr.Row():
                        delete_dataset_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop", size="sm")
                        delete_status = gr.Textbox(label="", visible=False)
                    
                    column_selector = gr.CheckboxGroup(
                        label="Select columns to plot",
                        choices=[],
                        interactive=True
                    )
                
                with gr.Column(scale=2):
                    gr.Markdown("### æ•°æ®é‡‡é›†é…ç½®")
                    
                    with gr.Accordion("IoTDB è¿æ¥é…ç½®", open=False):
                        with gr.Row():
                            host_input = gr.Textbox(label="Host", value="192.168.199.185")
                            port_input = gr.Textbox(label="Port", value="6667")
                        with gr.Row():
                            user_input = gr.Textbox(label="User", value="root")
                            pwd_input = gr.Textbox(label="Password", value="root", type="password")

                    gr.Markdown("### æŸ¥è¯¢å‚æ•°")
                    source_input = gr.Textbox(
                        label="IoTDB æºè·¯å¾„ (Path)",
                        placeholder="root.zhlh_202307_202412.ZHLH_4C_1216",
                        value="root.zhlh_202307_202412.ZHLH_4C_1216",
                        scale=2
                    )
                    
                    with gr.Row():
                         point_input = gr.Textbox(
                            label="ç‚¹ä½åç§° (Point Name)",
                            placeholder="FI_10401C.PV (ç•™ç©ºæŸ¥è¯¢æ‰€æœ‰*)",
                            value="FI_10401C.PV"
                        )
                    
                    with gr.Row():
                        start_time_input = gr.Textbox(label="å¼€å§‹æ—¶é—´", value="2023-07-18 12:00:00")
                        end_time_input = gr.Textbox(label="ç»“æŸæ—¶é—´", value="2024-11-05 23:59:59")

                    target_points = gr.Slider(
                        label="ç›®æ ‡ç‚¹æ•°",
                        minimum=1000,
                        maximum=10000,
                        value=5000,
                        step=500,
                        scale=1
                    )
                    
                    acquire_btn = gr.Button("ğŸ“¥ å¼€å§‹é‡‡é›†", variant="primary")
                    acquire_output = gr.Markdown(value="ç­‰å¾…é‡‡é›†...")
            
            # æ•°æ®é¢„è§ˆåŒºåŸŸ - å›¾è¡¨ä¼˜å…ˆï¼Œè¡¨æ ¼å¯æŠ˜å 
            with gr.Row():
                preview_plot = gr.Image(label="Curve Preview", height=350)
            
            with gr.Accordion("ğŸ“‹ Data Table (first 5000 rows)", open=False):
                preview_table = gr.Dataframe(
                    label="",
                    interactive=False
                )
            
            # äº‹ä»¶ç»‘å®š - æ•°æ®è·å–
            refresh_datasets_btn.click(
                fn=get_datasets_table,
                outputs=datasets_table
            )
            refresh_datasets_btn.click(
                fn=lambda: gr.Dropdown(choices=get_dataset_names()),
                outputs=preview_dropdown
            )
            delete_dataset_btn.click(
                fn=delete_selected_dataset,
                inputs=preview_dropdown,
                outputs=[datasets_table, preview_dropdown, delete_status]
            )
            preview_dropdown.change(
                fn=preview_dataset,
                inputs=preview_dropdown,
                outputs=[preview_table, column_selector, preview_plot]
            )
            column_selector.change(
                fn=update_plot_from_selection,
                inputs=[preview_dropdown, column_selector],
                outputs=preview_plot
            )
            acquire_btn.click(
                fn=start_acquire_task,
                inputs=[
                    source_input, host_input, port_input, user_input, pwd_input,
                    point_input, start_time_input, end_time_input, target_points
                ],
                outputs=acquire_output
            )
        
        # ==================== æ¨ç†ç›‘æ§ Tab ====================
        with gr.Tab("ğŸ” æ¨ç†ç›‘æ§"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ–°å»ºæ¨ç†ä»»åŠ¡")
                    algo_dropdown = gr.Dropdown(
                        label="é€‰æ‹©ç®—æ³•",
                        choices=get_algorithms(),
                        value="chatts",
                        interactive=True
                    )
                    
                    # æ¨¡å‹é…ç½®ç»„
                    with gr.Group():
                        base_model_input = gr.Textbox(
                            label="Base Model Path (Base Model)", 
                            value="/home/share/llm_models/bytedance-research/ChatTS-8B",
                            info="åŸºç¡€æ¨¡å‹è·¯å¾„"
                        )
                        lora_adapter_select = gr.Dropdown(
                            label="LoRA Adapter Path (å¯é€‰)",
                            choices=get_inference_models(), # è¿”å›çš„æ˜¯ LoRA è·¯å¾„åˆ—è¡¨
                            interactive=True,
                            info="å¾®è°ƒåçš„ LoRA é€‚é…å™¨è·¯å¾„"
                        )
                        
                    files_select = gr.CheckboxGroup(
                        label="é€‰æ‹©è¾“å…¥æ–‡ä»¶",
                        choices=get_dataset_names()
                    )
                    
                    with gr.Accordion("âš™ï¸ é«˜çº§é…ç½® (å¯é€‰)", open=False):
                        with gr.Row():
                            n_downsample_input = gr.Slider(
                                label="é™é‡‡æ ·ç‚¹æ•° (n_downsample)", 
                                minimum=100, maximum=10000, step=100, value=settings.DEFAULT_DOWNSAMPLE_POINTS
                            )
                            threshold_input = gr.Number(
                                label="å¼‚å¸¸é˜ˆå€¼ (threshold)", value=8.0
                            )
                        
                        # ChatTS ä¸“å±å‚æ•°
                        with gr.Group(visible=True) as chatts_group:
                            gr.Markdown("#### ChatTS é…ç½®")
                            with gr.Row():
                                load_in_4bit_input = gr.Dropdown(
                                    label="4-bit é‡åŒ–", choices=["auto", "true", "false"], value="auto",
                                    info="æ˜¾å­˜ä¸è¶³æ—¶å»ºè®®å¼€å¯(true)"
                                )
                                prompt_template_input = gr.Dropdown(
                                    label="Prompt æ¨¡æ¿",
                                    choices=["default", "detailed", "minimal", "industrial", "english"],
                                    value="default"
                                )
                            with gr.Row():
                                chatts_device_input = gr.Textbox(label="Device", value="cuda:1")
                                chatts_use_cache_input = gr.Dropdown(
                                    label="Use Cache (KV)", choices=["auto", "true", "false"], value="auto"
                                )
                            max_new_tokens_input = gr.Number(
                                label="æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max New Tokens)", value=4096, precision=0
                            )

                        # Timer ä¸“å±å‚æ•°
                        with gr.Group(visible=False) as timer_group:
                            gr.Markdown("#### Timer é…ç½®")
                            with gr.Row():
                                timer_device_input = gr.Textbox(label="Device", value="cuda:0")
                                timer_lookback_input = gr.Number(label="Lookback Length", value=256, precision=0)
                            with gr.Row():
                                timer_threshold_k_input = gr.Number(label="Threshold K", value=3.5)
                                timer_method_input = gr.Dropdown(label="Method", choices=["mad", "sigma"], value="mad")
                            timer_streaming_input = gr.Checkbox(label="Enable Streaming Mode", value=False)
                            
                        # ADTK ä¸“å±å‚æ•°
                        with gr.Group(visible=False) as adtk_group:
                            gr.Markdown("#### ADTK HBOS é…ç½®")
                            with gr.Row():
                                adtk_bin_nums_input = gr.Number(label="Bin Nums (åˆ†ç®±æ•°)", value=20, precision=0)
                                adtk_hbos_ratio_input = gr.Number(label="HBOS Ratio (è·³å˜é˜ˆå€¼)", value=None)

                    with gr.Row():
                        submit_inference_btn = gr.Button("ğŸš€ æäº¤ä»»åŠ¡", variant="primary")
                        stop_inference_btn = gr.Button("ğŸ›‘ åœæ­¢ä»»åŠ¡", variant="stop", visible=False)
                    
                    # éšè—çš„çŠ¶æ€ç»„ä»¶ï¼Œç”¨äºå­˜å‚¨ current task id
                    current_task_id_state = gr.State("")
                
                with gr.Column(scale=2):
                    gr.Markdown("### ä»»åŠ¡çŠ¶æ€ & æ—¥å¿—")
                    with gr.Tabs():
                        with gr.Tab("å®æ—¶æ—¥å¿—"):
                            inference_logs = gr.Textbox(
                                value="",
                                label="Execution Logs",
                                interactive=False,
                                lines=20,
                                max_lines=20,
                                autoscroll=True
                            )
                        with gr.Tab("ä»»åŠ¡ç»“æœ"):
                             # å½“å‰ä»»åŠ¡çŠ¶æ€
                             inference_result_md = gr.Markdown(value="ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                             download_files = gr.File(label="å½“å‰ä»»åŠ¡ç»“æœ", file_count="multiple", interactive=False, visible=False)
                             
                             # æ•´åˆçš„ç»“æœæ–‡ä»¶ç®¡ç†åŒº
                             gr.Markdown("### ğŸ“‚ ç»“æœæ–‡ä»¶ç®¡ç†")
                             with gr.Row():
                                 results_method_select = gr.Dropdown(
                                     label="ç­›é€‰æ–¹æ³•",
                                     choices=["chatts", "timer", "adtk_hbos"],
                                     value="chatts",
                                     scale=1
                                 )
                                 refresh_results_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm", scale=0)
                             
                             # ç»Ÿä¸€æ–‡ä»¶åˆ—è¡¨ï¼ˆå¤šé€‰ï¼‰
                             file_manager_list = gr.CheckboxGroup(
                                 label="æ–‡ä»¶åˆ—è¡¨ (æ–‡ä»¶å | è¾ƒæ–°çš„åœ¨å‰)",
                                 choices=get_result_filenames("chatts"),
                                 value=[],
                                 interactive=True
                             )
                             
                             with gr.Row():
                                 download_selected_btn = gr.Button("â¬‡ï¸ ä¸‹è½½é€‰ä¸­", size="sm")
                                 delete_selected_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop", size="sm")

                             operation_status = gr.Markdown(value="")

                             # ä¸‹è½½åŒºåŸŸ (åŠ¨æ€æ˜¾ç¤º)
                             history_download_files = gr.File(
                                 label="ğŸ“¥ ä¸‹è½½åŒºåŸŸ (è¯·å…ˆé€‰æ‹©æ–‡ä»¶å¹¶ç‚¹å‡»â€œä¸‹è½½é€‰ä¸­â€)",
                                 file_count="multiple",
                                 interactive=False,
                                 visible=True
                             )
                    
                    # ä»»åŠ¡å†å²è®°å½• - æ”¾å…¥å¯æŠ˜å åŒºåŸŸ
                    with gr.Accordion("ğŸ“‹ ä»»åŠ¡å†å²è®°å½•", open=False):
                        with gr.Row():
                            refresh_tasks_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", size="sm")
                            clear_tasks_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", size="sm", variant="stop")
                        clear_status = gr.Markdown(value="", visible=True)
                        task_table = gr.Dataframe(
                            headers=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"],
                            value=[],
                            interactive=False
                        )
            
            # äº‹ä»¶ç»‘å®š - æ¨ç†ç›‘æ§
            
            # æäº¤ä»»åŠ¡
            submit_event = submit_inference_btn.click(
                fn=start_inference_task,
                inputs=[
                    algo_dropdown, base_model_input, lora_adapter_select, files_select,
                    # é€šç”¨å‚æ•°
                    n_downsample_input, threshold_input,
                    # ChatTS å‚æ•°
                    load_in_4bit_input, prompt_template_input, max_new_tokens_input, chatts_device_input, chatts_use_cache_input,
                    # Timer å‚æ•°
                    timer_device_input, timer_lookback_input, timer_threshold_k_input, timer_method_input, timer_streaming_input,
                    # ADTK å‚æ•°
                    adtk_bin_nums_input, adtk_hbos_ratio_input
                ],
                outputs=[
                    inference_logs, 
                    inference_result_md, 
                    stop_inference_btn, 
                    submit_inference_btn, 
                    current_task_id_state, 
                    download_files
                ]
            )
            
            # åœæ­¢ä»»åŠ¡
            stop_inference_btn.click(
                fn=stop_task_action,
                inputs=[current_task_id_state],
                outputs=[
                    inference_result_md, 
                    stop_inference_btn, 
                    submit_inference_btn, 
                    current_task_id_state, 
                    download_files
                ]
            )
            
            refresh_tasks_btn.click(
                fn=get_task_status_table,
                outputs=task_table
            )
            refresh_tasks_btn.click(
                fn=lambda: gr.CheckboxGroup(choices=get_dataset_names()),
                outputs=files_select
            )
            refresh_tasks_btn.click(
                fn=lambda: gr.Dropdown(choices=get_inference_models()),
                outputs=lora_adapter_select
            )
            
            # æ¸…ç©ºå†å²è®°å½•
            clear_tasks_btn.click(
                fn=clear_task_history,
                outputs=[task_table, clear_status]
            )
            
            # å†å²ç»“æœæ–‡ä»¶åˆ·æ–°
            refresh_results_btn.click(
                fn=lambda m: gr.CheckboxGroup(choices=get_result_filenames(m)),
                inputs=results_method_select,
                outputs=file_manager_list
            )
            
            # åˆ‡æ¢æ–¹æ³•æ—¶åˆ·æ–°ç»“æœåˆ—è¡¨
            results_method_select.change(
                fn=lambda m: gr.CheckboxGroup(choices=get_result_filenames(m)),
                inputs=results_method_select,
                outputs=file_manager_list
            )
            
            # åˆ é™¤é€‰ä¸­æ–‡ä»¶
            delete_selected_btn.click(
                fn=delete_selected_files,
                inputs=[results_method_select, file_manager_list],
                outputs=[file_manager_list, history_download_files, operation_status]
            )
            
            # ä¸‹è½½é€‰ä¸­æ–‡ä»¶
            # ä¸‹è½½é€‰ä¸­æ–‡ä»¶
            download_selected_btn.click(
                fn=prepare_download_files,
                inputs=[results_method_select, file_manager_list],
                outputs=[history_download_files, operation_status]
            )
            
            # ç®—æ³•åˆ‡æ¢äº‹ä»¶ï¼šæ§åˆ¶å‚æ•°ç»„æ˜¾ç¤º
            algo_dropdown.change(
                fn=toggle_algo_params,
                inputs=algo_dropdown,
                outputs=[chatts_group, timer_group, adtk_group]
            )
        
        # ==================== æ ‡æ³¨å·¥å…· Tab ====================
        with gr.Tab("ğŸ·ï¸ æ ‡æ³¨å·¥å…·"):
            gr.Markdown("### æ—¶åºæ•°æ®æ ‡æ³¨")
            gr.Markdown(f"""
> [!NOTE]
> æ ‡æ³¨å·¥å…·è¿è¡Œåœ¨ç‹¬ç«‹æœåŠ¡ä¸Šï¼Œç‚¹å‡»ä¸‹æ–¹é“¾æ¥è·³è½¬ã€‚

**æ ‡æ³¨å·¥å…·åœ°å€**: [http://localhost:5000](http://localhost:5000)

---

### ä½¿ç”¨è¯´æ˜

1. **æ‰“å¼€æ ‡æ³¨å·¥å…·**: ç‚¹å‡»ä¸Šæ–¹é“¾æ¥è¿›å…¥æ ‡æ³¨ç•Œé¢
2. **é€‰æ‹©æ•°æ®æ–‡ä»¶**: åœ¨æ ‡æ³¨å·¥å…·ä¸­é€‰æ‹©è¦æ ‡æ³¨çš„ CSV æ–‡ä»¶
3. **è¿›è¡Œæ ‡æ³¨**: ä½¿ç”¨æ¡†é€‰å·¥å…·æ ‡è®°å¼‚å¸¸åŒºé—´
4. **ä¿å­˜æ ‡æ³¨**: å®Œæˆåä¿å­˜æ ‡æ³¨ç»“æœ

### æ ‡æ³¨ä¸è¿­ä»£æµç¨‹

```
ğŸ“ æ•°æ®è·å– â†’ ğŸ·ï¸ äººå·¥æ ‡æ³¨ â†’ ğŸ¯ å¾®è°ƒè®­ç»ƒ â†’ ğŸ” æ¨ç†æ£€æµ‹ â†’ ğŸ·ï¸ å®¡æ ¸ä¿®æ­£ â†’ ğŸ¯ å†æ¬¡å¾®è°ƒ â†’ ...
```

### å¿«é€Ÿæ“ä½œ
""")
            with gr.Row():
                open_annotator_btn = gr.Button("ğŸ”— æ‰“å¼€æ ‡æ³¨å·¥å…· (æ–°æ ‡ç­¾é¡µ)", variant="primary", size="lg")
                gr.Markdown("""
<script>
function openAnnotator() {
    window.open('http://localhost:5000', '_blank');
}
</script>
""", visible=False)
            
            gr.Markdown("### å½“å‰æ ‡æ³¨çŠ¶æ€")
            with gr.Row():
                with gr.Column():
                    gr.Markdown("**å¯æ ‡æ³¨æ–‡ä»¶æ•°é‡**")
                    annotatable_count = gr.Textbox(
                        value=f"{len(get_dataset_names())} ä¸ªæ–‡ä»¶",
                        interactive=False,
                        show_label=False
                    )
                with gr.Column():
                    gr.Markdown("**æ ‡æ³¨å·¥å…·çŠ¶æ€**")
                    annotator_status = gr.Textbox(
                        value="è¯·è®¿é—®æ ‡æ³¨å·¥å…·æŸ¥çœ‹",
                        interactive=False,
                        show_label=False
                    )
            
            # JavaScript è·³è½¬ (Gradio é™åˆ¶ï¼Œä½¿ç”¨ HTML)
            open_annotator_btn.click(
                fn=lambda: "âœ… è¯·åœ¨æ–°æ ‡ç­¾é¡µä¸­æŸ¥çœ‹æ ‡æ³¨å·¥å…· (http://localhost:5000)",
                outputs=annotator_status
            )
        
        # ==================== å¾®è°ƒè®­ç»ƒ Tab (åŸæœ‰) ====================
        with gr.Tab("ğŸ¯ å¼€å§‹è®­ç»ƒ"):
            with gr.Row():
                with gr.Column(scale=2):
                    # åŸºç¡€é…ç½®
                    gr.Markdown("### åŸºç¡€é…ç½®")
                    config_dropdown = gr.Dropdown(
                        label="è®­ç»ƒé…ç½®",
                        choices=get_training_configs(),
                        interactive=True
                    )
                    output_name = gr.Textbox(
                        label="è¾“å‡ºç›®å½•åç§°",
                        placeholder="ä¾‹å¦‚: my_model_v1"
                    )
                    
                    with gr.Row():
                        learning_rate = gr.Textbox(
                            label="å­¦ä¹ ç‡",
                            value="2e-5"
                        )
                        num_epochs = gr.Number(
                            label="è®­ç»ƒè½®æ•°",
                            value=3.0
                        )
                        batch_size = gr.Slider(
                            label="æ‰¹æ¬¡å¤§å°",
                            minimum=1,
                            maximum=32,
                            value=2,
                            step=1
                        )
                    
                    # LoRA é…ç½®ï¼ˆæŠ˜å ï¼‰
                    with gr.Accordion("LoRA é…ç½®", open=False):
                        with gr.Row():
                            lora_rank = gr.Slider(
                                label="LoRA Rank",
                                minimum=1,
                                maximum=128,
                                value=8,
                                step=1
                            )
                            lora_alpha = gr.Slider(
                                label="LoRA Alpha",
                                minimum=1,
                                maximum=256,
                                value=16,
                                step=1
                            )
                    
                    # æ§åˆ¶æŒ‰é’®
                    with gr.Row():
                        start_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary")
                        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°é…ç½®")
                
                with gr.Column(scale=1):
                    # è¾“å‡ºåŒºåŸŸ
                    gr.Markdown("### è®­ç»ƒçŠ¶æ€")
                    output_box = gr.Markdown(value="ç­‰å¾…å¼€å§‹è®­ç»ƒ...")
            
            # äº‹ä»¶ç»‘å®š
            start_btn.click(
                fn=start_training,
                inputs=[
                    config_dropdown, learning_rate, num_epochs,
                    batch_size, lora_rank, lora_alpha, output_name
                ],
                outputs=output_box
            )
            refresh_btn.click(
                fn=lambda: gr.Dropdown(choices=get_training_configs()),
                outputs=config_dropdown
            )
        
        with gr.Tab("ğŸ“Š å·²è®­ç»ƒæ¨¡å‹"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ¨¡å‹åˆ—è¡¨")
                    model_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æ¨¡å‹",
                        choices=get_trained_models(),
                        interactive=True
                    )
                    refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                
                with gr.Column(scale=2):
                    gr.Markdown("### æ¨¡å‹è¯¦æƒ…")
                    model_info = gr.Markdown(value="è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
                    
                    gr.Markdown("### Loss æ›²çº¿")
                    loss_image = gr.Image(label="Training Loss", type="filepath")
            
            # äº‹ä»¶ç»‘å®š
            model_dropdown.change(
                fn=get_model_info,
                inputs=model_dropdown,
                outputs=model_info
            )
            model_dropdown.change(
                fn=get_loss_plot,
                inputs=model_dropdown,
                outputs=loss_image
            )
            refresh_models_btn.click(
                fn=lambda: gr.Dropdown(choices=get_trained_models()),
                outputs=model_dropdown
            )
            
        with gr.Tab("âš–ï¸ æ¨¡å‹å¯¹æ¯”"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### é€‰æ‹©å¯¹æ¯”æ¨¡å‹")
                    compare_models = gr.CheckboxGroup(
                        label="æ¨¡å‹åˆ—è¡¨",
                        choices=get_trained_models()
                    )
                    compare_btn = gr.Button("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾", variant="primary")
                    refresh_compare_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                
                with gr.Column(scale=3):
                    gr.Markdown("### å¯¹æ¯”ç»“æœ")
                    comparison_plot = gr.Image(label="Loss Comparison")
            
            # äº‹ä»¶ç»‘å®š
            compare_btn.click(
                fn=get_comparison_plot,
                inputs=compare_models,
                outputs=comparison_plot
            )
            refresh_compare_btn.click(
                fn=lambda: gr.CheckboxGroup(choices=get_trained_models()),
                outputs=compare_models
            )
        
        with gr.Tab("âš™ï¸ é…ç½®è¯´æ˜"):
            gr.Markdown("""
## è®­ç»ƒé…ç½®è¯´æ˜

### åŸºç¡€å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| **å­¦ä¹ ç‡** | æ¨¡å‹æ›´æ–°æ­¥é•¿ | 2e-5 ~ 5e-5 |
| **è®­ç»ƒè½®æ•°** | å®Œæ•´éå†æ•°æ®é›†æ¬¡æ•° | 3 ~ 5 |
| **æ‰¹æ¬¡å¤§å°** | æ¯æ¬¡æ›´æ–°çš„æ ·æœ¬æ•° | 2 ~ 8 (å–å†³äºæ˜¾å­˜) |

### LoRA å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| **LoRA Rank** | ä½ç§©åˆ†è§£ç»´åº¦ | 8 ~ 64 |
| **LoRA Alpha** | ç¼©æ”¾å› å­ | é€šå¸¸ä¸º Rank çš„ 2 å€ |

### æ˜¾å­˜éœ€æ±‚

| æ¨¡å‹ | æ‰¹æ¬¡å¤§å° | æ˜¾å­˜éœ€æ±‚ |
|------|---------|---------|
| ChatTS-8B + LoRA | 2 | ~16GB |
| ChatTS-8B + LoRA | 4 | ~24GB |
| ChatTS-14B + LoRA | 2 | ~24GB |

### è®­ç»ƒè„šæœ¬

å¯ç”¨çš„è®­ç»ƒé…ç½®æ¥è‡ª `/home/douff/ts/ChatTS-Training/scripts/lora/` ç›®å½•ã€‚
""")
    
        # åˆå§‹åŒ–åŠ è½½
        demo.load(
            fn=get_dataset_names,
            outputs=preview_dropdown
        ).then(
            fn=lambda x: x[0] if x else None,
            inputs=preview_dropdown,
            outputs=preview_dropdown
        ).then(
            fn=preview_dataset,
            inputs=preview_dropdown,
            outputs=[preview_table, column_selector, preview_plot]
        )
            
    return demo


# åˆ›å»ºå…¨å±€ Gradio åº”ç”¨å®ä¾‹
training_ui = create_training_ui()
