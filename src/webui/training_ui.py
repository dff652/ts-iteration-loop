"""
Gradio ç»Ÿä¸€ç®¡ç†ç•Œé¢
åŒ…å«ï¼šæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒã€æ¨¡å‹å¯¹æ¯”
"""
import gradio as gr
from pathlib import Path
from typing import List, Dict, Optional
import json
import time
import pandas as pd
import tempfile
import os

from configs.settings import settings
from src.adapters.chatts_training import ChatTSTrainingAdapter
from src.adapters.data_processing import DataProcessingAdapter
from src.adapters.check_outlier import CheckOutlierAdapter
from src.utils.iotdb_config import load_iotdb_config
from src.utils.file_filters import is_inference_or_generated_csv, match_result_method


TRAINING_MODEL_FAMILIES = ["chatts", "qwen"]
TRAINING_METHODS = ["all", "lora", "full"]

# åˆå§‹åŒ–é€‚é…å™¨
chatts_adapter = ChatTSTrainingAdapter(model_family="chatts")
qwen_adapter = ChatTSTrainingAdapter(model_family="qwen")
training_adapter = chatts_adapter
data_adapter = DataProcessingAdapter()
inference_adapter = CheckOutlierAdapter()

# ä¸ºäº†å…¼å®¹æ€§ä¿ç•™æ—§å˜é‡å
adapter = training_adapter


def get_training_adapter(model_family: str) -> ChatTSTrainingAdapter:
    return qwen_adapter if model_family == "qwen" else chatts_adapter

# ç»“æœæ–‡ä»¶ç›®å½•ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ï¼‰
RESULTS_BASE_PATH = Path(settings.DATA_INFERENCE_DIR)

# ç»Ÿä¸€æ•°æ®æºï¼šä½¿ç”¨ data_adapter çš„å®é™…è·¯å¾„
# æ³¨æ„ï¼šè¿™é‡Œä¸å†ç¡¬ç¼–ç è·¯å¾„ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸æ•°æ®è·å–é¡µé¢ç›¸åŒçš„è·¯å¾„

# æ–‡ä»¶ååˆ°å®Œæ•´è·¯å¾„çš„æ˜ å°„ (ç”¨äºåœ¨UIæ˜¾ç¤ºæ–‡ä»¶åï¼Œå†…éƒ¨ä½¿ç”¨å®Œæ•´è·¯å¾„)
_unified_file_mapping: Dict[str, str] = {}

# UI logs can grow very large; keep a tail to avoid infinite expansion.
LOG_TAIL_MAX_CHARS = 20000

# Force fixed-height scrolling for log widgets in Gradio 6.
LOG_SCROLL_CSS = """
#training-log, #inference-log {
  height: 320px !important;
  overflow: auto !important;
}
#training-log pre, #inference-log pre {
  max-height: 320px;
  overflow: auto;
}
"""


def get_unified_file_list() -> List[str]:
    """
    è·å–ç»Ÿä¸€çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸æ•°æ®è·å–é¡µé¢ç›¸åŒæ•°æ®æºï¼‰
    è¿”å›å®Œæ•´è·¯å¾„åˆ—è¡¨
    """
    global _unified_file_mapping
    all_files = []
    _unified_file_mapping.clear()
    
    # ä½¿ç”¨ä¸æ•°æ®è·å–é¡µé¢ç›¸åŒçš„æ•°æ®è·¯å¾„
    data_path = data_adapter.data_path
    if data_path.exists():
        for f in data_path.glob("*.csv"):
            if f.exists():
                # è¿‡æ»¤æ‰æ¨ç†ç»“æœ/ä¸­é—´æ–‡ä»¶
                if is_inference_or_generated_csv(f.name):
                    continue
                full_path = str(f)
                all_files.append(full_path)
                _unified_file_mapping[f.name] = full_path
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    def safe_mtime(p):
        try:
            return Path(p).stat().st_mtime
        except OSError:
            return 0
    
    all_files.sort(key=safe_mtime, reverse=True)
    return all_files[:50]  # æœ€å¤šè¿”å› 50 ä¸ª


def get_unified_file_names() -> List[str]:
    """è·å–ç»Ÿä¸€æ–‡ä»¶åˆ—è¡¨çš„æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰"""
    # ç¡®ä¿æ˜ å°„å·²æ›´æ–°
    get_unified_file_list()
    return list(_unified_file_mapping.keys())


def resolve_filenames_to_paths(filenames: List[str]) -> List[str]:
    """å°†æ–‡ä»¶ååˆ—è¡¨è½¬æ¢ä¸ºå®Œæ•´è·¯å¾„åˆ—è¡¨"""
    global _unified_file_mapping
    if not _unified_file_mapping:
        get_unified_file_list()
    
    paths = []
    for name in filenames:
        if name in _unified_file_mapping:
            paths.append(_unified_file_mapping[name])
        elif Path(name).exists():
            # å¦‚æœå·²ç»æ˜¯å®Œæ•´è·¯å¾„
            paths.append(name)
    return paths


def format_log_html(log_content: str) -> str:
    """Format log content as scrollable HTML"""
    if not log_content:
        log_content = ""
    # Simple escaping
    safe_content = log_content.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    return f"""
    <div style="height: 300px;
                overflow-y: scroll;
                background-color: #f5f5f5;
                font-family: monospace;
                white-space: pre-wrap;
                font-size: 13px;
                line-height: 1.4;
                padding: 10px;
                border: 1px solid #ccc;
                border-radius: 4px;">
        {safe_content}
    </div>
    """



def get_existing_results(method: str = "chatts") -> List[str]:
    """è·å–å·²æœ‰çš„ç»“æœæ–‡ä»¶åˆ—è¡¨"""
    results_dir = RESULTS_BASE_PATH / method
    if not results_dir.exists():
        return []
    
    # è·å–æ‰€æœ‰ CSV æ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    csv_files = []
    for f in results_dir.glob("*.csv"):
        if f.exists(): # åªåŒ…å«å­˜åœ¨çš„æ–‡ä»¶ï¼ˆè¿‡æ»¤æ‰æ–­è£‚çš„ç¬¦å·é“¾æ¥ï¼‰
            csv_files.append(f)
            
    # å®‰å…¨æ’åºï¼šå¦‚æœ stat å¤±è´¥ï¼ˆä¾‹å¦‚ç«æ€æ¡ä»¶ï¼‰ï¼Œä½¿ç”¨ 0 ä½œä¸ºæ—¶é—´æˆ³
    def safe_get_mtime(p):
        try:
            return p.stat().st_mtime
        except OSError:
            return 0
            
    csv_files.sort(key=safe_get_mtime, reverse=True)
    return [str(f) for f in csv_files[:20]]  # æœ€å¤šè¿”å› 20 ä¸ª


def delete_selected_files(method: str, filenames: List[str]) -> tuple:
    """æ‰¹é‡åˆ é™¤é€‰ä¸­çš„ç»“æœæ–‡ä»¶"""
    if not filenames:
        return (
            gr.CheckboxGroup(choices=get_result_filenames(method)), 
            gr.File(value=None), 
            "âš ï¸ è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶"
        )
    
    results_dir = RESULTS_BASE_PATH / method
    deleted_count = 0
    errors = []
    
    for fname in filenames:
        file_path = results_dir / fname.strip()  # Strip whitespace just in case
        print(f"DEBUG: Attempting to delete {file_path}")
        
        # å¤„ç†ç¬¦å·é“¾æ¥å’Œæ™®é€šæ–‡ä»¶
        # is_file() å¯¹ç¬¦å·é“¾æ¥å¦‚æœæŒ‡å‘å­˜åœ¨æ–‡ä»¶åˆ™ä¸ºçœŸ
        # is_symlink() åˆ¤æ–­æ˜¯å¦ä¸ºç¬¦å·é“¾æ¥
        # exists() å¦‚æœæ˜¯æ–­è£‚ç¬¦å·é“¾æ¥åˆ™ä¸ºå‡
        
        try:
            # å°è¯•åˆ é™¤ï¼ˆå¦‚æœæ˜¯ç¬¦å·é“¾æ¥åˆ™åˆ é™¤é“¾æ¥ï¼Œå¦‚æœæ˜¯æ–‡ä»¶åˆ™åˆ é™¤æ–‡ä»¶ï¼‰
            if file_path.is_symlink() or file_path.exists():
                file_path.unlink()
                deleted_count += 1
                print(f"DEBUG: Deleted {file_path}")
                
                # åŒæ­¥åˆ é™¤å…³è”çš„ç¬¦å·é“¾æ¥ (åœ¨ downsampled å’Œç”¨æˆ·ç›®å½•ä¸­)
                try:
                    # 1. åˆ é™¤ downsampled ç›®å½•ä¸‹çš„åŒåé“¾æ¥
                    symlink_path = Path(settings.DATA_DOWNSAMPLED_DIR) / fname.strip()
                    if symlink_path.is_symlink():
                        symlink_path.unlink()
                        print(f"DEBUG: Deleted symlink {symlink_path}")
                        
                    # 2. åˆ é™¤ Annotator ç”¨æˆ·ç›®å½•ä¸‹çš„åŒåé“¾æ¥
                    annotator_users_file = Path(settings.DATA_PROCESSING_PATH).parent / "annotator" / "backend" / "users.json"
                    if annotator_users_file.exists():
                        import json
                        with open(annotator_users_file, 'r') as f:
                            users = json.load(f)
                        for u_info in users.values():
                            if 'data_path' in u_info:
                                u_link = Path(u_info['data_path']) / fname.strip()
                                if u_link.is_symlink():
                                    u_link.unlink()
                                    print(f"DEBUG: Deleted user symlink {u_link}")
                except Exception as e_link:
                    print(f"DEBUG: Error cleaning up symlinks: {e_link}")
            else:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦æ˜¯â€œæ–­è£‚çš„ç¬¦å·é“¾æ¥â€ï¼ˆexists()è¿”å›Falseä½†é“¾æ¥æœ¬èº«å­˜åœ¨ï¼‰
                # Path.is_symlink() å³ä½¿ç›®æ ‡ä¸å­˜åœ¨ä¹Ÿè¿”å› True
                if file_path.is_symlink():
                     file_path.unlink()
                     deleted_count += 1
                     print(f"DEBUG: Deleted broken symlink {file_path}")
                else:
                     print(f"DEBUG: File not found {file_path}")
                     # æ­¤æ—¶å¯èƒ½ç”¨æˆ·é€‰äº†ä¸€ä¸ªå·²ç»ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼ˆç¼“å­˜é—®é¢˜ï¼‰ï¼Œä¸æŠ¥é”™ï¼Œåªè®°å½•
        except Exception as e:
            errors.append(f"{fname}: {str(e)}")
            print(f"DEBUG: Error deleting {file_path}: {e}")
    
    # åˆ·æ–°åˆ—è¡¨
    time.sleep(0.5)  # ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
    new_choices = get_result_filenames(method)
    
    status_msg = f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶"
    if errors:
        status_msg += f"\nâŒ é”™è¯¯: {'; '.join(errors)}"
        
    return (
        gr.update(choices=new_choices, value=[]), 
        gr.update(value=None, label="ğŸ“¥ ä¸‹è½½åŒºåŸŸ (è¯·å…ˆé€‰æ‹©æ–‡ä»¶)"),
        status_msg
    )


def prepare_download_files(method: str, filenames: List[str]) -> tuple:
    """å‡†å¤‡ä¸‹è½½é€‰ä¸­çš„æ–‡ä»¶"""
    if not filenames:
        return None, "âš ï¸ è¯·å…ˆé€‰æ‹©è¦ä¸‹è½½çš„æ–‡ä»¶"
    
    results_dir = RESULTS_BASE_PATH / method
    paths = []
    for fname in filenames:
        p = results_dir / fname.strip()
        if p.exists():
            paths.append(str(p))
            
    if not paths:
        return gr.update(value=None), "âŒ æœªæ‰¾åˆ°é€‰ä¸­çš„æ–‡ä»¶"
        
    return (
        gr.update(value=paths, label="ğŸ“¥ ç‚¹å‡»æ­¤å¤„ä¸‹è½½ / Click to Download", visible=True),
        f"âœ… å·²å‡†å¤‡å¥½ {len(paths)} ä¸ªæ–‡ä»¶ï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹ä¸‹è½½åŒºåŸŸè¿›è¡Œä¸‹è½½"
    )


def get_result_filenames(method: str = "chatts") -> List[str]:
    """è·å–ç»“æœæ–‡ä»¶ååˆ—è¡¨ï¼ˆç”¨äºä¸‹æ‹‰æ¡†ï¼‰"""
    results_dir = RESULTS_BASE_PATH / method  # æ–°ç›®å½•ç»“æ„ï¼š/home/share/data/inference/{method}
    if not results_dir.exists():
        return []
    
    csv_files = []
    for f in results_dir.glob("*.csv"):
        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ˜¯æ–­è£‚çš„ç¬¦å·é“¾æ¥ï¼Œf.exists() ä¼šè¿”å› False
        if f.exists() or f.is_symlink():
            # ä¸¥æ ¼è¿‡æ»¤ (Addressing Issue: mixed files in directories)
            if not match_result_method(f.name, method):
                continue
            
            csv_files.append(f)
            
    def safe_get_mtime(p):
        try:
            return p.stat().st_mtime
        except OSError:
            return 0

    csv_files.sort(key=safe_get_mtime, reverse=True)
    return [f.name for f in csv_files[:20]]


def delete_result_file(method: str, filename: str) -> tuple:
    # å·²å¼ƒç”¨ï¼Œä½¿ç”¨ delete_selected_files
    pass


def get_training_configs(model_family: str = "chatts", method: str = "all") -> List[str]:
    """è·å–è®­ç»ƒé…ç½®åˆ—è¡¨"""
    configs = get_training_adapter(model_family).list_configs()
    method = (method or "all").lower()
    if method in ["lora", "full"]:
        configs = [c for c in configs if c.get("method") == method]
    return [c["name"] for c in configs]


def update_training_dropdowns(model_family: str, method: str) -> tuple:
    configs = get_training_configs(model_family, method)
    datasets = get_training_adapter(model_family).get_dataset_list()
    base_models = get_training_adapter(model_family).get_base_models()
    return (
        gr.Dropdown(choices=configs, value=(configs[0] if configs else None)),
        gr.Dropdown(choices=datasets, value=(datasets[0] if datasets else None)),
        gr.Dropdown(choices=base_models, value=(base_models[0] if base_models else None)),
    )


def update_training_config_only(model_family: str, method: str) -> gr.Dropdown:
    configs = get_training_configs(model_family, method)
    return gr.Dropdown(choices=configs, value=(configs[0] if configs else None))


def _is_checkpoint_model(model: Dict) -> bool:
    name = str(model.get("name", ""))
    path = str(model.get("path", ""))
    if name.startswith("checkpoint-"):
        return True
    return "/checkpoint-" in path.replace("\\", "/")


def _filter_models(model_family: str, model_type: str = "all", include_checkpoints: bool = False) -> List[Dict]:
    adapter = get_training_adapter(model_family)
    models = adapter.list_models()
    saves_prefix = str(adapter.saves_path).replace("\\", "/") + "/"
    models = [m for m in models if str(m.get("path", "")).replace("\\", "/").startswith(saves_prefix)]
    model_type = (model_type or "all").lower()
    if model_type in ["lora", "full"]:
        models = [m for m in models if m.get("type") == model_type]
    if not include_checkpoints:
        models = [m for m in models if not _is_checkpoint_model(m)]
    return models


def _relative_model_path(model_path: str) -> str:
    path = Path(model_path)
    parts = path.parts
    if "saves" in parts:
        idx = parts.index("saves")
        return str(Path(*parts[idx:]))
    return str(path)


def _format_model_choice(model: Dict) -> tuple[str, str]:
    label = f"{model.get('name', '')} ({model.get('type', 'unknown')}) Â· {_relative_model_path(model.get('path', ''))}"
    return (label, str(model.get("path", "")))


def get_trained_model_choices(model_family: str, model_type: str = "all", include_checkpoints: bool = False) -> List[tuple]:
    models = _filter_models(model_family, model_type, include_checkpoints)
    return [_format_model_choice(m) for m in models]


def _sorted_checkpoints(run_path: str) -> List[str]:
    if not run_path:
        return []
    root = Path(run_path)
    if not root.exists():
        return []
    checkpoints = []
    for cp in root.glob("checkpoint-*"):
        try:
            step = int(cp.name.split("-")[1])
        except Exception:
            step = 0
        checkpoints.append((step, cp.name))
    checkpoints.sort(key=lambda x: x[0])
    return [name for _, name in checkpoints]


def get_lora_run_choices(model_family: str) -> List[tuple]:
    return get_trained_model_choices(model_family, model_type="lora", include_checkpoints=False)


def get_checkpoint_choices(run_path: str) -> List[tuple]:
    names = _sorted_checkpoints(run_path)
    if not names:
        return [("æ— ", "")]
    return [("æ— ", ""), ("æœ€æ–°", "__latest__")] + [(n, n) for n in names]


def resolve_lora_adapter_path(run_path: str, checkpoint_value: str) -> str:
    if not run_path:
        return ""
    if not checkpoint_value:
        return run_path
    if checkpoint_value == "__latest__":
        names = _sorted_checkpoints(run_path)
        if not names:
            return run_path
        return str(Path(run_path) / names[-1])
    return str(Path(run_path) / checkpoint_value)


def update_lora_run_dropdown(model_family: str, current_value: Optional[str] = None) -> gr.Dropdown:
    choices = get_inference_models(model_family)
    values = {v for _, v in choices}
    value = current_value if current_value in values else (choices[0][1] if choices else None)
    return gr.Dropdown(choices=choices, value=value)


def update_checkpoint_dropdown(run_path: str, current_value: Optional[str] = None) -> gr.Dropdown:
    choices = get_checkpoint_choices(run_path)
    values = {v for _, v in choices}
    value = current_value if current_value in values else ""
    return gr.Dropdown(choices=choices, value=value)


def sync_lora_family_from_algo(algorithm: str, current_family: str) -> gr.Dropdown:
    value = algorithm if algorithm in TRAINING_MODEL_FAMILIES else current_family
    return gr.Dropdown(value=value)


def get_trained_models() -> List[str]:
    """è·å–å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨ (ä¿ç•™æ—§æ¥å£ï¼Œé»˜è®¤ chatts/all/ä¸å« checkpoint)"""
    return [label for label, _ in get_trained_model_choices("chatts", "all", False)]


def get_model_info(model_path: str, model_family: str) -> str:
    """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
    if not model_path:
        return "è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹"
    
    adapter = get_training_adapter(model_family)
    models = adapter.list_models()
    model = next((m for m in models if str(m.get("path")) == str(model_path)), None)
    
    if not model:
        return "æ¨¡å‹ä¸å­˜åœ¨"
    
    info_lines = [
        f"**æ¨¡å‹åç§°**: {model['name']}",
        f"**ç±»å‹**: {model.get('type', 'unknown')}",
        f"**æ£€æŸ¥ç‚¹**: {', '.join(model.get('checkpoints', []))}",
        f"**è®­ç»ƒæ­¥æ•°**: {model.get('global_step', 'N/A')}",
    ]
    
    # è®­ç»ƒç»“æœ
    train_results = model.get("train_results", {})
    if train_results:
        info_lines.append(f"**è®­ç»ƒ Loss**: {train_results.get('train_loss', 'N/A'):.4f}")
        info_lines.append(f"**è®­ç»ƒæ—¶é•¿**: {train_results.get('train_runtime', 'N/A'):.1f}s")
    
    return "\n\n".join(info_lines)


def get_loss_plot(model_path: str, model_family: str):
    """è·å– Loss æ›²çº¿å›¾"""
    if not model_path:
        return None
    
    models = get_training_adapter(model_family).list_models()
    model = next((m for m in models if str(m.get("path")) == str(model_path)), None)
    
    if not model or not model.get("loss_image"):
        return None
    
    loss_image = model.get("loss_image")
    if Path(loss_image).exists():
        return loss_image
    return None


def get_comparison_plot(model_paths: List[str], model_family: str):
    """è·å–å¤šä¸ªæ¨¡å‹çš„ Loss å¯¹æ¯”å›¾ (ä½¿ç”¨ Matplotlib åŠ¨æ€ç”Ÿæˆ)"""
    if not model_paths or len(model_paths) == 0:
        return None
    
    import matplotlib.pyplot as plt
    import pandas as pd
    
    plt.figure(figsize=(10, 6))
    
    models = get_training_adapter(model_family).list_models()
    for path in model_paths:
        model = next((m for m in models if str(m.get("path")) == str(path)), None)
        if not model:
            continue
            
        logs = adapter.get_training_log(model["path"])
        if not logs:
            continue
            
        df = pd.DataFrame([{"step": l.get("current_steps", 0), "loss": l.get("loss")} for l in logs if "loss" in l])
        if not df.empty:
            label = model.get("name") or _relative_model_path(model.get("path", ""))
            plt.plot(df["step"], df["loss"], label=label)
            
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.title("Model Comparison: Training Loss")
    plt.legend()
    plt.grid(True)
    
    # ä¿å­˜åˆ°é¡¹ç›®æœ¬åœ°ä¸´æ—¶ç›®å½•ï¼Œé¿å…ç³»ç»Ÿ /tmp æƒé™é—®é¢˜
    import uuid
    
    temp_dir = Path("temp_images")
    temp_dir.mkdir(exist_ok=True)
    
    output_path = temp_dir / f"compare_{uuid.uuid4().hex[:8]}.png"
    plt.savefig(str(output_path))
    plt.close()
    
    return str(output_path)


# ==================== æ•°æ®è·å–è¾…åŠ©å‡½æ•° ====================

def get_datasets_table() -> pd.DataFrame:
    """è·å–æ•°æ®é›†åˆ—è¡¨å¹¶è¿”å› DataFrame"""
    datasets = data_adapter.list_datasets()
    if not datasets:
        return pd.DataFrame(columns=["æ–‡ä»¶å", "å¤§å° (KB)", "ä¿®æ”¹æ—¶é—´"])
    
    from datetime import datetime
    rows = []
    for d in datasets:
        rows.append({
            "æ–‡ä»¶å": d["filename"],
            "å¤§å° (KB)": round(d["size_bytes"] / 1024, 2),
            "ä¿®æ”¹æ—¶é—´": datetime.fromtimestamp(d["modified_time"]).strftime("%Y-%m-%d %H:%M")
        })
    return pd.DataFrame(rows)


def get_dataset_names() -> List[str]:
    """è·å–æ•°æ®é›†æ–‡ä»¶ååˆ—è¡¨"""
    datasets = data_adapter.list_datasets()
    return [d["filename"] for d in datasets]


def delete_selected_dataset(filename: str):
    """åˆ é™¤é€‰ä¸­çš„æ•°æ®é›†"""
    print(f"[DEBUG] delete_selected_dataset called with: '{filename}'")
    if not filename:
        return get_datasets_table(), gr.Dropdown(choices=get_dataset_names(), value=None), "âŒ No dataset selected"
    
    result = data_adapter.delete_dataset(filename)
    if result.get("success"):
        # åˆ·æ–°åˆ—è¡¨
        new_table = get_datasets_table()
        new_choices = get_dataset_names()
        return new_table, gr.Dropdown(choices=new_choices, value=None), f"âœ… Deleted: {filename}"
    else:
        return get_datasets_table(), gr.Dropdown(choices=get_dataset_names()), f"âŒ {result.get('error')}"


def preview_dataset(filename: str) -> tuple:
    """é¢„è§ˆæ•°æ®é›†ï¼Œè¿”å› (è¡¨æ ¼æ•°æ®, åˆ—é€‰æ‹©å™¨æ›´æ–°, æ›²çº¿å›¾)"""
    print(f"[DEBUG] preview_dataset called with filename: '{filename}'")
    
    if isinstance(filename, list):
        filename = filename[0] if filename else None
    
    if not filename:
        print("[DEBUG] Empty filename, returning empty")
        return [], gr.CheckboxGroup(choices=[], value=[]), None
    
    try:
        # è·å–é¢„è§ˆæ•°æ®
        print(f"[DEBUG] Calling preview_csv for: {filename}")
        data = data_adapter.preview_csv(filename, limit=5000)
        print(f"[DEBUG] preview_csv returned {len(data)} records")
        
        df = pd.DataFrame(data)
        print(f"[DEBUG] DataFrame created: shape={df.shape}, columns={df.columns.tolist()}")
        
        # è¿‡æ»¤æ‰ Unnamed å’Œ category åˆ—
        df = df.loc[:, ~df.columns.str.contains('^Unnamed|^category', case=False)]
        print(f"[DEBUG] After filtering: shape={df.shape}, columns={df.columns.tolist()}")
        
        # è·å–æ•°å€¼åˆ—ä½œä¸ºå¯é€‰é¡¹
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        print(f"[DEBUG] Numeric columns: {numeric_cols}")
        
        # é»˜è®¤é€‰ä¸­ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—
        default_selected = numeric_cols[:1] if numeric_cols else []
        print(f"[DEBUG] Default selected: {default_selected}")
        
        # ç”Ÿæˆé»˜è®¤æ›²çº¿å›¾
        plot_path = None
        
        # ä¼˜åŒ–ï¼šå°è¯•ä½¿ç”¨é¢„ç”Ÿæˆçš„å›¾ç‰‡ (å¦‚æœå­˜åœ¨)
        # å‡è®¾å›¾ç‰‡åœ¨ DATA_IMAGES_DIR æˆ– DATA_DOWNSAMPLED_DIR (ç”¨æˆ·å¯èƒ½æ‰‹åŠ¨æ”¾è¿™)
        # ä¼˜å…ˆæŸ¥ DATA_IMAGES_DIR
        possible_img_name = filename.replace(".csv", ".jpg")
        img_dir = Path(settings.DATA_IMAGES_DIR)
        img_dir.mkdir(parents=True, exist_ok=True)
        pre_gen_img_path = img_dir / possible_img_name
        
        # å¦‚æœå›¾ç‰‡ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨å…¬å…±ç»„ä»¶è‡ªåŠ¨ç”Ÿæˆ
        if not pre_gen_img_path.exists():
            print(f"[DEBUG] Generating missing thumbnail: {pre_gen_img_path}")
            try:
                # ä¼˜å…ˆä½¿ç”¨ value åˆ—ï¼Œå¦åˆ™ä½¿ç”¨ç¬¬ä¸€åˆ—
                target_col = 'value' if 'value' in df.columns else (numeric_cols[0] if numeric_cols else df.columns[0])
                if target_col in df.columns:
                    generate_ts_thumbnail(df[[target_col]], str(pre_gen_img_path))
            except Exception as e:
                print(f"[ERROR] Failed to auto-generate thumbnail: {e}")

        if pre_gen_img_path.exists():
            print(f"[DEBUG] Using image: {pre_gen_img_path}")
            plot_path = str(pre_gen_img_path)
            
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæˆ–è€…ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šçš„åˆ—ç»„åˆ(è¿™é‡Œåˆå§‹åŒ–é»˜è®¤é€‰ç¬¬ä¸€åˆ—ï¼Œå‡è®¾é¢„ç”Ÿæˆå›¾ä¹Ÿæ˜¯ç”»çš„ä¸»åˆ—)
        # ä¸ºäº†ä¸¥è°¨ï¼Œå¦‚æœä½¿ç”¨äº†é¢„ç”Ÿæˆå›¾ï¼Œæˆ‘ä»¬ä¹Ÿè®¸åº”è¯¥æ˜¾ç¤ºå®ƒã€‚
        # ä½†å¦‚æœç”¨æˆ·åç»­ä¿®æ”¹äº† Checkboxï¼Œä¼šè§¦å‘ update_plot_from_selectionï¼Œé‚£æ—¶å€™ä¼šé‡ç”»ï¼Œè¿™æ˜¯å¯¹çš„ã€‚
        
        if not plot_path:
             plot_path = generate_plot(df, filename, default_selected)
             print(f"[DEBUG] Plot generated: {plot_path}")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œç¡®ä¿ Gradio 6.x å…¼å®¹
        # ä½¿ç”¨ values åˆ—è¡¨ + headers çš„æ–¹å¼
        table_data = df.values.tolist()
        headers = df.columns.tolist()
        print(f"[DEBUG] Table data rows: {len(table_data)}, headers: {headers}")
        
        return gr.Dataframe(value=table_data, headers=headers), gr.update(choices=numeric_cols, value=default_selected), plot_path
    except Exception as e:
        import traceback
        print(f"[DEBUG ERROR] Exception: {e}")
        traceback.print_exc()
        return [], gr.CheckboxGroup(choices=[], value=[]), None


def generate_plot(df: pd.DataFrame, filename: str, selected_cols: list):
    """æ ¹æ®é€‰æ‹©çš„åˆ—ç”Ÿæˆæ›²çº¿å›¾ (ç»Ÿä¸€é£æ ¼)"""
    if df.empty or not selected_cols:
        return None
    
    try:
        temp_dir = Path("temp_images")
        temp_dir.mkdir(exist_ok=True)
        import uuid
        plot_path = temp_dir / f"preview_{uuid.uuid4().hex[:8]}.jpg"
        
        # ä½¿ç”¨ç»Ÿä¸€ç»˜å›¾ç»„ä»¶ (ä»…æ”¯æŒå•åˆ—/ç¬¬ä¸€åˆ—é£æ ¼)
        data_subset = df[selected_cols]
        generate_ts_thumbnail(data_subset, str(plot_path))
        
        return str(plot_path)
    except Exception as e:
        print(f"Plot generation failed: {e}")
        return None


def update_plot_from_selection(filename: str, selected_cols: list):
    """æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„åˆ—æ›´æ–°æ›²çº¿å›¾"""
    if not filename or not selected_cols:
        return None
    
    try:
        data = data_adapter.preview_csv(filename, limit=5000)
        df = pd.DataFrame(data)
        df = df.loc[:, ~df.columns.str.contains('^Unnamed|^category', case=False)]
        return generate_plot(df, filename, selected_cols)
    except:
        return None


def start_acquire_task(
    source: str, 
    host: str,
    port: str,
    user: str,
    password: str,
    point_name: str,
    start_time: str,
    end_time: str,
    target_points: int
):
    """å¯åŠ¨æ•°æ®é‡‡é›†ä»»åŠ¡ï¼ˆæµå¼è¾“å‡ºæ—¥å¿—ï¼‰"""
    if not source:
        yield "âŒ Please enter IoTDB source path"
        return
    
    # ä½¿ç”¨æµå¼è¾“å‡ºç‰ˆæœ¬
    for log in data_adapter.run_acquire_task_streaming(
        task_id="manual",
        source=source,
        host=host,
        port=port,
        user=user,
        password=password,
        point_name=point_name,
        target_points=int(target_points),
        start_time=start_time,
        end_time=end_time
    ):
        yield log


# ==================== æ¨ç†ç›‘æ§è¾…åŠ©å‡½æ•° ====================

def get_algorithms() -> List[str]:
    """è·å–å¯ç”¨ç®—æ³•åˆ—è¡¨"""
    return ["chatts", "qwen", "adtk_hbos", "ensemble", "timer"]


def get_inference_models(model_family: str) -> List[tuple]:
    """è·å–å¯ç”¨äºæ¨ç†çš„ LoRA è®­ç»ƒä»»åŠ¡åˆ—è¡¨ (ä¸å« checkpoint)"""
    return get_lora_run_choices(model_family)

def toggle_algo_params(algorithm: str):
    """æ ¹æ®é€‰æ‹©çš„ç®—æ³•åˆ‡æ¢å‚æ•°ç»„å¯è§æ€§"""
    show_chatts = (algorithm == "chatts" or algorithm == "qwen")
    show_timer = (algorithm == "timer")
    show_adtk = (algorithm == "adtk_hbos")
    return (
        gr.update(visible=show_chatts), 
        gr.update(visible=show_timer), 
        gr.update(visible=show_adtk)
    )

def start_inference_task(
    algorithm: str, 
    base_model_path: str,
    lora_run_path: str,
    lora_checkpoint: str,
    files: List[str],
    n_downsample: int,
    threshold: float,
    downsample_mode: str,
    downsampler: str,
    ratio: float,
    min_threshold: int,
    # ChatTS args
    load_in_4bit: str,
    prompt_template: str,
    max_new_tokens: int,
    chatts_device: str,
    chatts_use_cache: str,
    # Timer args
    timer_device: str,
    timer_lookback: int,
    timer_threshold_k: float,
    timer_method: str,
    timer_streaming: bool,
    # ADTK args
    adtk_bin_nums: int,
    adtk_hbos_ratio: float
):
    """å¯åŠ¨æ¨ç†ä»»åŠ¡"""
    if not algorithm:
        yield "âŒ è¯·é€‰æ‹©ç®—æ³•", "âŒ è¯·é€‰æ‹©ç®—æ³•"
        return
    if not files:
        yield "âŒ è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶", "âŒ è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶"
        return
    
    # å°†é€‰ä¸­çš„æ–‡ä»¶åè½¬æ¢ä¸ºå®Œæ•´è·¯å¾„ï¼ˆä½¿ç”¨ç»Ÿä¸€æ•°æ®æºæ˜ å°„ï¼‰
    file_paths = resolve_filenames_to_paths(files)
    
    if not file_paths:
        yield "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶", "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶"
        return
    
    import uuid
    task_id = str(uuid.uuid4())
    
    # è§£æ LoRA Adapter è·¯å¾„ï¼ˆæ”¯æŒ checkpoint åˆ†å±‚é€‰æ‹©ï¼‰
    lora_adapter_path = resolve_lora_adapter_path(lora_run_path, lora_checkpoint)

    # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
    from datetime import datetime
    from src.db.database import SessionLocal, Task
    db = SessionLocal()
    try:
        task = Task(
            id=task_id,
            type="inference",
            status="running",
            config=json.dumps({
                "algorithm": algorithm,
                "files": files,
                "base_model_path": base_model_path,
                "lora_adapter_path": lora_adapter_path
            }),
            created_at=datetime.utcnow(),
            started_at=datetime.utcnow()
        )
        db.add(task)
        db.commit()
    except Exception as e:
        print(f"[DB Error] Failed to save task: {e}")
    finally:
        db.close()
    
    accumulated_log = f"ğŸš€ Starting batch inference for {len(file_paths)} files...\\n"
    yield (
        format_log_html(accumulated_log), 
        "ğŸ”„ æ­£åœ¨åˆå§‹åŒ–...", 
        gr.update(visible=True), # Show stop button
        gr.update(visible=False), # Hide submit button
        task_id, # Return task_id to state
        None # download_files
    )
    
    try:
        # Resolve downsample args
        resolved_downsampler = downsampler or "m4"
        resolved_n_downsample = n_downsample
        resolved_ratio = ratio
        resolved_min_threshold = min_threshold

        if str(downsample_mode).lower() in ["off", "none", "å…³é—­", "no"]:
            resolved_downsampler = "none"
        elif str(downsample_mode).lower() in ["ratio", "æ¯”ä¾‹"]:
            # ratio + min_threshold only affects adtk_hbos/stl_wavelet
            pass
        # auto/fixed keep defaults

        # å‡†å¤‡é«˜çº§å‚æ•°
        advanced_args = {
            "n_downsample": resolved_n_downsample,
            "threshold": threshold,
            "downsampler": resolved_downsampler,
            "ratio": resolved_ratio,
            "min_threshold": resolved_min_threshold,
            "base_model_path": base_model_path,
            "lora_adapter_path": lora_adapter_path, 
            # ChatTS
            "chatts_load_in_4bit": load_in_4bit,
            "chatts_prompt_template": prompt_template,
            "chatts_max_new_tokens": max_new_tokens,
            "chatts_device": chatts_device,
            "chatts_use_cache": chatts_use_cache,
            # Timer
            "timer_device": timer_device,
            "timer_lookback_length": timer_lookback,
            "timer_threshold_k": timer_threshold_k,
            "timer_method": timer_method,
            "timer_streaming": timer_streaming,
            # ADTK
            "bin_nums": adtk_bin_nums,
            "hbos_ratio": adtk_hbos_ratio
        }
        
        generated_files = []
        
        # æ‰§è¡Œæ¨ç†ï¼ˆæµå¼ï¼‰
        accumulated_log = ""
        for log_chunk in inference_adapter.run_batch_inference_streaming(
            task_id=task_id,
            model=lora_adapter_path, # å…¼å®¹æ—§æ¥å£å‘½åï¼Œå®é™…é€»è¾‘åœ¨ adapter ä¸­å·²å¤„ç†
            algorithm=algorithm,
            input_files=file_paths,
            **advanced_args
        ):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„è¿”å›
            if isinstance(log_chunk, dict) and "file_path" in log_chunk:
                # adapter è¿”å›äº†å®Œæ•´è·¯å¾„
                generated_files.append(log_chunk["file_path"])
            elif isinstance(log_chunk, dict) and "file_name" in log_chunk:
                # å…¼å®¹æ—§æ ¼å¼ï¼Œä»…æœ‰æ–‡ä»¶å
                generated_files.append(log_chunk["file_name"])
            elif isinstance(log_chunk, dict):
                 pass # å…¶ä»–ç»“æ„åŒ–æ¶ˆæ¯
            else:
                accumulated_log += log_chunk
                if len(accumulated_log) > LOG_TAIL_MAX_CHARS:
                    accumulated_log = accumulated_log[-LOG_TAIL_MAX_CHARS:]
                yield (
                    format_log_html(accumulated_log), 
                    "ğŸ”„ æ­£åœ¨æ‰§è¡Œ...",
                    gr.update(visible=True),
                    gr.update(visible=False),
                    task_id,
                    None
                )
        
        # ä»»åŠ¡ç»“æŸï¼Œå°è¯•æŸ¥æ‰¾ç”Ÿæˆçš„ç»“æœæ–‡ä»¶
        # å‡è®¾ä¿å­˜åœ¨ /home/share/results/data/<method> ä¸‹ï¼ŒæŒ‰æ—¶é—´æœ€æ–°æŸ¥æ‰¾ï¼Ÿ
        # è¿™æ¯”è¾ƒ hackyã€‚æ›´å¥½çš„æ–¹æ³•æ˜¯ adapter è¿”å›ã€‚
        # æˆ‘ä»¬åœ¨ adapter ä¸­å¢åŠ äº† yield {"file_name": ...} é€»è¾‘
        # è¿™é‡Œéœ€è¦å¤„ç†å®ƒã€‚
        
        # æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        db = SessionLocal()
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "completed"
                task.completed_at = datetime.utcnow()
                db.commit()
        except Exception as e:
            print(f"[DB Error] Failed to update task: {e}")
        finally:
            db.close()
        
        # è‡ªåŠ¨å°†ç»“æœæ–‡ä»¶é“¾æ¥åˆ°ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œä»¥ä¾¿æ ‡æ³¨å·¥å…·é»˜è®¤å¯è§
        try:
            # ç›®æ ‡ç›®å½•åˆ—è¡¨ï¼šdata_adapter ç›®å½• + Annotator ç”¨æˆ·ç›®å½•
            target_dirs = [data_adapter.data_path]
            
            # å°è¯•è¯»å– Annotator ç”¨æˆ·é…ç½®
            try:
                annotator_users_file = Path(settings.DATA_PROCESSING_PATH).parent / "annotator" / "backend" / "users.json"
                if annotator_users_file.exists():
                    # import json  <-- Removed to avoid shadowing global json
                    with open(annotator_users_file, 'r') as f:
                        users = json.load(f)
                    # éå†æ‰€æœ‰ç”¨æˆ·ï¼Œå°†ç»“æœé“¾æ¥åˆ°æ¯ä¸ªç”¨æˆ·çš„ data_path
                    for username, user_info in users.items():
                        if 'data_path' in user_info:
                            user_dir = Path(user_info['data_path'])
                            # Only add if directory exists and is writable by current user
                            if user_dir.exists() and user_dir not in target_dirs:
                                if os.access(user_dir, os.W_OK):
                                    target_dirs.append(user_dir)
                                else:
                                    print(f"[Auto-Link] Skipping {user_dir}: No write permission")
            except Exception as e:
                print(f"[Auto-Link] Warning: Could not read annotator users.json: {e}")
            
            for res_file in generated_files:
                res_path = Path(res_file)
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„æˆ–æ–‡ä»¶åï¼Œå°è¯•åœ¨ç»“æœç›®å½•æŸ¥æ‰¾
                if not res_path.exists():
                    res_path = RESULTS_BASE_PATH / algorithm / res_file
                
                if res_path.exists():
                    for target_dir in target_dirs:
                        # é¿å…è‡ªå¼•ç”¨é“¾æ¥ (å½“ç›®æ ‡ç›®å½•å°±æ˜¯ç»“æœæ–‡ä»¶æ‰€åœ¨ç›®å½•æ—¶)
                        try:
                            if target_dir.resolve() == res_path.parent.resolve():
                                continue
                        except Exception:
                            pass

                        target_link = target_dir / res_path.name
                        try:
                            # å¦‚æœé“¾æ¥ä¸å­˜åœ¨æˆ–å·²æ–­è£‚ï¼Œé‡æ–°åˆ›å»º
                            if target_link.is_symlink() or target_link.exists():
                                target_link.unlink()
                            target_link.symlink_to(res_path)
                            print(f"[Auto-Link] Created symlink for {res_path.name} in {target_dir}")
                        except Exception as link_err:
                            print(f"[Auto-Link] Failed to link to {target_dir}: {link_err}")
        except Exception as e:
            print(f"[Auto-Link Error] Failed to link results: {e}")
        
        yield (
            format_log_html(accumulated_log + "\nâœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ"), 
            "âœ… ä»»åŠ¡å®Œæˆ",
             gr.update(visible=False),
             gr.update(visible=True),
             task_id,
             generated_files # TODO: å¡«å…… output files if capture logic works perfectly
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        
        # æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        db = SessionLocal()
        try:
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                task.status = "failed"
                task.error = str(e)
                task.completed_at = datetime.utcnow()
                db.commit()
        except Exception as db_e:
            print(f"[DB Error] Failed to update task: {db_e}")
        finally:
            db.close()
        
        yield (
            format_log_html(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"), 
            f"âŒ é”™è¯¯: {str(e)}",
            gr.update(visible=False),
            gr.update(visible=True),
            None,
            None
        )

def stop_task_action(task_id_state):
    """å®é™…æ‰§è¡Œåœæ­¢åŠ¨ä½œ"""
    print(f"DEBUG: Stop requested for task ID: {task_id_state}")
    if task_id_state:
        if inference_adapter.stop_inference_task(task_id_state):
            print(f"DEBUG: Stop successful for {task_id_state}")
            return "ğŸ›‘ ä»»åŠ¡å·²è¯·æ±‚åœæ­¢", gr.update(visible=False), gr.update(visible=True), None, None
        else:
            print(f"DEBUG: Stop failed for {task_id_state} (not found or error)")
            return f"âŒ åœæ­¢å¤±è´¥: ä»»åŠ¡ {task_id_state} ä¸å­˜åœ¨æˆ–å·²ç»“æŸ", gr.update(visible=True), gr.update(visible=False), task_id_state, None
    print("DEBUG: No active task ID found")
    return "âš ï¸ æ— æ´»åŠ¨ä»»åŠ¡", gr.update(visible=False), gr.update(visible=True), None, None


def get_task_status_table() -> pd.DataFrame:
    """è·å–ä»»åŠ¡çŠ¶æ€åˆ—è¡¨ (ä»æ•°æ®åº“è¯»å–)"""
    try:
        from src.db.database import SessionLocal, Task
        db = SessionLocal()
        tasks = db.query(Task).order_by(Task.created_at.desc()).limit(20).all()
        db.close()
        
        if not tasks:
            return pd.DataFrame(columns=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"])
        
        rows = []
        for t in tasks:
            rows.append({
                "ID": t.id[:8] + "...",
                "ç±»å‹": t.type,
                "çŠ¶æ€": t.status,
                "åˆ›å»ºæ—¶é—´": t.created_at.strftime("%H:%M:%S") if t.created_at else "N/A"
            })
        return pd.DataFrame(rows)
    except Exception as e:
        return pd.DataFrame({"é”™è¯¯": [str(e)]})


def clear_task_history() -> tuple:
    """æ¸…ç©ºä»»åŠ¡å†å²è®°å½•"""
    try:
        from src.db.database import SessionLocal, Task
        db = SessionLocal()
        deleted = db.query(Task).delete()
        db.commit()
        db.close()
        return pd.DataFrame(columns=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"]), f"âœ… å·²æ¸…ç©º {deleted} æ¡å†å²è®°å½•"
    except Exception as e:
        return get_task_status_table(), f"âŒ æ¸…ç©ºå¤±è´¥: {str(e)}"


def start_training(
    config_name: str,
    learning_rate: str,
    num_epochs: float,
    batch_size: int,
    lora_rank: int,
    lora_alpha: int,
    output_name: str,
    model_path: Optional[str] = None,
    dataset_name: Optional[str] = None
) -> str:
    """å¯åŠ¨è®­ç»ƒ (Quick Start)"""
    if not config_name:
        return "âŒ è¯·é€‰æ‹©è®­ç»ƒé…ç½® (æ¨¡æ¿)"
    
    if not output_name:
        return "âŒ è¯·è¾“å…¥è¾“å‡ºç›®å½•åç§°"
    
    # ç”Ÿæˆä»»åŠ¡ID
    import uuid
    task_id = f"job-{str(uuid.uuid4())[:8]}"
    
    try:
        # è°ƒç”¨åç«¯é€‚é…å™¨å¯åŠ¨è®­ç»ƒ
        result = training_adapter.run_training(
            task_id=task_id,
            config_name=config_name,
            version_tag=output_name,
            # Quick Start Overrides
            override_model_path=model_path,
            override_dataset=dataset_name,
            override_learning_rate=learning_rate,
            override_epochs=num_epochs,
            override_batch_size=batch_size,
            override_lora_rank=lora_rank,
            override_lora_alpha=lora_alpha
        )
        
        if result.get("success"):
            return f"""âœ… è®­ç»ƒä»»åŠ¡å·²æˆåŠŸå¯åŠ¨!
            
**ä»»åŠ¡ ID**: {task_id}
**è¾“å‡ºç›®å½•**: `{result.get('output_dir')}`
**åŸºç¡€æ¨¡å‹**: `{model_path or 'Default (from script)'}`
**æ•°æ®é›†**: `{dataset_name or 'Default (from script)'}`

æ­£åœ¨åå°è¿è¡Œä¸­... è¯·ç•™æ„æ—¥å¿—è¾“å‡ºæˆ–ç¨ååˆ·æ–°æ¨¡å‹åˆ—è¡¨ã€‚
"""
        else:
            return f"""âŒ å¯åŠ¨å¤±è´¥
            
**é”™è¯¯ä¿¡æ¯**: {result.get('error')}
"""
            
    except Exception as e:
        return f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}"


def create_training_ui() -> gr.Blocks:
    """åˆ›å»ºç»Ÿä¸€ç®¡ç†ç•Œé¢ï¼ˆæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒï¼‰"""
    
    with gr.Blocks(title="TS-Iteration-Loop", theme=gr.themes.Soft(), css=LOG_SCROLL_CSS) as demo:
        gr.Markdown("# ğŸ”„ TS-Iteration-Loop æ—¶åºè¿­ä»£å¹³å°")
        gr.Markdown("æ•´åˆæ•°æ®è·å–ã€æ¨ç†ç›‘æ§ã€å¾®è°ƒè®­ç»ƒçš„ç»Ÿä¸€ç®¡ç†ç•Œé¢")
        
        # ==================== æ•°æ®è·å– Tab ====================
        with gr.Tab("ğŸ“ æ•°æ®è·å–"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ•°æ®é›†åˆ—è¡¨")
                    datasets_table = gr.Dataframe(
                        value=get_datasets_table(),
                        label="å·²æœ‰æ•°æ®é›†",
                        interactive=False
                    )
                    refresh_datasets_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                    
                    gr.Markdown("### é¢„è§ˆæ•°æ®")
                    preview_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æ•°æ®é›†",
                        choices=get_dataset_names(),
                        interactive=True
                    )
                    with gr.Row():
                        delete_dataset_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop", size="sm")
                        delete_status = gr.Textbox(label="", visible=False)
                    
                    column_selector = gr.CheckboxGroup(
                        label="Select columns to plot",
                        choices=[],
                        interactive=True
                    )
                
                with gr.Column(scale=2):
                    gr.Markdown("### æ•°æ®é‡‡é›†é…ç½®")
                    
                    with gr.Accordion("IoTDB è¿æ¥é…ç½®", open=False):
                        # ä»å…±äº«é…ç½®åŠ è½½é»˜è®¤å€¼
                        _iotdb_cfg = load_iotdb_config()
                        with gr.Row():
                            host_input = gr.Textbox(label="Host", value=_iotdb_cfg.get("host", "192.168.199.185"))
                            port_input = gr.Textbox(label="Port", value=_iotdb_cfg.get("port", "6667"))
                        with gr.Row():
                            user_input = gr.Textbox(label="User", value=_iotdb_cfg.get("user", "root"))
                            pwd_input = gr.Textbox(label="Password", value=_iotdb_cfg.get("password", "root"), type="password")

                    gr.Markdown("### æŸ¥è¯¢å‚æ•°")
                    source_input = gr.Textbox(
                        label="IoTDB æºè·¯å¾„ (Path)",
                        placeholder="root.zhlh_202307_202412.ZHLH_4C_1216",
                        value="root.zhlh_202307_202412.ZHLH_4C_1216",
                        scale=2
                    )
                    
                    with gr.Row():
                         point_input = gr.Textbox(
                            label="ç‚¹ä½åç§° (Point Name)",
                            placeholder="FI_10401C.PV (ç•™ç©ºæŸ¥è¯¢æ‰€æœ‰*)",
                            value="FI_10401C.PV"
                        )
                    
                    with gr.Row():
                        start_time_input = gr.Textbox(label="å¼€å§‹æ—¶é—´", value="2023-07-18 12:00:00")
                        end_time_input = gr.Textbox(label="ç»“æŸæ—¶é—´", value="2024-11-05 23:59:59")

                    target_points = gr.Slider(
                        label="ç›®æ ‡ç‚¹æ•°",
                        minimum=1000,
                        maximum=10000,
                        value=5000,
                        step=500,
                        scale=1
                    )
                    
                    acquire_btn = gr.Button("ğŸ“¥ å¼€å§‹é‡‡é›†", variant="primary")
                    acquire_output = gr.Markdown(value="ç­‰å¾…é‡‡é›†...")
            
            # æ•°æ®é¢„è§ˆåŒºåŸŸ - å›¾è¡¨ä¼˜å…ˆï¼Œè¡¨æ ¼å¯æŠ˜å 
            with gr.Row():
                preview_plot = gr.Image(label="Curve Preview", height=350)
            
            with gr.Accordion("ğŸ“‹ Data Table (first 5000 rows)", open=False):
                preview_table = gr.Dataframe(
                    label="",
                    interactive=False
                )
            
            # äº‹ä»¶ç»‘å®š - æ•°æ®è·å–
            refresh_datasets_btn.click(
                fn=get_datasets_table,
                outputs=datasets_table
            )
            refresh_datasets_btn.click(
                fn=lambda: gr.Dropdown(choices=get_dataset_names()),
                outputs=preview_dropdown
            )
            delete_dataset_btn.click(
                fn=delete_selected_dataset,
                inputs=preview_dropdown,
                outputs=[datasets_table, preview_dropdown, delete_status]
            )
            preview_dropdown.change(
                fn=preview_dataset,
                inputs=preview_dropdown,
                outputs=[preview_table, column_selector, preview_plot]
            )
            column_selector.change(
                fn=update_plot_from_selection,
                inputs=[preview_dropdown, column_selector],
                outputs=preview_plot
            )
            acquire_btn.click(
                fn=start_acquire_task,
                inputs=[
                    source_input, host_input, port_input, user_input, pwd_input,
                    point_input, start_time_input, end_time_input, target_points
                ],
                outputs=acquire_output
            )
        
        # ==================== æ¨ç†ç›‘æ§ Tab ====================
        with gr.Tab("ğŸ” æ¨ç†ç›‘æ§") as inference_tab:
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ–°å»ºæ¨ç†ä»»åŠ¡")
                    algo_dropdown = gr.Dropdown(
                        label="é€‰æ‹©ç®—æ³•",
                        choices=get_algorithms(),
                        value="chatts",
                        interactive=True
                    )
                    
                    # æ¨¡å‹é…ç½®ç»„
                    with gr.Group():
                        base_model_input = gr.Textbox(
                            label="Base Model Path (Base Model)", 
                            value="/home/share/llm_models/bytedance-research/ChatTS-8B",
                            info="åŸºç¡€æ¨¡å‹è·¯å¾„"
                        )
                        lora_family_select = gr.Dropdown(
                            label="LoRA æ¨¡å‹ç±»å‹ (Model Family)",
                            choices=TRAINING_MODEL_FAMILIES,
                            value="chatts",
                            interactive=True
                        )
                        lora_run_select = gr.Dropdown(
                            label="LoRA é€‚é…å™¨ (è®­ç»ƒä»»åŠ¡)",
                            choices=get_inference_models("chatts"),
                            interactive=True,
                            info="ä»…æ˜¾ç¤ºè®­ç»ƒä»»åŠ¡ç›®å½•ï¼Œä¸å« checkpoint"
                        )
                        lora_checkpoint_select = gr.Dropdown(
                            label="Checkpoint (å¯é€‰)",
                            choices=get_checkpoint_choices(""),
                            value="",
                            interactive=True
                        )
                        
                    files_select = gr.CheckboxGroup(
                        label="é€‰æ‹©è¾“å…¥æ–‡ä»¶",
                        choices=get_unified_file_names()
                    )
                    
                    with gr.Accordion("âš™ï¸ é«˜çº§é…ç½® (å¯é€‰)", open=False):
                        gr.Markdown("#### é™é‡‡æ ·é…ç½®")
                        with gr.Row():
                            downsample_mode_input = gr.Dropdown(
                                label="é™é‡‡æ ·æ¨¡å¼",
                                choices=["auto", "off", "fixed", "ratio"],
                                value="auto",
                                info="auto: é•¿åº¦>n_downsample æ‰é™é‡‡æ ·; off: å…³é—­"
                            )
                            downsampler_input = gr.Dropdown(
                                label="é™é‡‡æ ·ç®—æ³•",
                                choices=["m4", "minmax", "none"],
                                value="m4"
                            )
                        with gr.Row():
                            n_downsample_input = gr.Slider(
                                label="é™é‡‡æ ·ç‚¹æ•° (n_downsample)", 
                                minimum=100, maximum=10000, step=100, value=settings.DEFAULT_DOWNSAMPLE_POINTS
                            )
                            ratio_input = gr.Slider(
                                label="é™é‡‡æ ·æ¯”ä¾‹ (ratio)", 
                                minimum=0.01, maximum=1.0, step=0.01, value=0.1
                            )
                        min_threshold_input = gr.Number(
                            label="æœ€å°ç‚¹æ•°é˜ˆå€¼ (min_threshold)", value=200000, precision=0
                        )
                        threshold_input = gr.Number(
                            label="å¼‚å¸¸é˜ˆå€¼ (threshold)", value=8.0
                        )
                        
                        # ChatTS ä¸“å±å‚æ•°
                        with gr.Group(visible=True) as chatts_group:
                            gr.Markdown("#### ChatTS é…ç½®")
                            with gr.Row():
                                load_in_4bit_input = gr.Dropdown(
                                    label="4-bit é‡åŒ–", choices=["auto", "true", "false"], value="auto",
                                    info="æ˜¾å­˜ä¸è¶³æ—¶å»ºè®®å¼€å¯(true)"
                                )
                                prompt_template_input = gr.Dropdown(
                                    label="Prompt æ¨¡æ¿",
                                    choices=["default", "detailed", "minimal", "industrial", "english"],
                                    value="default"
                                )
                            with gr.Row():
                                chatts_device_input = gr.Textbox(label="Device", value="cuda:1")
                                chatts_use_cache_input = gr.Dropdown(
                                    label="Use Cache (KV)", choices=["auto", "true", "false"], value="auto"
                                )
                            max_new_tokens_input = gr.Number(
                                label="æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max New Tokens)", value=4096, precision=0
                            )

                        # Timer ä¸“å±å‚æ•°
                        with gr.Group(visible=False) as timer_group:
                            gr.Markdown("#### Timer é…ç½®")
                            with gr.Row():
                                timer_device_input = gr.Textbox(label="Device", value="cuda:0")
                                timer_lookback_input = gr.Number(label="Lookback Length", value=256, precision=0)
                            with gr.Row():
                                timer_threshold_k_input = gr.Number(label="Threshold K", value=3.5)
                                timer_method_input = gr.Dropdown(label="Method", choices=["mad", "sigma"], value="mad")
                            timer_streaming_input = gr.Checkbox(label="Enable Streaming Mode", value=False)
                            
                        # ADTK ä¸“å±å‚æ•°
                        with gr.Group(visible=False) as adtk_group:
                            gr.Markdown("#### ADTK HBOS é…ç½®")
                            with gr.Row():
                                adtk_bin_nums_input = gr.Number(label="Bin Nums (åˆ†ç®±æ•°)", value=20, precision=0)
                                adtk_hbos_ratio_input = gr.Number(label="HBOS Ratio (è·³å˜é˜ˆå€¼)", value=None)

                    with gr.Row():
                        submit_inference_btn = gr.Button("ğŸš€ æäº¤ä»»åŠ¡", variant="primary")
                        stop_inference_btn = gr.Button("ğŸ›‘ åœæ­¢ä»»åŠ¡", variant="stop", visible=False)
                    
                    # éšè—çš„çŠ¶æ€ç»„ä»¶ï¼Œç”¨äºå­˜å‚¨ current task id
                    current_task_id_state = gr.State("")
                
                with gr.Column(scale=2):
                    gr.Markdown("### ä»»åŠ¡çŠ¶æ€ & æ—¥å¿—")
                    with gr.Tabs():
                        with gr.Tab("å®æ—¶æ—¥å¿—"):
                            inference_logs = gr.HTML(
                                value=format_log_html("Waiting for task..."),
                                label="Execution Logs",
                                elem_id="inference-log"
                            )
                        with gr.Tab("ä»»åŠ¡ç»“æœ"):
                             # å½“å‰ä»»åŠ¡çŠ¶æ€
                             inference_result_md = gr.Markdown(value="ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                             download_files = gr.File(label="å½“å‰ä»»åŠ¡ç»“æœ", file_count="multiple", interactive=False, visible=False)
                             
                             # æ•´åˆçš„ç»“æœæ–‡ä»¶ç®¡ç†åŒº
                             gr.Markdown("### ğŸ“‚ ç»“æœæ–‡ä»¶ç®¡ç†")
                             with gr.Row():
                                 results_method_select = gr.Dropdown(
                                     label="ç­›é€‰æ–¹æ³•",
                                     choices=["chatts", "qwen", "timer", "adtk_hbos"],
                                     value="chatts",
                                     scale=1
                                 )
                                 refresh_results_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨", size="sm", scale=0)
                             
                             # ç»Ÿä¸€æ–‡ä»¶åˆ—è¡¨ï¼ˆå¤šé€‰ï¼‰
                             file_manager_list = gr.CheckboxGroup(
                                 label="æ–‡ä»¶åˆ—è¡¨ (æ–‡ä»¶å | è¾ƒæ–°çš„åœ¨å‰)",
                                 choices=get_result_filenames("chatts"),
                                 value=[],
                                 interactive=True
                             )
                             
                             with gr.Row():
                                 download_selected_btn = gr.Button("â¬‡ï¸ ä¸‹è½½é€‰ä¸­", size="sm")
                                 delete_selected_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop", size="sm")

                             operation_status = gr.Markdown(value="")

                             # ä¸‹è½½åŒºåŸŸ (åŠ¨æ€æ˜¾ç¤º)
                             history_download_files = gr.File(
                                 label="ğŸ“¥ ä¸‹è½½åŒºåŸŸ (è¯·å…ˆé€‰æ‹©æ–‡ä»¶å¹¶ç‚¹å‡»â€œä¸‹è½½é€‰ä¸­â€)",
                                 file_count="multiple",
                                 interactive=False,
                                 visible=True
                             )
                    
                    # ä»»åŠ¡å†å²è®°å½• - æ”¾å…¥å¯æŠ˜å åŒºåŸŸ
                    with gr.Accordion("ğŸ“‹ ä»»åŠ¡å†å²è®°å½•", open=False):
                        with gr.Row():
                            refresh_tasks_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", size="sm")
                            clear_tasks_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå†å²", size="sm", variant="stop")
                        clear_status = gr.Markdown(value="", visible=True)
                        task_table = gr.Dataframe(
                            headers=["ID", "ç±»å‹", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´"],
                            value=[],
                            interactive=False
                        )
            
            # äº‹ä»¶ç»‘å®š - æ¨ç†ç›‘æ§
            
            # æäº¤ä»»åŠ¡
            submit_event = submit_inference_btn.click(
                fn=start_inference_task,
                inputs=[
                    algo_dropdown, base_model_input, lora_run_select, lora_checkpoint_select, files_select,
                    # é€šç”¨å‚æ•°
                    n_downsample_input, threshold_input,
                    downsample_mode_input, downsampler_input, ratio_input, min_threshold_input,
                    # ChatTS å‚æ•°
                    load_in_4bit_input, prompt_template_input, max_new_tokens_input, chatts_device_input, chatts_use_cache_input,
                    # Timer å‚æ•°
                    timer_device_input, timer_lookback_input, timer_threshold_k_input, timer_method_input, timer_streaming_input,
                    # ADTK å‚æ•°
                    adtk_bin_nums_input, adtk_hbos_ratio_input
                ],
                outputs=[
                    inference_logs, 
                    inference_result_md, 
                    stop_inference_btn, 
                    submit_inference_btn, 
                    current_task_id_state, 
                    download_files
                ]
            )
            
            # åœæ­¢ä»»åŠ¡
            stop_inference_btn.click(
                fn=stop_task_action,
                inputs=[current_task_id_state],
                outputs=[
                    inference_result_md, 
                    stop_inference_btn, 
                    submit_inference_btn, 
                    current_task_id_state, 
                    download_files
                ]
            )
            
            refresh_tasks_btn.click(
                fn=get_task_status_table,
                outputs=task_table
            )
            refresh_tasks_btn.click(
                fn=lambda: gr.CheckboxGroup(choices=get_unified_file_names()),
                outputs=files_select
            )
            refresh_tasks_btn.click(
                fn=update_lora_run_dropdown,
                inputs=[lora_family_select, lora_run_select],
                outputs=lora_run_select
            )
            refresh_tasks_btn.click(
                fn=update_checkpoint_dropdown,
                inputs=[lora_run_select, lora_checkpoint_select],
                outputs=lora_checkpoint_select
            )

            lora_family_select.change(
                fn=update_lora_run_dropdown,
                inputs=[lora_family_select, lora_run_select],
                outputs=lora_run_select
            ).then(
                fn=update_checkpoint_dropdown,
                inputs=[lora_run_select, lora_checkpoint_select],
                outputs=lora_checkpoint_select
            )

            lora_run_select.change(
                fn=update_checkpoint_dropdown,
                inputs=[lora_run_select, lora_checkpoint_select],
                outputs=lora_checkpoint_select
            )
            
            # Tab åˆ‡æ¢æ—¶è‡ªåŠ¨åˆ·æ–°æ–‡ä»¶åˆ—è¡¨
            inference_tab.select(
                fn=lambda: gr.CheckboxGroup(choices=get_unified_file_names()),
                outputs=files_select
            )
            inference_tab.select(
                fn=update_lora_run_dropdown,
                inputs=[lora_family_select, lora_run_select],
                outputs=lora_run_select
            )
            inference_tab.select(
                fn=update_checkpoint_dropdown,
                inputs=[lora_run_select, lora_checkpoint_select],
                outputs=lora_checkpoint_select
            )
            # æ¸…ç©ºå†å²è®°å½•
            clear_tasks_btn.click(
                fn=clear_task_history,
                outputs=[task_table, clear_status]
            )
            
            # å†å²ç»“æœæ–‡ä»¶åˆ·æ–°
            refresh_results_btn.click(
                fn=lambda m: gr.CheckboxGroup(choices=get_result_filenames(m), value=[]),
                inputs=results_method_select,
                outputs=file_manager_list
            )
            
            # åˆ‡æ¢æ–¹æ³•æ—¶åˆ·æ–°ç»“æœåˆ—è¡¨
            results_method_select.change(
                fn=lambda m: gr.CheckboxGroup(choices=get_result_filenames(m), value=[]),
                inputs=results_method_select,
                outputs=file_manager_list
            )
            
            # åˆ é™¤é€‰ä¸­æ–‡ä»¶
            delete_selected_btn.click(
                fn=delete_selected_files,
                inputs=[results_method_select, file_manager_list],
                outputs=[file_manager_list, history_download_files, operation_status]
            )
            
            # ä¸‹è½½é€‰ä¸­æ–‡ä»¶
            # ä¸‹è½½é€‰ä¸­æ–‡ä»¶
            download_selected_btn.click(
                fn=prepare_download_files,
                inputs=[results_method_select, file_manager_list],
                outputs=[history_download_files, operation_status]
            )
            
            # ç®—æ³•åˆ‡æ¢äº‹ä»¶ï¼šæ§åˆ¶å‚æ•°ç»„æ˜¾ç¤º
            algo_dropdown.change(
                fn=toggle_algo_params,
                inputs=algo_dropdown,
                outputs=[chatts_group, timer_group, adtk_group]
            )

            algo_dropdown.change(
                fn=sync_lora_family_from_algo,
                inputs=[algo_dropdown, lora_family_select],
                outputs=lora_family_select
            ).then(
                fn=update_lora_run_dropdown,
                inputs=[lora_family_select, lora_run_select],
                outputs=lora_run_select
            ).then(
                fn=update_checkpoint_dropdown,
                inputs=[lora_run_select, lora_checkpoint_select],
                outputs=lora_checkpoint_select
            )
            
            # è‡ªåŠ¨åŒæ­¥ç­›é€‰æ–¹æ³• (User requested unification)
            algo_dropdown.change(
                fn=lambda x: x if x in ["chatts", "qwen", "timer", "adtk_hbos"] else "chatts",
                inputs=algo_dropdown,
                outputs=results_method_select
            )
        
        # ==================== æ ‡æ³¨å·¥å…· Tab ====================
        with gr.Tab("ğŸ·ï¸ æ ‡æ³¨å·¥å…·"):
            with gr.Row():
                with gr.Column(scale=3):
                    gr.Markdown("### ğŸ”— å¿«é€Ÿè®¿é—®")
                    # ä½¿ç”¨ HTML æŒ‰é’®æ‰“å¼€é“¾æ¥ï¼Œæ›´ç›´è§‚
                    gr.HTML("""
                    <div style="padding: 10px; background-color: #f0f9ff; border-radius: 8px; border: 1px solid #bae6fd;">
                        <p style="margin-bottom: 10px; font-weight: bold; color: #0369a1;">
                            æ ‡æ³¨å·¥å…·è¿è¡Œåœ¨ç‹¬ç«‹æœåŠ¡ç«¯å£ (5000)
                        </p>
                        <a href="http://192.168.199.126:5000" target="_blank" style="
                            display: inline-block;
                            padding: 10px 20px;
                            background-color: #0284c7;
                            color: white;
                            text-decoration: none;
                            border-radius: 6px;
                            font-weight: bold;
                        ">
                            ğŸš€ æ‰“å¼€æ ‡æ³¨å·¥å…· (Open Annotator)
                        </a>
                    </div>
                    """)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“Š çŠ¶æ€æ¦‚è§ˆ")
                    # åŠ¨æ€è·å–æ ‡æ³¨æ–‡ä»¶æ•°
                    def get_annotation_stats():
                        # ä½¿ç”¨ Settings
                        ann_dir = Path(settings.ANNOTATIONS_ROOT) / "douff"
                        if not ann_dir.exists():
                            return "æš‚æ— æ ‡æ³¨ç›®å½•"
                        count = len(list(ann_dir.glob("*.json")))
                        return f"å·²æ ‡æ³¨æ–‡ä»¶æ•°: {count}"

                    annotation_stats = gr.Textbox(
                        value=get_annotation_stats(),
                        label="å½“å‰æ ‡æ³¨è¿›åº¦",
                        interactive=False
                    )
                    refresh_ann_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", size="sm")
                    refresh_ann_btn.click(fn=get_annotation_stats, outputs=annotation_stats)

            gr.Markdown("---")
            gr.Markdown("### ğŸ”„ æ•°æ®è½¬æ¢ (Annotator -> ChatTS)")
            
            with gr.Row():
                with gr.Column(scale=1):
                    # é…ç½®åŒºåŸŸ
                    conv_model_family = gr.Radio(
                        choices=["chatts", "qwen"],
                        value="chatts",
                        label="ç›®æ ‡æ¨¡å‹æ ¼å¼ (Target Format)"
                    )
                    
                    with gr.Accordion("âš™ï¸ è·¯å¾„ä¸å‚æ•°é…ç½® (Settings)", open=False):
                        conf_input_dir = gr.Textbox(
                            label="æ ‡æ³¨æ–‡ä»¶æ¥æº (Annotation Dir)", 
                            value=str(Path(settings.ANNOTATIONS_ROOT) / settings.DEFAULT_USER)
                        )
                        conf_image_dir = gr.Textbox(
                            label="å›¾ç‰‡æ–‡ä»¶æ¥æº (Image Dir)", 
                            value=settings.DATA_DOWNSAMPLED_DIR
                        )
                        conf_output_path = gr.Textbox(
                            label="è½¬æ¢è¾“å‡ºè·¯å¾„ (Output Path)",
                            value=str(Path(settings.DATA_TRAINING_CHATTS_DIR) / "converted_data.json")
                        )

                    # è·å–æ ‡æ³¨æ–‡ä»¶åˆ—è¡¨
                    def get_file_choices(ann_dir, filter_keyword=None):
                        path_obj = Path(ann_dir)
                        if not path_obj.exists():
                            return []
                        try:
                            files = list(path_obj.glob("*.json"))
                            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                            
                            # è¿‡æ»¤é€»è¾‘
                            if filter_keyword == "qwen":
                                # Qwen æ¨¡å¼ï¼šåªæ˜¾ç¤ºåŒ…å« qwen çš„æ–‡ä»¶
                                files = [f for f in files if "qwen" in f.name.lower()]
                            elif filter_keyword == "chatts":
                                # ChatTS æ¨¡å¼ï¼šæ’é™¤ qwen æ–‡ä»¶ (æ˜¾ç¤º chatts å’Œ legacy)
                                files = [f for f in files if "qwen" not in f.name.lower()]
                                
                            return [f.name for f in files]
                        except Exception:
                            return []

                    # åˆå§‹åŠ è½½
                    default_ann_dir = str(Path(settings.ANNOTATIONS_ROOT) / settings.DEFAULT_USER)
                    # é»˜è®¤ filter="chatts" å¯¹åº” conv_model_family default value
                    initial_choices = get_file_choices(default_ann_dir, "chatts")
                    initial_val = initial_choices[0] if initial_choices else None

                    ann_file_dropdown = gr.Dropdown(
                        label="é€‰æ‹©è¦é¢„è§ˆ/è½¬æ¢çš„æ–‡ä»¶",
                        choices=initial_choices,
                        value=initial_val,
                        multiselect=False,
                        interactive=True,
                        allow_custom_value=False
                    )
                    
                    def refresh_files(ann_dir, family):
                        choices = get_file_choices(ann_dir, family)
                        val = choices[0] if choices else None
                        return gr.update(choices=choices, value=val)
                        
                    refresh_files_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨ (Refresh)", size="sm")
                    
                    with gr.Row():
                        convert_curr_btn = gr.Button("ğŸš€ ä»…è½¬æ¢é€‰ä¸­", variant="primary")
                        convert_all_btn = gr.Button("ğŸ“¦ æ‰¹é‡è½¬æ¢æ‰€æœ‰", variant="secondary")
                    
                with gr.Column(scale=2):
                    convert_status = gr.Textbox(label="æ“ä½œæ—¥å¿— (Execution Log)", lines=10, interactive=False)
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("#### ğŸ“ è½¬æ¢å‰ (Annotator JSON)")
                    before_json = gr.JSON(label="Source Data", height=400)
                with gr.Column():
                    after_json_label = gr.Markdown("#### ğŸ¯ è½¬æ¢å (ChatTS Training Data)")
                    after_json = gr.JSON(label="Converted Data", height=400)
                
            def preview_source_file(selected_file, input_dir_val, image_dir_val, model_family="qwen"):
                """é€‰æ‹©æ–‡ä»¶æ—¶ç«‹å³é¢„è§ˆï¼Œå¹¶æ‰§è¡ŒçœŸå®è½¬æ¢ï¼ˆä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼‰"""
                if not selected_file or not input_dir_val:
                    return None, None
                 
                # 1. è¯»å–æºæ–‡ä»¶
                src_p = Path(input_dir_val) / selected_file
                source_content = None
                
                try:
                    if src_p.exists():
                        with open(src_p, 'r', encoding='utf-8') as f:
                            source_content = json.load(f)
                except Exception as e:
                    source_content = {"error": str(e)}
                
                # 2. æ‰§è¡Œè½¬æ¢é¢„è§ˆ (çœŸå®è°ƒç”¨é€‚é…å™¨)
                converted_content = None
                try:
                    # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä½œä¸ºè¾“å‡º
                    with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as tmp:
                        tmp_path = tmp.name
                    
                    # é»˜è®¤ image_dir å¦‚æœä¸ºç©º
                    img_d = image_dir_val if image_dir_val else settings.DATA_DOWNSAMPLED_DIR
                    
                    res = data_adapter.convert_annotations(
                        input_dir=input_dir_val,
                        output_path=tmp_path,
                        image_dir=img_d,
                        filename=selected_file,
                        model_family=model_family,
                        csv_src_dir=str(RESULTS_BASE_PATH / "qwen") if model_family == "qwen" else settings.DATA_DOWNSAMPLED_DIR
                    )
                    
                    if res["success"]:
                        try:
                            with open(tmp_path, 'r', encoding='utf-8') as f:
                                converted_content = json.load(f)
                        except Exception as read_err:
                            converted_content = {"error": f"Read converted file failed: {read_err}"}
                    else:
                        converted_content = {
                            "error": "Conversion failed", 
                            "stderr": res.get("stderr", ""),
                            "stdout": res.get("stdout", "")
                        }
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        Path(tmp_path).unlink(missing_ok=True)
                    except:
                        pass

                except Exception as e:
                    converted_content = {"error": f"Preview failed: {str(e)}"}
                
                return source_content, converted_content

            def convert_core(selected_file, input_dir, image_dir, output_path, model_family, mode="single"):
                """æ ¸å¿ƒè½¬æ¢é€»è¾‘"""
                # åˆ›å»ºè¾“å‡ºç›®å½•
                try:
                    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    return f"âŒ åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {str(e)}", {}, {}
                
                target_filename = selected_file if mode == "single" else None
                
                # æ‰§è¡Œè½¬æ¢
                result = data_adapter.convert_annotations(
                    input_dir, 
                    output_path, 
                    image_dir=image_dir,
                    filename=target_filename,
                    model_family=model_family,
                    csv_src_dir=str(RESULTS_BASE_PATH / "qwen") if model_family == "qwen" else settings.DATA_DOWNSAMPLED_DIR
                )
                
                status_msg = ""
                source_sample = {}
                target_sample = {}
                
                if result.get("success"):
                    log_output = result.get("stdout", "")
                    prefix = "å•æ–‡ä»¶" if mode == "single" else "æ‰¹é‡"
                    final_output_path = result.get("output_path") or output_path
                    status_msg = f"âœ… {prefix}è½¬æ¢æˆåŠŸ! \nè¾“å‡º: {final_output_path}\n\næ‰§è¡Œæ—¥å¿—:\n{log_output}"
                    
                    # å°è¯•è¯»å–æºæ–‡ä»¶è¿›è¡Œé¢„è§ˆ
                    try:
                         # å¦‚æœæ˜¯æ‰¹é‡è½¬æ¢ä½†ä¾ç„¶é€‰äº†æ–‡ä»¶ï¼Œæˆ–è€…å•æ–‡ä»¶æ¨¡å¼
                         preview_file = selected_file
                         if not preview_file and Path(input_dir).exists():
                             # å¦‚æœéƒ½æ²¡é€‰ï¼Œæ‰¾ç¬¬ä¸€ä¸ª
                             all_jsons = list(Path(input_dir).glob("*.json"))
                             if all_jsons:
                                 preview_file = all_jsons[0].name
                         
                         if preview_file:
                             src_p = Path(input_dir) / preview_file
                             if src_p.exists():
                                 with open(src_p, 'r', encoding='utf-8') as f:
                                    source_sample = json.load(f)
                    except Exception as e:
                        source_sample = {"error": f"Read source failed: {str(e)}"}

                    # è¯»å–è½¬æ¢åçš„ç»“æœ
                    try:
                        with open(final_output_path, 'r', encoding='utf-8') as f:
                            converted_data = json.load(f)
                            if converted_data and isinstance(converted_data, list):
                                if preview_file:
                                    # å°è¯•åŒ¹é… image è·¯å¾„
                                    core_name = preview_file.replace("annotations_æ•°æ®é›†", "").replace(".json", "")
                                    # ç®€å•å»åç¼€
                                    core_name = core_name.replace(".csv", "")
                                    
                                    matched = False
                                    for item in converted_data:
                                        if core_name in item.get("image", ""):
                                            target_sample = item
                                            matched = True
                                            break
                                    if not matched:
                                        target_sample = converted_data[-1] if mode == "single" else converted_data[0]
                                        status_msg = f"(æ³¨æ„ï¼šé¢„è§ˆæœªç²¾ç¡®åŒ¹é…ï¼Œæ˜¾ç¤º{'æœ€åä¸€æ¡' if mode=='single' else 'ç¬¬ä¸€æ¡'})\n" + status_msg
                                else:
                                    target_sample = converted_data[0]
                    except Exception as e:
                        target_sample = {"error": f"Read output failed: {str(e)}"}
                        
                else:
                    status_msg = f"âŒ è½¬æ¢å¤±è´¥: {result.get('error')}\næ—¥å¿—:\n{result.get('stderr', '')}"
                
                return status_msg, source_sample, target_sample

            # ç»‘å®šäº‹ä»¶
            # ç»‘å®šäº‹ä»¶
            convert_curr_btn.click(
                fn=lambda f, i, m, o, fam: convert_core(f, i, m, o, fam, mode="single"),
                inputs=[ann_file_dropdown, conf_input_dir, conf_image_dir, conf_output_path, conv_model_family],
                outputs=[convert_status, before_json, after_json]
            )
            
            convert_all_btn.click(
                fn=lambda f, i, m, o, fam: convert_core(f, i, m, o, fam, mode="batch"),
                inputs=[ann_file_dropdown, conf_input_dir, conf_image_dir, conf_output_path, conv_model_family],
                outputs=[convert_status, before_json, after_json]
            )
            
            # åŠ¨æ€æ›´æ–°è¾“å‡ºè·¯å¾„å’ŒLabel
            def on_model_family_change(family, ann_dir):
                new_path = settings.DATA_TRAINING_CHATTS_DIR if family == "chatts" else settings.DATA_TRAINING_QWEN_DIR
                new_conf = str(Path(new_path) / "converted_data.json")
                
                # Update output label
                new_label = "#### ğŸ¯ è½¬æ¢å (ChatTS Training Data)" if family == "chatts" else "#### ğŸ¯ è½¬æ¢å (Qwen Training Data)"
                
                # Update Image/Data Dir label and value
                if family == "chatts":
                    img_dir_label = "æ•°æ®æ–‡ä»¶æ¥æº (Source Data Dir)"
                    img_dir_val = settings.DATA_DOWNSAMPLED_DIR
                else:
                    img_dir_label = "å›¾ç‰‡æ–‡ä»¶æ¥æº (Source Image Dir)"
                    img_dir_val = settings.DATA_IMAGES_DIR
                
                # Filter file choices
                new_choices = get_file_choices(ann_dir, family)
                new_val = new_choices[0] if new_choices else None
                
                return (
                    new_conf, 
                    gr.update(value=new_label),
                    gr.update(value=img_dir_val, label=img_dir_label),
                    gr.update(choices=new_choices, value=new_val)
                )

            conv_model_family.change(
                fn=on_model_family_change,
                inputs=[conv_model_family, conf_input_dir],
                outputs=[conf_output_path, after_json_label, conf_image_dir, ann_file_dropdown]
            )
            
            refresh_files_btn.click(
                fn=refresh_files,
                inputs=[conf_input_dir, conv_model_family],
                outputs=ann_file_dropdown
            )
            
            # é€‰æ‹©æ–‡ä»¶ç«‹å³é¢„è§ˆ
            ann_file_dropdown.change(
                fn=preview_source_file,
                inputs=[ann_file_dropdown, conf_input_dir, conf_image_dir, conv_model_family],
                outputs=[before_json, after_json]
            )
            
            # æ¨¡å‹æ ¼å¼åˆ‡æ¢è§¦å‘é¢„è§ˆæ›´æ–°
            conv_model_family.change(
                fn=preview_source_file,
                inputs=[ann_file_dropdown, conf_input_dir, conf_image_dir, conv_model_family],
                outputs=[before_json, after_json]
            )
            
            # åˆå§‹åŒ–é¢„è§ˆ (Default to chatts as per Radio default)
            if initial_val:
                init_src, init_ex = preview_source_file(initial_val, default_ann_dir, settings.DATA_DOWNSAMPLED_DIR, "chatts")
                before_json.value = init_src
                after_json.value = init_ex

        # ==================== æ•°æ®èµ„äº§ç®¡ç† Tab (New) ====================
        with gr.Tab("ğŸ“¦ æ•°æ®èµ„äº§ç®¡ç†"):
            with gr.Tabs():
                # 1. æ ‡æ³¨æ•°æ®ç®¡ç†
                with gr.Tab("æ ‡æ³¨æ•°æ® (Annotations)"):
                    gr.Markdown("### ğŸ“ æ ‡æ³¨æ–‡ä»¶ç®¡ç†")
                    with gr.Row():
                        with gr.Column(scale=1):
                            ann_mgr_family = gr.Dropdown(
                                label="æ¨¡å‹ç±»å‹",
                                choices=["all", "chatts", "qwen", "timer", "adtk_hbos", "ensemble"],
                                value="all",
                                interactive=True
                            )
                            ann_mgr_dir = gr.Textbox(label="æ ‡æ³¨ç›®å½•", value=str(Path(settings.ANNOTATIONS_ROOT) / "douff"), interactive=False)
                            ann_mgr_list = gr.Dropdown(label="é€‰æ‹©æ–‡ä»¶", interactive=True)
                            refresh_ann_mgr = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                            delete_ann_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­æ–‡ä»¶", variant="stop")
                            ann_op_status = gr.Textbox(label="æ“ä½œçŠ¶æ€", interactive=False)
                        
                        with gr.Column(scale=2):
                            ann_mgr_view = gr.JSON(label="æ–‡ä»¶å†…å®¹é¢„è§ˆ", height=600)

                    # Logic
                    def list_ann_files(path_str, model_type="all"):
                        p = Path(path_str)
                        if not p.exists(): return []
                        files = list(p.glob("*.json"))
                        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                        names = [f.name for f in files]
                        mt = (model_type or "all").lower()
                        if mt == "all":
                            return names

                        def _matches(name: str) -> bool:
                            lower = name.lower()
                            if mt == "qwen":
                                return "qwen" in lower
                            if mt == "timer":
                                return "timer" in lower
                            if mt == "adtk_hbos":
                                return "adtk_hbos" in lower
                            if mt == "ensemble":
                                return "ensemble" in lower
                            if mt == "chatts":
                                return not any(k in lower for k in ["qwen", "timer", "adtk_hbos", "ensemble"])
                            return True

                        return [n for n in names if _matches(n)]

                    def load_ann_content(path_str, filename):
                        if not filename: return None
                        try:
                            with open(Path(path_str) / filename, 'r') as f:
                                return json.load(f)
                        except Exception as e:
                            return {"error": str(e)}

                    def delete_ann_file(path_str, filename):
                        if not filename: return "æœªé€‰æ‹©æ–‡ä»¶", gr.update()
                        try:
                            p = Path(path_str) / filename
                            p.unlink()
                            # åˆ·æ–°åˆ—è¡¨
                            new_list = list_ann_files(path_str)
                            return f"å·²åˆ é™¤: {filename}", gr.update(choices=new_list, value=None)
                        except Exception as e:
                            return f"åˆ é™¤å¤±è´¥: {e}", gr.update()

                    # Bindings
                    ann_mgr_dir.change(
                        fn=lambda p, mt: gr.update(choices=list_ann_files(p, mt)),
                        inputs=[ann_mgr_dir, ann_mgr_family],
                        outputs=ann_mgr_list
                    )
                    ann_mgr_family.change(
                        fn=lambda p, mt: gr.update(choices=list_ann_files(p, mt)),
                        inputs=[ann_mgr_dir, ann_mgr_family],
                        outputs=ann_mgr_list
                    )
                    refresh_ann_mgr.click(
                        fn=lambda p, mt: gr.update(choices=list_ann_files(p, mt)),
                        inputs=[ann_mgr_dir, ann_mgr_family],
                        outputs=ann_mgr_list
                    )
                    ann_mgr_list.change(fn=load_ann_content, inputs=[ann_mgr_dir, ann_mgr_list], outputs=ann_mgr_view)
                    delete_ann_btn.click(fn=delete_ann_file, inputs=[ann_mgr_dir, ann_mgr_list], outputs=[ann_op_status, ann_mgr_list])

                # 2. è®­ç»ƒæ•°æ®ç®¡ç†
                with gr.Tab("è®­ç»ƒæ•°æ® (Training Data)"):
                    gr.Markdown("### ğŸ¯ å¾®è°ƒæ•°æ®ç®¡ç† (Converted JSONL)")
                    with gr.Row():
                        with gr.Column(scale=1):
                            train_mgr_family = gr.Dropdown(
                                label="æ¨¡å‹ç±»å‹",
                                choices=TRAINING_MODEL_FAMILIES,
                                value="chatts",
                                interactive=True
                            )
                            train_mgr_dir = gr.Textbox(
                                label="æ•°æ®ç›®å½•",
                                value=settings.DATA_TRAINING_CHATTS_DIR,
                                interactive=False
                            )
                            train_mgr_list = gr.Dropdown(label="é€‰æ‹©æ–‡ä»¶", interactive=True)
                            refresh_train_mgr = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                            delete_train_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­æ–‡ä»¶", variant="stop")
                            train_op_status = gr.Textbox(label="æ“ä½œçŠ¶æ€", interactive=False)
                        
                        with gr.Column(scale=2):
                            train_mgr_view = gr.JSON(label="æ–‡ä»¶å†…å®¹é¢„è§ˆ (Head 50 lines / JSON)", height=600)

                    # Logic
                    def list_train_files(path_str):
                        p = Path(path_str)
                        if not p.exists(): return []
                        files = list(p.glob("*.json")) + list(p.glob("*.jsonl"))
                        # Filter useless files
                        files = [
                            f for f in files 
                            if not f.name.startswith("dataset_info") 
                            and not f.name.startswith(".") 
                            and not f.name.startswith("_")
                        ]
                        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                        return [f.name for f in files]

                    def load_train_content(path_str, filename):
                        if not filename: return None
                        try:
                            p = Path(path_str) / filename
                            
                            # Strategy 1: Small file (<50MB) -> Try full JSON load
                            # This handles standard JSON lists (pretty printed or minified)
                            if p.stat().st_size < 50 * 1024 * 1024: 
                                try:
                                    with open(p, 'r') as f:
                                        data = json.load(f)
                                    if isinstance(data, list):
                                        return data[:50]  # Preview first 50 items
                                    return data
                                except:
                                    pass # Fallback to Strategy 2
                            
                            # Strategy 2: JSONL or Large File -> Line-by-line
                            records = []
                            with open(p, 'r') as f:
                                for _ in range(50):
                                    line = f.readline()
                                    if not line: break
                                    line = line.strip()
                                    if not line: continue
                                    try:
                                        records.append(json.loads(line))
                                    except:
                                        # If it looks like start/end of array, skip or show raw
                                        if line in ['[', ']', '],']: continue
                                        records.append({"raw_text": line})
                            return records
                        except Exception as e:
                            return {"error": str(e)}

                    def delete_train_file(path_str, filename):
                        if not filename: return "æœªé€‰æ‹©æ–‡ä»¶", gr.update()
                        try:
                            p = Path(path_str) / filename
                            p.unlink()
                            new_list = list_train_files(path_str)
                            return f"å·²åˆ é™¤: {filename}", gr.update(choices=new_list, value=None)
                        except Exception as e:
                            return f"åˆ é™¤å¤±è´¥: {e}", gr.update()

                    # Bindings
                    # Init load
                    train_mgr_dir.change(fn=lambda p: gr.update(choices=list_train_files(p)), inputs=train_mgr_dir, outputs=train_mgr_list)
                    refresh_train_mgr.click(fn=lambda p: gr.update(choices=list_train_files(p)), inputs=train_mgr_dir, outputs=train_mgr_list)
                    train_mgr_list.change(fn=load_train_content, inputs=[train_mgr_dir, train_mgr_list], outputs=train_mgr_view)
                    delete_train_btn.click(fn=delete_train_file, inputs=[train_mgr_dir, train_mgr_list], outputs=[train_op_status, train_mgr_list])

                    def resolve_train_mgr_dir(model_family: str) -> str:
                        if model_family == "qwen":
                            return settings.DATA_TRAINING_QWEN_DIR
                        return settings.DATA_TRAINING_CHATTS_DIR

                    train_mgr_family.change(
                        fn=lambda mf: gr.update(value=resolve_train_mgr_dir(mf)),
                        inputs=train_mgr_family,
                        outputs=train_mgr_dir
                    ).then(
                        fn=lambda p: gr.update(choices=list_train_files(p), value=None),
                        inputs=train_mgr_dir,
                        outputs=train_mgr_list
                    )
        
        # ==================== å¾®è°ƒè®­ç»ƒ Tab (åŸæœ‰) ====================
        with gr.Tab("ğŸ¯ å¼€å§‹è®­ç»ƒ"):
            with gr.Row():
                with gr.Column(scale=2):
                    # åŸºç¡€é…ç½®
                    gr.Markdown("### åŸºç¡€é…ç½®")
                    
                    # New Dropdowns for Quick Start
                    with gr.Row():
                        model_family_dropdown = gr.Dropdown(
                            label="æ¨¡å‹ç±»å‹ (Model Family)",
                            choices=TRAINING_MODEL_FAMILIES,
                            value="chatts",
                            interactive=True
                        )
                        model_path_dropdown = gr.Dropdown(
                            label="åŸºç¡€æ¨¡å‹ (Base Model)",
                            choices=get_training_adapter("chatts").get_base_models(),
                            value=get_training_adapter("chatts").get_base_models()[0] if get_training_adapter("chatts").get_base_models() else None,
                            interactive=True,
                            allow_custom_value=True
                        )
                        dataset_dropdown = gr.Dropdown(
                            label="å¾®è°ƒæ•°æ®é›† (Dataset)",
                            choices=get_training_adapter("chatts").get_dataset_list(),
                            value=get_training_adapter("chatts").get_dataset_list()[0] if get_training_adapter("chatts").get_dataset_list() else None,
                            interactive=True
                        )
                    with gr.Row():
                        train_method_dropdown = gr.Dropdown(
                            label="è®­ç»ƒæ–¹å¼ (Method)",
                            choices=TRAINING_METHODS,
                            value="lora",
                            interactive=True
                        )
                        config_dropdown = gr.Dropdown(
                            label="è®­ç»ƒæ¨¡æ¿ (Template Script)",
                            choices=get_training_configs("chatts", "lora"),
                            interactive=True,
                            info="é€‰æ‹©ä¸€ä¸ªè„šæœ¬ä½œä¸ºå‚æ•°æ¨¡æ¿ (å¦‚ DeepSpeed é…ç½®)"
                        )
                    output_name = gr.Textbox(
                        label="è¾“å‡ºç›®å½•åç§°",
                        placeholder="ä¾‹å¦‚: my_model_v1"
                    )
                    
                    with gr.Row():
                        learning_rate = gr.Textbox(
                            label="å­¦ä¹ ç‡",
                            value="2e-5"
                        )
                        num_epochs = gr.Number(
                            label="è®­ç»ƒè½®æ•°",
                            value=3.0
                        )
                        batch_size = gr.Slider(
                            label="æ‰¹æ¬¡å¤§å°",
                            minimum=1,
                            maximum=32,
                            value=2,
                            step=1
                        )
                    
                    # LoRA é…ç½®ï¼ˆæŠ˜å ï¼‰
                    with gr.Accordion("LoRA é…ç½®", open=False):
                        with gr.Row():
                            lora_rank = gr.Slider(
                                label="LoRA Rank",
                                minimum=1,
                                maximum=128,
                                value=8,
                                step=1
                            )
                            lora_alpha = gr.Slider(
                                label="LoRA Alpha",
                                minimum=1,
                                maximum=256,
                                value=16,
                                step=1
                            )

                    # è¿è¡Œä¸æ˜¾å­˜é…ç½®ï¼ˆæŠ˜å ï¼‰
                    with gr.Accordion("è¿è¡Œä¸æ˜¾å­˜é…ç½®", open=False):
                        with gr.Row():
                            nproc_per_node = gr.Slider(
                                label="NPROC_PER_NODE (è¿›ç¨‹æ•°)",
                                minimum=1,
                                maximum=8,
                                value=1,
                                step=1
                            )
                            cuda_visible_devices = gr.Textbox(
                                label="CUDA_VISIBLE_DEVICES",
                                value="0",
                                placeholder="ä¾‹å¦‚: 0 æˆ– 0,1 (ç•™ç©º=ä¸è®¾ç½®)"
                            )
                        with gr.Row():
                            grad_accum_steps = gr.Slider(
                                label="Gradient Accumulation Steps",
                                minimum=1,
                                maximum=64,
                                value=8,
                                step=1
                            )
                            precision = gr.Dropdown(
                                label="Precision",
                                choices=["bf16", "fp16", "fp32"],
                                value="bf16"
                            )
                        with gr.Row():
                            cutoff_len = gr.Number(
                                label="cutoff_len",
                                value=4096
                            )
                            image_max_pixels = gr.Number(
                                label="image_max_pixels",
                                value=3200000
                            )
                            image_min_pixels = gr.Number(
                                label="image_min_pixels",
                                value=1024
                            )
                        extra_args = gr.Textbox(
                            label="é¢å¤–å‚æ•° (Raw Args)",
                            placeholder="ä¾‹å¦‚: --logging_steps 5 --save_steps 50",
                            lines=2
                        )

                    # é«˜çº§è®­ç»ƒå‚æ•°ï¼ˆæŠ˜å ï¼‰
                    with gr.Accordion("é«˜çº§è®­ç»ƒå‚æ•°", open=False):
                        with gr.Row():
                            logging_steps = gr.Textbox(
                                label="logging_steps (å¯é€‰)",
                                placeholder="ç•™ç©ºä¸è¦†ç›–"
                            )
                            save_steps = gr.Textbox(
                                label="save_steps (å¯é€‰)",
                                placeholder="ç•™ç©ºä¸è¦†ç›–"
                            )
                            lr_scheduler_type = gr.Dropdown(
                                label="lr_scheduler_type (å¯é€‰)",
                                choices=["", "cosine", "linear", "constant", "polynomial", "cosine_with_restarts", "constant_with_warmup"],
                                value=""
                            )
                        with gr.Row():
                            warmup_steps = gr.Textbox(
                                label="warmup_steps (å¯é€‰)",
                                placeholder="ç•™ç©ºä¸è¦†ç›–"
                            )
                            warmup_ratio = gr.Textbox(
                                label="warmup_ratio (å¯é€‰)",
                                placeholder="ç•™ç©ºä¸è¦†ç›–"
                            )
                        with gr.Row():
                            lora_dropout = gr.Textbox(
                                label="lora_dropout (å¯é€‰)",
                                placeholder="ç•™ç©ºä¸è¦†ç›–"
                            )
                            lora_target = gr.Textbox(
                                label="lora_target (å¯é€‰)",
                                placeholder="å¦‚: q_proj,k_proj,v_proj,o_proj"
                            )
                        with gr.Row():
                            freeze_vision_tower = gr.Dropdown(
                                label="freeze_vision_tower (å¯é€‰)",
                                choices=["", "True", "False"],
                                value=""
                            )
                            freeze_multi_modal_projector = gr.Dropdown(
                                label="freeze_multi_modal_projector (å¯é€‰)",
                                choices=["", "True", "False"],
                                value=""
                            )
                        with gr.Row():
                            freeze_trainable_layers = gr.Textbox(
                                label="freeze_trainable_layers (å¯é€‰)",
                                placeholder="ç•™ç©ºä¸è¦†ç›–"
                            )
                            freeze_trainable_modules = gr.Textbox(
                                label="freeze_trainable_modules (å¯é€‰)",
                                placeholder="å¦‚: all"
                            )
                    
                    # æ§åˆ¶æŒ‰é’®
                    # --- Backend Functions ---
                    def validate_dataset_wrap(model_family: str, dataset_name: Optional[str]) -> str:
                        data_dir = (settings.DATA_TRAINING_QWEN_DIR
                                    if model_family == "qwen"
                                    else settings.DATA_TRAINING_CHATTS_DIR)
                        info_path = Path(data_dir) / "dataset_info.json"
                        if not info_path.exists():
                            return f"âŒ æ‰¾ä¸åˆ° dataset_info.json: {info_path}"
                        try:
                            with open(info_path, "r", encoding="utf-8") as f:
                                info = json.load(f)
                        except Exception as e:
                            return f"âŒ è¯»å–å¤±è´¥: {e}"

                        if not info:
                            return f"âŒ dataset_info.json ä¸ºç©º: {info_path}"
                        if not dataset_name:
                            return "âš ï¸ æœªé€‰æ‹©æ•°æ®é›†"
                        if dataset_name not in info:
                            return f"âŒ æ•°æ®é›†æœªæ³¨å†Œ: {dataset_name}"

                        file_name = info[dataset_name].get("file_name")
                        if not file_name:
                            return f"âŒ æœªé…ç½® file_name: {dataset_name}"
                        data_path = Path(file_name)
                        if not data_path.is_absolute():
                            data_path = Path(data_dir) / file_name

                        if not data_path.exists():
                            return f"âŒ æ•°æ®ä¸å­˜åœ¨: {data_path}"

                        if data_path.is_dir():
                            files = list(data_path.glob("*.json")) + list(data_path.glob("*.jsonl"))
                            if not files:
                                return f"âŒ ç›®å½•ä¸‹æ²¡æœ‰ json/jsonl: {data_path}"
                            return f"âœ… æ ¡éªŒé€šè¿‡: {dataset_name} (ç›®å½•, {len(files)} ä¸ªæ–‡ä»¶)"

                        return f"âœ… æ ¡éªŒé€šè¿‡: {dataset_name} ({data_path.name})"

                    def start_training_wrap(
                        model_family, config_name, lr, epochs, batch_size, rank, alpha, output_name,
                        model_path, dataset_name, nproc, cuda_devices, grad_accum, prec, cutoff,
                        img_max, img_min, extra_args, log_steps, save_steps, lr_sched, warm_steps,
                        warm_ratio, lora_drop, lora_tgt, freeze_vision, freeze_proj, freeze_layers, freeze_modules
                    ):
                        if not config_name:
                            return "âŒ è¯·é€‰æ‹©è®­ç»ƒæ¨¡æ¿", "", model_family
                            
                        task_id = f"qs_{int(time.time())}"
                        
                        local_adapter = get_training_adapter(model_family)
                        print(f"[TRAIN_UI] start_training_wrap task_id={task_id} model_family={model_family} config={config_name}")

                        cuda_devices = (cuda_devices or "").strip() or None
                        extra_args = (extra_args or "").strip() or None
                        log_steps = (log_steps or "").strip() or None
                        save_steps = (save_steps or "").strip() or None
                        lr_sched = (lr_sched or "").strip() or None
                        warm_steps = (warm_steps or "").strip() or None
                        warm_ratio = (warm_ratio or "").strip() or None
                        lora_drop = (lora_drop or "").strip() or None
                        lora_tgt = (lora_tgt or "").strip() or None
                        freeze_vision = (freeze_vision or "").strip() or None
                        freeze_proj = (freeze_proj or "").strip() or None
                        freeze_layers = (freeze_layers or "").strip() or None
                        freeze_modules = (freeze_modules or "").strip() or None

                        # Call backend
                        overrides = {
                            "override_learning_rate": lr,
                            "override_epochs": epochs,
                            "override_batch_size": batch_size,
                            "override_lora_rank": rank,
                            "override_lora_alpha": alpha,
                            "override_model_path": model_path,
                            "override_dataset": dataset_name,
                            "override_nproc_per_node": nproc,
                            "override_cuda_visible_devices": cuda_devices,
                            "override_grad_accum_steps": grad_accum,
                            "override_precision": prec,
                            "override_cutoff_len": cutoff,
                            "override_image_max_pixels": img_max,
                            "override_image_min_pixels": img_min,
                            "override_extra_args": extra_args,
                            "override_logging_steps": log_steps,
                            "override_save_steps": save_steps,
                            "override_lr_scheduler_type": lr_sched,
                            "override_warmup_steps": warm_steps,
                            "override_warmup_ratio": warm_ratio,
                            "override_lora_dropout": lora_drop,
                            "override_lora_target": lora_tgt,
                            "override_freeze_vision_tower": freeze_vision,
                            "override_freeze_multi_modal_projector": freeze_proj,
                            "override_freeze_trainable_layers": freeze_layers,
                            "override_freeze_trainable_modules": freeze_modules
                        }
                        
                        res = local_adapter.run_training(task_id, config_name, version_tag=output_name, **overrides)
                        
                        if res.get("success"):
                            return f"âœ… è®­ç»ƒä»»åŠ¡å·²æˆåŠŸå¯åŠ¨!\nä»»åŠ¡ID: {task_id}\nè¾“å‡ºç›®å½•: {res.get('output_dir')}\n\næ­£åœ¨åå°è¿è¡Œä¸­... è¯·ç•™æ„ä¸‹æ–¹å®æ—¶æ—¥å¿—ã€‚", task_id, model_family
                        else:
                            return f"âŒ å¯åŠ¨é”™è¯¯: {res.get('error')}", "", model_family

                    def stream_logs(model_family, task_id, current_log_text, offset):
                        if not task_id:
                            return format_log_html(current_log_text), current_log_text, offset
                        
                        # Read increment
                        res = get_training_adapter(model_family).get_training_log(task_id, offset)
                        new_content = res.get("log", "")
                        new_offset = res.get("offset", offset)
                        
                        if new_content:
                            current_log_text = (current_log_text or "") + new_content
                            if len(current_log_text) > LOG_TAIL_MAX_CHARS:
                                current_log_text = current_log_text[-LOG_TAIL_MAX_CHARS:]
                            
                        return format_log_html(current_log_text), current_log_text, new_offset

                    def stop_training_wrap(model_family, task_id):
                        if not task_id:
                            return "æ— è¿è¡Œä¸­çš„ä»»åŠ¡"
                        res = get_training_adapter(model_family).stop_training(task_id)
                        if res.get("success"):
                            return "ğŸ›‘ ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢"
                        else:
                            return f"åœæ­¢å¤±è´¥: {res.get('error')}"

                    # --- Layout & Events ---
                    with gr.Column():
                        with gr.Row():
                            start_btn = gr.Button("ğŸš€ (Quick Start) å¼€å§‹è®­ç»ƒ", variant="primary", scale=2)
                            stop_btn = gr.Button("ğŸ›‘ åœæ­¢è®­ç»ƒ", variant="stop", scale=1)
                            validate_btn = gr.Button("âœ… æ ¡éªŒæ•°æ®", scale=1)
                            refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°é…ç½®", scale=1)
                        
                        # Hidden state for Task ID and Log Offset
                        task_id_state = gr.State("")
                        task_family_state = gr.State("chatts")
                        log_offset_state = gr.State(0)
                        training_log_state = gr.State("") # Raw text state
                        
                        output_box = gr.Textbox(label="è®­ç»ƒçŠ¶æ€", lines=4)
                        validate_box = gr.Textbox(label="æ•°æ®æ ¡éªŒç»“æœ", lines=2)
                        log_box = gr.HTML(label="å®æ—¶æ—¥å¿— (Real-time Logs)", value=format_log_html(""), elem_id="training-log")
                        
                        # Timer for polling
                        timer = gr.Timer(1) # 1s interval

                        # Events
                        start_btn.click(
                            fn=start_training_wrap,
                            inputs=[
                                model_family_dropdown, config_dropdown, learning_rate, num_epochs,
                                batch_size, lora_rank, lora_alpha, output_name,
                                model_path_dropdown, dataset_dropdown,
                                nproc_per_node, cuda_visible_devices, grad_accum_steps, precision,
                                cutoff_len, image_max_pixels, image_min_pixels, extra_args,
                                logging_steps, save_steps, lr_scheduler_type, warmup_steps,
                                warmup_ratio, lora_dropout, lora_target, freeze_vision_tower,
                                freeze_multi_modal_projector, freeze_trainable_layers, freeze_trainable_modules
                            ],
                            outputs=[output_box, task_id_state, task_family_state],
                            queue=False
                        ).then(
                            fn=lambda: 0, outputs=log_offset_state # Reset offset
                        ).then(
                            fn=lambda: "", outputs=training_log_state # Clear raw log
                        ).then(
                            fn=lambda: format_log_html(""), outputs=log_box # Clear visual log
                        )

                        model_family_dropdown.change(
                            fn=update_training_dropdowns,
                            inputs=[model_family_dropdown, train_method_dropdown],
                            outputs=[config_dropdown, dataset_dropdown, model_path_dropdown]
                        )

                        train_method_dropdown.change(
                            fn=update_training_config_only,
                            inputs=[model_family_dropdown, train_method_dropdown],
                            outputs=config_dropdown
                        )
                        
                        stop_btn.click(
                            fn=stop_training_wrap,
                            inputs=[task_family_state, task_id_state],
                            outputs=output_box
                        )

                        validate_btn.click(
                            fn=validate_dataset_wrap,
                            inputs=[model_family_dropdown, dataset_dropdown],
                            outputs=validate_box
                        )
                        
                        # Timer ticks -> Update logs
                        timer.tick(
                            fn=stream_logs,
                            inputs=[task_family_state, task_id_state, training_log_state, log_offset_state],
                            outputs=[log_box, training_log_state, log_offset_state],
                            queue=False
                        )
                        refresh_btn.click(
                            fn=update_training_dropdowns,
                            inputs=[model_family_dropdown, train_method_dropdown],
                            outputs=[config_dropdown, dataset_dropdown, model_path_dropdown]
                        )

            # ==================== Advanced Mode (Native Integration) ====================
            with gr.Accordion("âš™ï¸ é«˜çº§æ¨¡å¼ (Native WebUI Integration)", open=False):
                gr.Markdown("""
                > **ä¸“å®¶æ¨¡å¼**: å°†å½“å‰é€‰æ‹©çš„ Shell è„šæœ¬è‡ªåŠ¨è½¬æ¢ä¸º LLaMA-Factory é…ç½®ï¼Œå¹¶å¯åŠ¨åŸç”Ÿ WebUI è¿›è¡Œå¾®è°ƒã€‚
                > é€‚åˆéœ€è¦è°ƒæ•´ DeepSpeedã€LR Scheduler ç­‰é«˜çº§å‚æ•°çš„ç”¨æˆ·ã€‚
                """)
                with gr.Row():
                    convert_btn = gr.Button("ğŸ› ï¸ 1. è½¬æ¢è„šæœ¬ä¸ºæ¨¡æ¿", variant="secondary")
                    launch_native_btn = gr.Button("ğŸš€ 2. å¯åŠ¨åŸç”Ÿ WebUI", variant="primary")
                
                native_status = gr.Markdown("ç­‰å¾…æ“ä½œ...")
                native_ui_link = gr.Markdown(visible=False)

                # Logic
                def convert_action(script_name):
                    if not script_name: return "âš ï¸ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªè„šæœ¬é…ç½®"
                    res = adapter.convert_script_to_config(script_name)
                    if res["success"]:
                        return f"âœ… {res['message']}"
                    return f"âŒ {res['error']}"

                def launch_action():
                    res = adapter.start_native_webui()
                    if res["success"]:
                        url = res['url']
                        return f"âœ… {res['message']}", gr.Markdown(f"### [ğŸ‘‰ ç‚¹å‡»è®¿é—®åŸç”Ÿ WebUI ({url})]({url})", visible=True)
                    return f"âŒ {res['error']}", gr.update(visible=False)

                convert_btn.click(convert_action, inputs=config_dropdown, outputs=native_status)
                launch_native_btn.click(launch_action, outputs=[native_status, native_ui_link])
        
        with gr.Tab("ğŸ“Š å·²è®­ç»ƒæ¨¡å‹"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### æ¨¡å‹åˆ—è¡¨")
                    with gr.Row():
                        model_family_view = gr.Dropdown(
                            label="æ¨¡å‹ç±»å‹ (Model Family)",
                            choices=TRAINING_MODEL_FAMILIES,
                            value="chatts",
                            interactive=True
                        )
                        model_type_view = gr.Dropdown(
                            label="è®­ç»ƒæ–¹å¼ (Method)",
                            choices=TRAINING_METHODS,
                            value="all",
                            interactive=True
                        )
                        include_ckpt_view = gr.Checkbox(
                            label="æ˜¾ç¤º checkpoint",
                            value=False
                        )
                    model_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æ¨¡å‹",
                        choices=get_trained_model_choices("chatts", "all", False),
                        interactive=True
                    )
                    refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                
                with gr.Column(scale=2):
                    gr.Markdown("### æ¨¡å‹è¯¦æƒ…")
                    model_info = gr.Markdown(value="è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹")
                    
                    gr.Markdown("### Loss æ›²çº¿")
                    loss_image = gr.Image(label="Training Loss", type="filepath")
            
            # äº‹ä»¶ç»‘å®š
            model_dropdown.change(
                fn=get_model_info,
                inputs=[model_dropdown, model_family_view],
                outputs=model_info
            )
            model_dropdown.change(
                fn=get_loss_plot,
                inputs=[model_dropdown, model_family_view],
                outputs=loss_image
            )
            refresh_models_btn.click(
                fn=lambda mf, mt, ck: gr.Dropdown(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[model_family_view, model_type_view, include_ckpt_view],
                outputs=model_dropdown
            )
            model_family_view.change(
                fn=lambda mf, mt, ck: gr.Dropdown(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[model_family_view, model_type_view, include_ckpt_view],
                outputs=model_dropdown
            )
            model_type_view.change(
                fn=lambda mf, mt, ck: gr.Dropdown(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[model_family_view, model_type_view, include_ckpt_view],
                outputs=model_dropdown
            )
            include_ckpt_view.change(
                fn=lambda mf, mt, ck: gr.Dropdown(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[model_family_view, model_type_view, include_ckpt_view],
                outputs=model_dropdown
            )
            
        with gr.Tab("âš–ï¸ æ¨¡å‹å¯¹æ¯”"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### é€‰æ‹©å¯¹æ¯”æ¨¡å‹")
                    with gr.Row():
                        compare_family = gr.Dropdown(
                            label="æ¨¡å‹ç±»å‹ (Model Family)",
                            choices=TRAINING_MODEL_FAMILIES,
                            value="chatts",
                            interactive=True
                        )
                        compare_type = gr.Dropdown(
                            label="è®­ç»ƒæ–¹å¼ (Method)",
                            choices=TRAINING_METHODS,
                            value="all",
                            interactive=True
                        )
                        compare_include_ckpt = gr.Checkbox(
                            label="æ˜¾ç¤º checkpoint",
                            value=False
                        )
                    compare_models = gr.CheckboxGroup(
                        label="æ¨¡å‹åˆ—è¡¨",
                        choices=get_trained_model_choices("chatts", "all", False)
                    )
                    compare_btn = gr.Button("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾", variant="primary")
                    refresh_compare_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                
                with gr.Column(scale=3):
                    gr.Markdown("### å¯¹æ¯”ç»“æœ")
                    comparison_plot = gr.Image(label="Loss Comparison")
            
            # äº‹ä»¶ç»‘å®š
            compare_btn.click(
                fn=get_comparison_plot,
                inputs=[compare_models, compare_family],
                outputs=comparison_plot
            )
            refresh_compare_btn.click(
                fn=lambda mf, mt, ck: gr.CheckboxGroup(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[compare_family, compare_type, compare_include_ckpt],
                outputs=compare_models
            )
            compare_family.change(
                fn=lambda mf, mt, ck: gr.CheckboxGroup(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[compare_family, compare_type, compare_include_ckpt],
                outputs=compare_models
            )
            compare_type.change(
                fn=lambda mf, mt, ck: gr.CheckboxGroup(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[compare_family, compare_type, compare_include_ckpt],
                outputs=compare_models
            )
            compare_include_ckpt.change(
                fn=lambda mf, mt, ck: gr.CheckboxGroup(choices=get_trained_model_choices(mf, mt, ck)),
                inputs=[compare_family, compare_type, compare_include_ckpt],
                outputs=compare_models
            )
        
        with gr.Tab("âš™ï¸ é…ç½®è¯´æ˜"):
            gr.Markdown("""
## è®­ç»ƒé…ç½®è¯´æ˜

### åŸºç¡€å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| **å­¦ä¹ ç‡** | æ¨¡å‹æ›´æ–°æ­¥é•¿ | 2e-5 ~ 5e-5 |
| **è®­ç»ƒè½®æ•°** | å®Œæ•´éå†æ•°æ®é›†æ¬¡æ•° | 3 ~ 5 |
| **æ‰¹æ¬¡å¤§å°** | æ¯æ¬¡æ›´æ–°çš„æ ·æœ¬æ•° | 2 ~ 8 (å–å†³äºæ˜¾å­˜) |

### LoRA å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| **LoRA Rank** | ä½ç§©åˆ†è§£ç»´åº¦ | 8 ~ 64 |
| **LoRA Alpha** | ç¼©æ”¾å› å­ | é€šå¸¸ä¸º Rank çš„ 2 å€ |

### æ˜¾å­˜éœ€æ±‚

| æ¨¡å‹ | æ‰¹æ¬¡å¤§å° | æ˜¾å­˜éœ€æ±‚ |
|------|---------|---------|
| ChatTS-8B + LoRA | 2 | ~16GB |
| ChatTS-8B + LoRA | 4 | ~24GB |
| ChatTS-14B + LoRA | 2 | ~24GB |

### è®­ç»ƒè„šæœ¬

å¯ç”¨çš„è®­ç»ƒé…ç½®æ¥è‡ª `/home/douff/ts/ts-iteration-loop/services/training/scripts/chatts/lora/` å’Œ
`/home/douff/ts/ts-iteration-loop/services/training/scripts/qwen/lora/` ç›®å½•ã€‚
""")
    
        # åˆå§‹åŒ–åŠ è½½
        demo.load(
            fn=get_dataset_names,
            outputs=preview_dropdown
        ).then(
            fn=lambda x: x[0] if x else None,
            inputs=preview_dropdown,
            outputs=preview_dropdown
        ).then(
            fn=preview_dataset,
            inputs=preview_dropdown,
            outputs=[preview_table, column_selector, preview_plot]
        )
            
    return demo


# åˆ›å»ºå…¨å±€ Gradio åº”ç”¨å®ä¾‹
training_ui = create_training_ui()
